

# **项目介绍**

## 秒杀项目

### 项目简介

主要功能模块分为：登录模块，商品展示模块，秒杀模块。


### 技术点

![秒杀技术点](C:\Users\13368\Desktop\秒杀技术点.png)

前端：Thymeleaf、jQuery、Bootstrap对HTML页面做了一些简单处理和渲染

后端：利用SpringBoot搭建项目，利用MybatisPlus做了一些数据库查询业务

中间件：利用Redis对用户信息，库存信息，订单信息等做缓存，减少数据库的访问，后续还利用Redis的分布式锁解决商品超卖的问题，Rabbitmq来异步处理订单创建业务，实现流量削峰。



### 功能模块

#### **登录模块**：

**首先**通过Mybatis-plus的**逆向工程生成**对应的pojo、mapper、service、serviceimpl、controller等类，其次前端使用一次MD5加密，防止用户密码在传输过程泄露。**接着**对**手机号码格式验证**(自定义注解通过jdk提供的@Constraint绑定一个实现了ConstraintValidator<>泛型接口类，类中重写验证方法，然后接受对象参数的时候使用@Valid去验证)，和**密码**是否正确做**验证**（service层查询数据库）。

*异常处理*：springboot对于异常处理有两种，一是使用ErrorController来处理所有异常包括未进入控制器的异常，二是使用@ControllerAdvice注解处理控制器抛出的异常，该方式可以定义多个拦截方法以及拦截不同异常类，项目中使用该方式来处理控制器方法中的全部异常(@RestControllerAdvice注解于异常处理类，类中异常处理方法注解@ExceptionHandler(Exception.class))。

##### **模块优化**

- 分布式Session：当部署多态服务器配合Nginx会出现用户请求分配到不同服务器，因为Session的存储问题，用户需要多次在不同服务器上登录。

**解决方案：**

​	1 **Session复制**：利用Tomcat配置实现，将服务器A的Session复制到服务器B，服务器B的复制到A。

​		优点：实现简单

​		缺点：同步过程占用内网带宽，服务器过多时同步性能差。

​	2 **Session粘滞**：利用`Nginx`服务器的反向代理(Hash算法)，将服务器A和服务器B进行代理，然后采用`ip_hash`的负载策略，将客户端和服务器进行绑定，也就是说客户端A第一次访问的是服务器B，那么第二次访问也必然是服务器B，这样就不存在session不一致的问题了。

​	缺点：如果服务器A宕机了，那么客户端A和客户端B的session就会出现丢失。	

​	3 **Session集中管理**(项目使用方案)：这种方式就是将所有服务器的`session`进行统一管理，可以使用`redis`等高性能服务器来集中管理session。

方式一：Spring Session 是 Spring 家族中的一个子项目， 它提供一组 API 和实现， 用于管理用户的 session 信息.它把 servlet 容器实现的 httpSession 替换为 spring-session， 专注于解决 session 管理问题， Session 信息存储在 Redis 中， 可简单快速且无缝的集成到我们的应用中，实现简单，导入依赖，配置redis即可。
方式二：提取用户信息存储至redis(配置RedisTemplate，如序列化jackson2Json)



- 参数校验：**避免每个控制器方法中都要对User对象进行判断是否登录**。

**解决方案**

​	对Springmvc进行配置引入ArgumentResolver()，参数传入Controller方法前利用Resolver先对参数进行解析。



#### 商品模块

通过Mybatis和Mybatis-plus查询商品的详情，返回的信息利用thymleaf传入html页面。

##### **模块优化**

- 页面静态化：每次请求生成新的页面，性能较差。

**解决方案**：

  对于动态展示的数据，利用ajax请求来获取后端返回的对象。



#### 秒杀模块

判断库存

判断重复抢购(一是秒杀订单表中，将userId和goodsId创建 唯一索引

但凡有两条一样的数据，整体的业务就会回滚，保证了一个人一条秒杀订单，二是秒杀成功的时候，我们将userId和商品id组合一个新的redis的key，放入到redis中。后续在进行重复判断的时候，只需在redis中去判断即可)

抢购成功后减库存以及生成订单

##### 模块优化

- 库存超卖

**解决方案**

方案一：乐观锁的一种方式，sql语句在更新库存的同时判断库存是否大于0。

方案二：商品库存存放在redis中，实际扣除库存利用lua脚本(保证上锁，比较锁，删除锁的原子性，如果一个线程一直没删锁后续线程要一直等待)来减去redis中库存，redis加分布式锁，加锁时为锁设置相应的标识，例如uuid，防止设置了过期时间，线程执行时间大于过期时间，线程删除的是后续线程设置的锁。

​	*可能存在缓存不一致 * (待解决，一个用户同时发送多次请求，redis中减库存，但在mysql中由于唯一索引会回滚数据)

​	**解决方案**	

​		1 先更新数据库，再删除缓存，但是**如果删除缓存失败或者还没有来得及删除**，那么，其他线程从缓存中读取到的就是旧值，还是会发生不一致，可以设置过期时间(实时性要求不是很高)，其次可以借助消息队列，更新数据库成功后往消息队列发消息，借助消息队列的重试机制来实现达到最终一致性的效果。

​		2 先删缓存，再更新数据库，但如果数据库还没有更新成功，此时如果读取缓存，缓存不存在，**去数据库中读取到的是旧值**，缓存更新后仍是旧值，之后线程访问的也都是旧值，可利用延时双删，在更新完数据库之后，再sleep一段时间，然后再次删除缓存。



- rabbitmq异步操作：我们生成一个消息对象，将用户和商品id放入到消息对象中，通过mq发送出去。我们只需要直接给用户反馈排队中，不需要我们在当前这个业务中写减库存，建订单的操作。而是交给MQ的消费者去做，起到流量削峰的作用。




- 秒杀接口地址隐藏：通过ajax请求先获取一个随机生成参数，将这个随机参数存储到redis中，在用户进行秒杀的时候，前端将用户这个参数传递过来,通过@Value注解获取再去redis中进行验证。




- 接口限流: 计数器算法，利用自定义注解(注解在需要限流的控制器方法上，给定时间和该时间范围最大访问次数)+拦截器实现(拦截器方法中通过redis设置key，以及相应过期时间等于给定时间，然后对value做判断)，算法思想是限制一定时间内用户的访问次数。

​		补充：令牌桶算法

令牌桶算法的原理是系统会以一个恒定的速度往桶里放入令牌，而如果请求需要被处理，则需要先从桶里获取一个令牌，当桶里没有令牌可取时，则拒绝服务。

### 表结构的设计

​	用户表		

​	订单表
​	商品表
​	秒杀订单表
​	秒杀商品表

#### 为什么要将秒杀订单和商品单独设计表？

原因：假如没有下面这两张秒杀表的存在（自然，就需要向上面两张表中添加关于秒杀的字段），这次我们仅仅是做了一个秒杀的业务，如果以后我们需要扩展优惠、促销等活动，还要去修改订单表的结构。

#### 数据库主键的选择？

实际使用的是mysql自增主键，但了解到数据量很庞大时不适合用自增主键以及UUID。

数据库主键如何选择，mysql自增？UUID？NO，NO。
mysql自增主键就不要想了，平时都很少用的
UUID有一些缺点，首先他相对比较长，另外UUID一般是无序的。

真正推荐的是： twitter的雪花算法(snowflake)

snowflake的结构如下(每部分用-分开):

0 - 0000000000 0000000000 0000000000 0000000000 0 - 00000 - 00000 - 000000000000

第一位为未使用，接下来的41位为毫秒级时间(时间序列)，然后是5位datacenterId和5位workerId(10位的长度机器标识） ，最后12位是毫秒内的计数（12位的计数顺序号支持每个节点每毫秒产生4096个ID序号）

## 测试

利用Jmeter做了对登录接口、商品列表接口以及秒杀接口做压力测试(接口之间无关联)，将项目分别部署在windows和linux上，分别做了压力测试。

## 压测步骤

### 1 **创建线程组**

设置线程数和循环次数。

### **2 配置元件**

配置我们需要进行测试的程序协议、地址和端口

### 3 **构造HTTP请求**

设置我们需要测试的API的请求路径和数据，登录接口的压测，利用一个脚本文件生成CSV格式用户和对应的Cookie信息，Jmeter里设置CSV文件设置，和Cookie管理器来获取CSV中的Cookie信息。

### 4 添加聚合报告和查看结果树

## 评价指标

1、Samples：表示这次测试一共发出了多少次请求，如果模拟10用户，每个用户迭代10次，那么这里显示100

3、Average：平均响应时间--默认情况下是单个Request的平均时间，当使用了Transaction Controller时，也可以以Transaction为单位显示平均响应时间

4、Median(平均值)：50%用户响应时间

5、90%Line：90%用户的响应时间

6、Min：最小响应时间

7、Max：最大响应时间

8、Error%：本次测试出现错误的请求的数量/请求总数

9、Troughput：**吞吐量**---默认情况下表示每秒完成的请求数量（Request per second）如上图所示，每秒完成的请求数为257.1个每秒

**Throughput吞吐量每秒请求的数大于并发数，则可以慢慢的往上面增加；若在压测的机器性能很好的情况下，出现吞吐量小于并发数，说明并发数不能再增加了，可以慢慢的往下减，找到最佳的并发数；**

10、KB/Sec：每秒从服务器端接收的数量，相当于Loadrunner的Throughput/Sec

## **图像超分辨率**

简单来说，图像超分的任务是将一幅低分辨率图像或图像序列恢复成高分辨率图像。

并不是导师的研究方向，只是导师阅读了几篇文献觉得挺感兴趣推荐给我，之前的师兄师姐也没有做相关方向的。

主要工作：1.阅读大量顶会文献，了解主流超分模型的设计方案。随着文献的积累，自己设计了一种对偶回归的卷积神经网络结构(基于Unet网络结构+对偶回归方案)。其中对偶回归方案比较有创新的点（对偶回归方法包含一个原始模型和一个对偶模型来同时学习两个相反的映射，除了低分辨率到高分辨率图像的映射，这是目前大部分网络模型的方案，除此之外，我引入高分辨率图像到低分辨率图像的映射，建立一个额外的监督，考虑到如果生成的高分辨率图像没有失真，那么这个高分辨率图像下采样生成的低分辨率图像和原始低分辨率图像的差异应该不大，所以设计了这样一个模型）。

2.搭建模型，因为图像超分任务对GPU要求比较高，所以还要将相关代码部署到服务器，然后就是设计完成一些对比实验。

3.评价指标没有源码实现，自己对照公式利用numpy，matplotlib实现了单幅的图片指标计算，然后利用python文件流处理，将所有输出文件的指标进行计算。

技术栈:使用pytorch搭建卷积神经网络模型，利用numpy、pandas计算图像的损失函数，进行神经网络的训练以及图像的预处理，利用matplotlib计算图像的评价指标。







# Java基础

## 面向过程与面向对象

面向过程：指的是把问题分解成一个一个步骤，每个步骤用函数实现，依次调用即可。
面向对象：指的是将问题分解成一个一个对象，通过不同对象的调用，解决问题。

* 面向过程:

  优点: 性能比面向对象高, 面向对象时类调用时需要实例化, 开销较大, 消耗资源.

  缺点: 代码可维护性较差, 不利于扩展及复用.

* 面向对象:

  优点:**易维护, 易复用, 易扩展**, 具有**封装, 继承, 多态**的特性, 可以设计出低耦合的系统, 使系统更加灵活, 易于维护.

  缺点: 性能较面向过程低.

## 封装, 继承, 多态

* 封装

  隐藏对象的属性和实现细节, 仅对外公开接口, 控制属性的读和修改权限,保护了对象的安全，简化了调用者的操作。

  SDK

* 继承

  子类继承父类的特征和行为, 使得子类对象(实例)具有父类的方法, 避免了共同特征的重复描述

  * 子类拥有父类对象的所有属性和方法, 包括私有属性和私有方法,但无法访问私有属性和方法
  * **子类可以对父类进行扩展, 拥有自己的属性和方法**
  * 子类可以用自己的方式实现父类方法

* 多态

  多态的底层实现是动态绑定，即在运行时才把方法调用与方法实现关联起来。invokevirtual (调用虚方法)和 invokeinterface(调用接口方法) 用于动态绑定，Java对于动态绑定的实现主要依赖于方法表。通过继承和接口的多态实现有所不同。

  - 继承：在方法区中找到该类的方法表，再**确认该方法在方法表中的偏移量**，找到该方法后如果被重写则直接调用，否则认为没有重写父类该方法，这时会按照继承关系搜索父类的方法表中该偏移量对应的方法。 
  - 接口：Java 允许一个类实现多个接口，从某种意义上来说相当于**多继承**，这样同一个接口的的方法在不同类方法表中的位置就可能不一样了。所以不能通过偏移量的方法，而是**通过搜索完整的方法表**。
  
  同一个行为具有不同的表现形式或形态的能力(**多态就是同一个接口, 使用不同的实例而执行不同操作**), Java中, 多态指程序中定义的**引用变量**所指向的**具体类型**和通过**该引用变量发出的方法调用**在编程时不确定, 而是在**程序运行期间**才确定, 即一个引用变量到底会指向哪个类的实例对象, 该引用变量发出的方法调用到底是哪个类中实现的方法, 必须在由程序运行期间才能决定。
  
  **实现多态的三个必要条件**: 继承, 重写, 父类引用指向子类对象(向上转型)
  
  >  当使用多态方式调用方法时, 首先检查父类中是否有该方法, 如果没有, 则编译错误；如果有, 再去调用子类的同名方法.
  
  **多态的好处**：增加灵活性，举一个简单例子，业务层(controller)调用持久层对象的方法来访问用户数据，针对不同种类数据库比如mysql、oracle等需要创建不同的持久层对象，因此持久层对象可能有多个种类，这时我们可以让这些对象实现同一个接口，然后业务层存放接口对象，用户通过set注入或者是构造器注入来确定使用哪个持久层的对象。
  
  
  
  #### SPI机制
  
  JDK内置的一种 服务提供发现机制，可以用来启用框架扩展和替换组件，比如java.sql.Driver接口，其他不同厂商可以针对同一接口做出不同的实现，MySQL和PostgreSQL都有不同的实现提供给用户，而Java的SPI机制可以为某个接口寻找服务实现。

## 重写与重载

* **重载**: 发生在同一个类中, 方法名必须相同, 参数类型不同、个数不同、顺序不同, 方法返回值和访问 修饰符可以不同.

* **重写:** 发生在运行期, 是子类对父类允许访问的方法的实现过程进行重新编写.

  * 返回值类型, 方法名, 参数列表必须相同(参数类型也必须相同，重写泛型接口中的方法编译器会产生一个桥接方法，一个实际传参的方法，因为和泛型接口方法编译后参数类型不同不符合重写规则，编译参数类型为Object的方法为桥接方法，泛型接口 中的 方法,，经过编译之后，参数类型变成了 java.lang.Object 类型), 抛出的异常小于等于父类,  返回值类型比父类小或相等, 访问修饰符大于等于父类.

    > 返回值是void或基本数据类型, 重写时不可修改, 引用类型可以返回引用类型的子类

  * 子类不能重写`private/final/static`修饰的方法, 但可以再次声明`static`方法

  

| 区别       | 重载     | 重写           |
| ---------- | -------- | -------------- |
| 范围       | 当前类   | 子类           |
| 参数列表   | 必须修改 | 不能修改       |
| 返回值类型 | 可以修改 | 比父类小或相等 |
| 异常       | 可修改   | 比父类小或相等 |
| 访问修饰符 | 可修改   | 比父类大或相等 |
| 发生阶段   | 编译器   | 运行期         |



## JVM、JRE及JDK的关系

*  JDK（Java Development Kit）是针对Java开发员的产品, 是整个Java的核心, 包括了Java运行环境JRE、Java工具和Java基础类库。
*  Java Runtime Environment（JRE）是运行JAVA程序所必须的环境的集合, 包含JVM标准实现及Java核心类库。
*  JVM是Java Virtual Machine（Java虚拟机）的缩写, 是整个java实现跨平台的最核心的部分, 能够运行以Java语言写作的软件程序。JVM 有针对不同系统的特定实现, 目的是使用相同的字节码, 它们都会给出相同的结果

> 简单来说就是JDK是Java的开发工具, JRE是Java程序运行所需的环境, JVM是Java虚拟机．它们之间的关系是JDK包含JRE和JVM, JRE包含JVM.



## JAVA语言特点

- Java是一种面向对象的语言

- Java通过Java虚拟机实现了平台无关性, 一次编译, 到处运行

- 支持多线程

- 支持网络编程

- 具有较高的安全性和可靠性

- 编译与解释并存

  Java中会将源代码通过javac编译成字节码（class文件），然后运行时JVM会将字节码解释为对应机器码，最后执行。

  在常见的HotSpot虚拟机中，为了避免解释型语言带来的执行效率低问题，采用了JIT compile(just in time compilation)技术，将运行频率很高的字节码直接编译为机器指令执行来提高性能。

### JIT

JVM读入.class文件解释后，将其发给JIT编译器。JIT编译器将字节码编译成本机机器代码。由于JIT对每条字节码都进行编译，造成了编译过程**负担过重**。为了避免这样的情况，当前的JIT仅**仅对常常运行的字节码进行编译**。



## JAVA和C++的区别

- Java 通过虚拟机从而实现跨平台特性, 但是 C++ 依赖于特定的平台。 
- Java 没有指针, 它的引用可以理解为安全指针, 而 C++ 具有和 C 一样的指针。 
- Java 支持自动垃圾回收, 而 C++ 需要手动回收。 
- Java 不支持多重继承, 只能通过实现多个接口来达到相同目的, 而 C++ 支持多重继承。



## Java的基本数据类型 

| 类型         | 关键字  | 包装器类型 | 占用内存(字节)(**重要**) | 取值范围                |  默认值  |
| ------------ | ------- | ---------- | ------------------------ | ----------------------- | :------: |
| 字节型       | byte    | Byte       | 1                        | -128(-2^7) ~ 127(2^7-1) |    0     |
| 短整型       | short   | Short      | 2                        | -2^15 ~ 2^15-1          |    0     |
| 整型         | int     | Integer    | 4                        | -2^31 ~ 2^31-1          |    0     |
| 长整型       | long    | Long       | 8                        | -2^63 ~ 2^63-1          |    0L    |
| 单精度浮点型 | float   | Float      | 4                        | 3.4e-45 ~ 1.4e38        |   0.0F   |
| 双精度浮点型 | double  | Double     | 8                        | 4.9e-324 ~ 1.8e308      |   0.0D   |
| 字符型       | char    | Character  | 2                        |                         | '\u0000' |
| 布尔型       | boolean | Boolean    | 1                        | true/false              |  false   |



## 自动装箱与拆箱

基本数据类型无法向上转型得到Object提供的方法无法参与泛型, 反射

* **装箱:使用valueOf方法将基本类型用包装器类型包装起来**
* **拆箱, 使用intValue方法将包装器类型转为基本类型**

> 通过`valueOf`方法创建`Integer`对象的时候, 如果数值在[-128,127]之间, 便返回指向`IntegerCache.cache`中已经存在的对象的引用；否则创建一个新的`Integer`对象。所以当值在[-128,127]区间时, 引用指向相同对象, 用==比较时返回true, 在区间外时, 不是同一个对象, ==返回false.



## == 和 equals 的区别

* 对于基本数据类型, `==`比较的是值；对于引用数据类型, `==`比较的是内存地址

* 对于没有重写`equals`方法的类, `equals`与`==`作用类似相同；

  对于重写过`equals`方法的类, `equals`比较的是值。



## 为什么重写equals方法后, hashCode方法也必须重

两个对象hashcode不同默认两个对象不同，实际开发中，使用equals判定为true的对象应该是相等的，如果不重写hashcode可能造成两个对象equals判定相等，但hashcode不等被认定为两个对象不等

**原因**: 

1. `hashCode()`方法默认是对堆上的对象产生独特值, 如果没有重写`hashCode()`方法, 则该类的两个对象的`hashCode`值一定不同 

2. hashcode只在Hash集合时有用:

   > **HashSet 如何检查重复**
   >
   > 当把对象加入`HashSet` 时`HashSet`会先计算对象的`hashcode`值来判断对象加⼊的位置,  同时也会与其他已经加⼊的对象的 `hashcode`值作比较, 如果没有相符的`hashcode`, `HashSet` 会假设对象没有重复出现。但是如果发现有相同`hashcode`值的对象, 这时会调用 equals() 方法来检查`hashcode` 相等的对象是否真的相同。如果两者相同, `HashSet`  就不会让其加⼊操作成功。如果不同的话, 就会重新散列到其他位置。

   如果重写equals而不重写hashcode的话, 会造成即使重写后equals返回为true的对象的hashcode仍是该对象地址的hash值, 使其在hashset比较时被判为不相同而散列到其他位置.造成hashset功能失效. 因此,  equals方法被覆盖过, 则`hashcode`方法也必须被覆盖。



## Java 中是值传递还是引用传递

**java中只有值传递**

* **值传递**：是指在调用函数时将实际参数**复制一份**传递到函数中，如果在函数中对参数进行修改，将不会影响到实际参数。 

* **引用传递**：是指在调用函数时将实际参数的**地址**直接传递到函数中，在函数中对参数所进行的修改，将影响到实际参数。

**对象传入的是当前指向对象的引用地址的值拷贝,在函数中对引用地址的改变,并不会造成函数外引用地址的改变**



## 成员变量与局部变量的区别有哪些

1.  **从语法形式上看**: 成员变量是属于类的，而局部变量是在方法中定义的变量或是方法的参数；成员变量可以被 public , private , static 等修饰符所修饰，而局部变量不能被访问控制修饰符及 static 所修饰；但是，成员变量和局部变量都能被 final 所修饰。
2.  **从变量在内存中的存储方式来看:** 如果**成员变量**是使用 static 修饰的，那么这个成员变量是属于类的，如果没有使用 static 修饰，这个成员变量是属于实例的。对象存于堆内存，如果**局部变量**类型为基本数据类型，那么存储在栈内存，如果为引用数据类型，那存放的是指向堆内存对象的引用或者是指向常量池中的地址。
3.  **从变量在内存中的生存时间上看**: 成员变量是对象的⼀部分，它随着对象的创建而存在，而局部变量随着方法的调用而自动消失。
4.  成员变量如果没有被赋初值:则会自动以类型的默认值而赋值（**被 final 修饰的成员变量也必须显式地赋值**），而局部变量则不会自动赋值。  



## String

#### String的不可变性 

在 Java 8 中, `String` 内部使用 `char` 数组存储数据。String类内部没有提供修改char数组属性的方法，同时String类被声明为`final`, 因此它不可被继承，子类无法修改value数组。

```java
private final char value[];
```

在 Java 9 之后, String 类的实现改用 byte 数组存储字符串

``` java
private final byte[] value;
```

为了节省空间, char 2B, byte 1B, 同时增加code属性, 0代表Latin-1, 1代表UTF-16

**不可变性的好处**：

1.可以缓存 `hash` 值

 因为 `String` 的`hash`值经常被使用, 例如`String` 用做 `HashMap` 的 `key`。不可变的特性可以使得 `hash`值也不可变, 因此只需要进行一次计算。

2.字符串常量池

 `String` 对象创建后, 会在字符串常量池(堆)中进行缓存, 如果下次创建同样的字符串时, 会直接返回该字符串的引用。

理解：java中常量池技术说的通俗点就是java级别的缓存技术, 方便快捷的创建一个对象。当需要一个对象时, 从池中去获取(如果池中没有, 就创建一个并放入池中), 当下次需要相同变量的时候, 不用重新创建, 从而节省空间。

3.线程安全

 `String` 不可变性(被声明为final)天生具备线程安全, 可以在多个线程中安全地使用。



## String和StringBuffer、StringBuilder的区别

 1.可变性

 `String`不可变

`StringBuilder` 与 `StringBuffer` 都继承自 `AbstractStringBuilder` 类，在 `AbstractStringBuilder` 中 也是使用字符数组保存字符串 `char[] value` 但是没有用 `final` 关键字修饰，所以这两种对象都是可变的。

 2.线程安全性

 `String`由于是不可变的，所以线程安全。

`StringBuffer`对方法加了`synchronized`同步锁或者对调用的方法加了同步锁(synchronized/lock)，所以是线程安全的。 

`StringBuilder`并没有对方法进行加同步锁，所以是非线程安全的。

3.性能

 `StringBuilder` > `StringBuffer` > `String`



## 访问修饰符

| 修饰符    | 当前类 | 同包内 | 子类(同包) | 其他包 |
| --------- | ------ | ------ | ---------- | ------ |
| public    | Y      | Y      | Y          | Y      |
| protected | Y      | Y      | Y          | N      |
| default   | Y      | Y      | N          | N      |
| private   | Y      | N      | N          | N      |

## 关键字

#### static关键字

 `static`关键字的主要用途**就是方便在没有创建对象时调用方法和变量和优化程序性能**

 **1.static变量（静态变量）**

 用`static`修饰的变量被称为静态变量，也被称为类变量，可以直接通过类名来访问它。静态变量被所有的对象共享，在内存中只有一个副本，仅当在类初次加载时会被初始化，而非静态变量在创建对象的时候被初始化，并且存在多个副本，各个对象拥有的副本互不影响。

 **2.static方法(静态方法)**

 `static`方法不依赖于任何对象就可以进行访问，在`static`方法中不能访问类的非静态成员变量和非静态成员方法，因为非静态成员方法/变量都是必须依赖具体的对象才能够被调用，但是在非静态成员方法中是可以访问静态成员方法/变量的。

```
public class Main {
    public static String s1 = "s1";//静态变量
    String s2  = "s2";
    public void fun1(){
        System.out.println(s1);
        System.out.println(s2);
    }

    public static void fun2(){
        System.out.println(s1);
        System.out.println(s2);//此处报错，静态方法不能调用非静态变量
    }
}
```

 **3.static代码块（静态代码块）**

 静态代码块的主要用途是可以用来优化程序的性能，因为它只会在类加载时加载一次，很多时候会将一些只需要进行一次的初始化操作都放在`static`代码块中进行。如果程序中有多个`static`块，在类初次被加载的时候，会按照`static`块的顺序来执行每个`static`块。

```
public class Main {
    static {
        System.out.println("hello,word");
    }
    public static void main(String[] args) {
        Main m = new Main();
    }
}
```

 **4.可以通过this访问静态成员变量吗？（可以）**

 `this`代表当前对象，可以访问静态变量，而静态方法中是不能访问非静态变量,也不能使用`this`引用。

 **5.初始化顺序**

 静态变量和静态语句块优先于实例变量和普通语句块，**静态变量和静态语句块的初始化顺序取决于它们在代码中的顺序**。如果存在继承关系的话，初始化顺序为**父类中的静态变量和静态代码块——子类中的静态变量和静态代码块——父类中的实例变量和普通代码块——父类的构造函数——子类的实例变量和普通代码块——子类的构造函数**

#### final 关键字

 `final`关键字主要用于修饰类，变量，方法。

1. 类：被`final`修饰的类不可以被继承 
2. 方法：被`final`修饰的方法不可以被重写 
3. 变量：被`final`修饰的变量是基本类型，变量的数值不能改变；被修饰的变量是引用类型，变量便不能在引用其他对象，但是变量所引用的对象本身是可以改变的。

#### final finally finalize区别

- `final`主要用于修饰类，变量，方法 
- `finally`一般作用在`try-catch`代码块中，在处理异常的时候，通常我们将一定要执行的代码方法`finally`代码块中，表示不管是否出现异常，该代码块都会执行，一般用来存放一些关闭资源的代码。 
- `finalize`是一个属于`Object`类的一个方法，该方法一般由垃圾回收器来调用，当我们调用`System.gc()`方法的时候，由垃圾回收器调用`finalize()`，回收垃圾，但Java语言规范并不保证`finalize`方法被及时地执行、而且根本不会保证它们会被执行。

#### this关键字

 1.`this`关键字可用来引用当前类的实例变量。主要用于形参与成员名字重名，用`this`来区分。

 2.`this`关键字可用于调用当前类方法。

 3.`this()`可以用来调用当前类的构造函数。(注意：`this()`一定要放在构造函数的第一行，否则编译不通过)

#### super关键字

 1.`super`可以用来引用直接父类的实例变量。和`this`类似，主要用于区分父类和子类中相同的字段

 2.`super`可以用来调用直接父类构造函数。(注意：`super()`一定要放在构造函数的第一行，否则编译不通过)

 3.`super`可以用来调用直接父类方法。

#### this与super的区别

- 相同点：
  1. `super()`和`this()`都必须在构造函数的第一行进行调用，否则就是错误的 
  2. `this()`和`super()`都指的是对象，所以，均不可以在`static`环境中使用。 
- 不同点：
  1. `super()`主要是对父类构造函数的调用，`this()`是对重载构造函数的调用 
  2. `super()`主要是在继承了父类的子类的构造函数中使用，是在不同类中的使用；`this()`主要是在同一类的不同构造函数中的使用



## 抽象类和接口的对比

1. 接口的方法默认是 public ，所有方法在接口中不能有实现(Java 8 开始接口方法可以有默认
   实现），而抽象类可以有非抽象的方法。
2. 接口中除了 static 、 final 变量，不能有其他变量，而抽象类中则不⼀定。
3. ⼀个类可以实现多个接口，但只能实现⼀个抽象类。接口自己本身可以通过 extends 关键
   字扩展多个接口。
4. 接口方法默认修饰符是 public ，抽象方法可以有 public 、 protected 和 default 这些修饰
   符（抽象方法就是为了被重写所以不能使用 private 关键字修饰！）。
5. 从设计层⾯来说，抽象是对类的抽象，是⼀种模板设计，而接口是对⾏为的抽象，是⼀种行
   为的规范。

**相同点：**

- 接口和抽象类都不能实例化 
- 都包含抽象方法，其子类都必须覆写这些抽象方法 

**不同点：**

| 类型       | 抽象类                                                       | 接口                                                   |
| ---------- | ------------------------------------------------------------ | ------------------------------------------------------ |
| 定义       | abstract class                                               | Interface                                              |
| 实现       | extends(需要提供抽象类中所有声明的方法的实现)                | implements（需要提供接口中所有声明的方法的实现）       |
| 继承       | 抽象类可以继承一个类和实现多个接口；子类只可以继承一个抽象类 | 接口只可以继承接口（一个或多个）；子类可以实现多个接口 |
| 访问修饰符 | 抽象方法可以有public、protected和default这些修饰符           | 接口方法默认修饰符是public。你不可以使用其它修饰符     |
| 构造器     | 抽象类可以有构造器                                           | 接口不能有构造器                                       |
| 字段声明   | 抽象类的字段声明可以是任意的                                 | 接口的字段默认都是 static 和 final 的                  |

## BIO,NIO,AIO 有什么区别?

- **BIO (Blocking I/O)：**服务器实现模式为一个连接一个线程，即**客户端有连接请求时服务器就需要启动一个线程进行处理**，**并且数据的读取写⼊必须阻塞在⼀个线程内等待其完成**,如果这个连接不做任何事情会造成不必要的线程开销，可以通过线程池机制来改善。BIO方式适用于连接数目比较小且固定的架构，这种方式对服务端资源要求比较高，并发局限于应用中，在jdk1.4以前是唯一的io

- **NIO (New I/O)：**服务器实现模式为一个请求一个线程，即客户端发送的连接请求都会注册到**多路复用器**上，**多路复用器轮询**到连接**有IO请求**时才启动一个线程进行处理。NIO方式适用于连接数目多且连接比较短（轻操作）的架构，比如聊天服务器，并发局限于应用中，编程比较复杂，jdk1,4开始支持

- **AIO (Asynchronous I/O)：**服务器实现模式为一个有效请求一个线程，客户端的**IO请求都是由操作系统先完成了再通知服务器用其启动线程进行处理**。AIO方式适用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分调用OS参与并发操作，编程比较复杂，jdk1.7开始支持。

  

- BIO (Blocking I/O): **同步阻塞 I/O 模式，。**在活动连接数不是特别高（小于单机 1000）的情况下，这种模型是比较不错的，可以让每⼀个连接专注于自己的 I/O 并且编程模型简单，也不用过多考虑系统的过载、限流等问题。线程池本身就是⼀个天然的漏斗，可以缓冲⼀些系统处理不了的连接或请求。但是，当⾯对⼗万甚⾄百万级连接的时候，传统的 BIO 模型是⽆能为⼒的。因此，我们需要⼀种更高效的 I/O 处理模型来应对更⾼的并发量。

- NIO (Non-blocking/New I/O): NIO 是⼀种同步非阻塞的 I/O 模型，在 Java 1.4 中引⼊了NIO 框架，对应 java.nio 包，提供了 Channel , Selector， Buffer 等抽象。 NIO 中的 N 可以理解为 Non-blocking，不单纯是 New。它支持面向缓冲的，基于通道的 I/O 操作⽅法。NIO 提供了与传统 BIO 模型中的 Socket 和 ServerSocket 相对应的 SocketChannel 和ServerSocketChannel 两种不同的套接字通道实现,两种通道都⽀持阻塞和非阻塞两种模式。阻塞模式使用就像传统中的⽀持⼀样，比较简单，但是性能和可靠性都不好；非阻塞模式正好与之相反。对于低负载、低并发的应用程序，可以使用同步阻塞 I/O 来提升开发速率和更好的维护性；对于高负载、高并发的（网络）应用，应使用 NIO 的非阻塞模式来开发

- AIO (Asynchronous I/O): AIO 也就是 NIO 2。在 Java 7 中引⼊了 NIO 的改进版 NIO 2,它是异步非阻塞的 IO 模型。异步 IO 是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那⾥，当后台处理完成，操作系统会通知相应的线程进⾏后续的操作。

### select poll epoll 

共同点：select，poll，epoll都是IO多路复用机制，即可以监视多个描述符，一旦某个描述符就绪（读或写就绪），能够通知程序进行相应读写操作。

不同点:

**select**--适合实时性要求比较高的场景
	**fd_set 使用数组实现**，单个进程可监视的fd数量被限制  
		1.fd_size 有限制 1024 bitmap
			fd【i】 = accept()
		2**.fdset不可重用**，新的fd进来，重新创建
		3.**用户态和内核态拷贝（fdset）产生开销**
		4.O(n)时间复杂度的二次遍历，第一次内核遍历(修改fd的标志表示写入了数据)，第二次程序遍历fd_set查看哪些fd中写入了内容
		

**poll**--没有最大描述符数量的限制，如果平台支持并且对实时性要求不高
	基于结构体存储fd，**没有最大文件描述符数量的限制**
	struct pollfd{
		int fd;
		short events;
		short revents; //可重用(不同于select标志位置位，每次读更改结构体中的标志位)
	}
	解决了select的1,2两点缺点

3.**用户态和内核态拷贝（fdset）产生开销**
4.O(n)时间复杂度的二次遍历，第一次内核遍历(修改fd的标志表示写入了数据)，第二次程序遍历fd_set查看哪些fd中写入了内容



**epoll-**-有大量的描述符需要同时轮询，并且描述符状态变化少，变化多了对描述符的状态改变都需要通过epoll_ctl()进行系统调用，频繁的系统调用降低了效率。
	
	不需要轮询，时间复杂度为O(1)
	epoll_create  创建一个白板 存放fd_events
	epoll_ctl 用于向内核注册新的描述符或者是改变某个文件描述符的状态。已注册的描述符在内核中会被维护在一棵红黑树上
	epoll_wait 通过回调函数内核会将 I/O 准备好的描述符加入到一个链表中管理，进程调用 epoll_wait() 检查是否有事件发生时，只需要检查链表中是否有元素即可。如果不为空，则把发生的事件复制到用户态，同时将事件数量返回给用户。

LT模式：当epoll_wait()检查到描述符事件到达时，将此事件通知进行，进程可以不立即处理该事件。

ET模型：和LT模式不同的是，通知之后进程必须立即处理事件，减少了epoll事件被重复触发的次数

## 反射

JAVA反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称为java语言的反射机制。

**反射就是在运行时才知道要操作的类是什么，并且可以在运行时获取类的完整构造，并调用对应的方法**

Java获取Class对象的三种方式

```
class Person {
    public String name = "zhangsan";
    public Person() {
    }
}

public class Main{
    public static void main(String[] args) throws ClassNotFoundException {
        //方式1
         Person p1 = new Person();
         Class c1 = p1.getClass();
         //方式2
        Class c2 = Person.class;
        //方式3可能会抛出ClassNotFoundException异常(推荐使用)
        Class c3 = Class.forName("com.company");
    }
}
```

因为在一个类在 JVM 中只会有一个 `Class` 实例，所以对`c1`、`c2`、`c3`进行`equals`比较时返回的都是`true`。

**反射优缺点：**

- 优点：运行期类型的判断，动态加载类，提高代码灵活度。 
- 缺点：性能稍差，不够安全，比如不受泛型的安全检查。 

**反射应用场景：**

1. Java的很多框架都用到了反射，例如`Spring`中的xml的配置模式等 
2. 动态代理设计模式也采用了反射机制

## JAVA异常（Exception）

### 异常种类

- 在 Java 中，所有的异常、Error都有一个共同的祖先 `java.lang` 包中的 `Throwable` 类。`Throwable` 类有两个重要的子类:

  - **`Exception`** :程序本身可以处理的异常，可以通过 `catch` 来进行捕获。`Exception` 又可以分为 Checked Exception (受检查异常，必须处理) 和 Unchecked Exception (不受检查异常，可以不处理)。

    - **Checked Exception** 即受检查异常，Java 代码在编译过程中，如果受检查异常没有被 `catch`/`throw` 处理的话，就没办法通过编译 。常见的受检查异常有： IO 相关的异常、`ClassNotFoundException`

    - **Unchecked Exception** 即 **不受检查异常** ，Java 代码在编译过程中 ，我们即使不处理不受检查异常也可以正常通过编译。`RuntimeException` 及其子类都统称为非受检查异常，例如：`NullPointerException`、`NumberFormatException`（字符串转换为数字）、`ArrayIndexOutOfBoundsException`（数组越界）、`ClassCastException`（类型转换错误）、`ArithmeticException`（算术错误）等。
- **`Error`** ：`Error` 属于程序无法处理的错误 ，我们没办法通过 `catch` 来进行捕获不建议通过`catch`捕获 。例如Java 虚拟机运行错误（`Virtual MachineError`）、**虚拟机内存不够错误**(`OutOfMemoryError`)、类定义错误（`NoClassDefFoundError`）等 。这些异常发生时，Java 虚拟机（JVM）一般会选择线程终止

### Java异常处理方式

<img src="C:/Users/y/Desktop/春招笔记/images/image-20201109173809704.png"  />

Java 通过面向对象的方法进行异常处理，一旦方法抛出异常，系统自动根据该异常对象寻找合适异常处理器（Exception Handler）来处理该异常，把各种不同的异常进行分类，并提供了良好的接口。在 Java 中，每个异常都是一个对

象，它是 Throwable 类或其子类的实例。当一个方法出现异常后便抛出一个异常对象，该对象中包含有异常信息，调用这个对象的方法可以捕获到这个异常并可以对其进行处理。Java 的异常处理是通过 5 个关键词来实现的：try、 catch、throw、throws 和 finally。

在Java应用中，异常的处理机制分为声明异常，抛出异常和捕获异常。

throw和throws的区别：
（1）位置不同：
throw：方法内部
throws: 方法的签名处，方法的声明处

（2）内容不同：
throw+异常对象（检查异常，运行时异常）
throws+异常的类型（可以多个类型，用，拼接）

（3）作用不同：
throw：异常出现的源头，制造异常。
throws:在方法的声明处，告诉方法的调用者，这个方法中可能会出现我声明的这些异常。然后调用者对这个异常进行处理：
要么自己处理要么再继续向外抛出异常

#### 1.throws声明异常

通常，应该捕获那些知道如何处理的异常，将不知道如何处理的异常继续传递下

去。传递异常可以在**方法签名处使用 throws 关键字声明可能会抛出的异常**。注意

非检查异常（Error、RuntimeException 或它们的子类）不可使用 throws 关键字来声明要抛出的异常。

​       一个方法出现编译时异常，就需要 try-catch/ throws 处理，否则会导致编译错误

**编译通过，不对异常处理运行期间仍无法正常运行。**

#### 2.throw抛出异常

如果你觉得解决不了某些异常问题，且不需要调用者处理，那么你可以抛出异常。 throw关键字作用是在**方法内部**抛出一个Throwable类型的异常。任何Java代码都可以通过throw语句抛出异常。

#### 3.trycatch捕获异常

程序通常在运行之前不报错，但是运行后可能会出现某些未知的错误，但是还不想直接抛出到上一级，那么就需要通过try…catch…的形式进行异常捕获，之后根据不同的异常情况来进行相应的处理。如何选择异常类型

可以根据下图来选择是捕获异常，声明异常还是抛出异常

<img src="C:/Users/y/Desktop/春招笔记/images/image-20201109173855168.png"  />

###  自定义异常在生产中如何应用(继承Exception类) 

 Java虽然提供了丰富的异常处理类，但是在项目中还会经常使用自定义异常，其主要原因是Java提供的异常类在某些情况下还是不能满足实际需球。例如以下情况：
  1、**系统中有些错误是符合Java语法，但不符合业务逻辑**。

  2、在分层的软件结构中，通常是在表现层统一对系统其他层次的异常进行捕获处理。

## JAVA泛型

Java 泛型是 JDK 5 中引入的一个新特性, 泛型提供了编译时类型安全检测机制，该机制允许程序员在编译时检测到非法的类型。泛型的本质是参数化类型，也就是说所操作的数据类型被指定为一个参数。

- 泛型擦除（这是面试考察泛型时经常问到的问题）

  Java的泛型基本上都是在编译器这个层次上实现的，在生成的字节码中是不包含泛型中的类型信息的，使用泛型的时候加上类型参数，在编译器编译的时候会去掉，这个过程成为类型擦除。看下面代码

  ```
  public class Main{
      public static void main(String[] args) {
          ArrayList<integer> arrayList1 = new ArrayList<>();
          ArrayList<string> arrayList2 = new ArrayList<>();
  
          System.out.println(arrayList1.getClass() == arrayList2.getClass());
      }
  }
  ```

  输出结果

  ```
  true
  ```

  可以看到`ArrayList<integer>`和`ArrayList<string>`的原始类型是相同，在编译成字节码文件后都会变成`List`，JVM看到的只有`List`，看不到泛型信息，这就是泛型的类型擦除。在看下面这段代码

  ```
  public class Main{
      public static void main(String[] args) throws Exception {
          ArrayList<integer> arrayList = new ArrayList<>();
          arrayList.add(1);
          arrayList.getClass().getMethod("add", Object.class).invoke(arrayList, "a");
          System.out.println(arrayList.get(0));
          System.out.println(arrayList.get(1));
      }
  }
  ```

  ```
  1
  a
  ```

  可以看到通过反射进行`add`操作，`ArrayList<integer>`竟然可以存储字符串，这是因为在反射就是在运行期调用的`add`方法，在运行期泛型信息已经被擦除。

- 既然存在类型擦除，那么Java是如何保证在`ArrayList<integer>`添加字符串会报错呢？

  Java编译器是通过先检查代码中泛型的类型，然后在进行类型擦除，再进行编译。

## JAVA序列化

**序列化**： 将数据结构或对象转换成二进制字节流的过程
**反序列化**：将在序列化过程中所生成的二进制字节流转换成数据结构或者对象的过程
序列化的主要目的是通过网络传输对象或者说是将对象存储到文件系统、数据库、内存中。

- 序列化的意义：将Java对象转换成字节序列，这些字节序列更加便于通过网络传输或存储在磁盘上，在需要时可以通过反序列化恢复成原来的对象。
- 实现方式：
  1. 实现**Serializable**接口 
  2. 实现**Externalizable**接口 
- 序列化的注意事项：
  1. 对象的类名、实例变量会被序列化；方法、类变量、`transient`实例变量都不会被序列化。 
  2. 想要某个变量不被序列化，可以使用`transient`修饰。 
  3. 序列化对象的引用类型成员变量，也必须是可序列化的，否则，会报错。 
  4. 反序列化时必须有序列化对象的`class`文件。

## 深拷贝与浅拷贝

- 深拷贝：对基本数据类型进行值传递，对引用数据类型，创建一个新的对象，并复制其内容，两个引用指向两个对象，但对象内容相同。
- 浅拷贝：对基本数据类型进行值传递，A类对象中存在属性B类对象，浅拷贝A类的实例对象，会在堆中新建一个A类对象，但是新建对象中的B类属性对象和原A类对象中的B类属性对象地址相同。

## 常见的Object方法

\> 这些方法都很重要，面试经常会问到，要结合其他知识将这些方法理解透彻

- `Object clone()`：创建与该对象的类相同的新对象 
- `boolean equals(Object)`：比较两对象是否相等 
- `void finalize()`：当垃圾回收器确定不存在对该对象的更多引用时，对象垃圾回收器调用该方法 
- `Class getClass()`：返回一个对象运行时的实例类 
- `int hashCode()`：返回该对象的散列码值 
- `void notify()`：唤醒等待在该对象的监视器上的一个线程 
- `void notifyAll()`：唤醒等待在该对象的监视器上的全部线程 
- `String toString()`：返回该对象的字符串表示 
- `void wait()`：在其他线程调用此对象的 `notify()` 方法或 `notifyAll()`方法前，导致当前线程等待

##  jdk1.8的新特性有哪些 

#### 一、接口的默认方法

Java 8允许我们给接口添加一个非抽象的方法实现，只需要使用 default关键字即可，这个特征又叫做扩展方法，示例如下：

代码如下:

interface Formula { double calculate(int a);

default double sqrt(int a) { return Math.sqrt(a); } }

Formula接口在拥有calculate方法之外同时还定义了sqrt方法，实现了Formula接口的子类只需要实现一个calculate方法，默认方法sqrt将在子类上可以直接使用。

代码如下:

Formula formula = new Formula() { @Override public double calculate(int a) { return sqrt(a * 100); } };

formula.calculate(100); // 100.0 formula.sqrt(16); // 4.0

文中的formula被实现为一个匿名类的实例，该代码非常容易理解，6行代码实现了计算 sqrt(a * 100)。在下一节中，我们将会看到实现单方法接口的更简单的做法。

译者注： 在Java中只有单继承，如果要让一个类赋予新的特性，通常是使用接口来实现，在C++中支持多继承，允许一个子类同时具有多个父类的接口与功能，在其他语言中，让一个类同时具有其他的可复用代码的方法叫做mixin。新的Java 8 的这个特新在编译器实现的角度上来说更加接近Scala的trait。 在C#中也有名为扩展方法的概念，允许给已存在的类型扩展方法，和Java 8的这个在语义上有差别。

#### 二、Lambda 表达式

首先看看在老版本的Java中是如何排列字符串的：

代码如下:

List<String> names = Arrays.asList("peterF", "anna", "mike", "xenia");

Collections.sort(names, new Comparator<String>() { @Override public int compare(String a, String b) { return b.compareTo(a); } });

只需要给静态方法 Collections.sort 传入一个List对象以及一个比较器来按指定顺序排列。通常做法都是创建一个匿名的比较器对象然后将其传递给sort方法。

在Java 8 中你就没必要使用这种传统的匿名对象的方式了，Java 8提供了更简洁的语法，lambda表达式：

代码如下:

Collections.sort(names, (String a, String b) -> { return b.compareTo(a); });

看到了吧，代码变得更段且更具有可读性，但是实际上还可以写得更短：

代码如下:

Collections.sort(names, (String a, String b) -> b.compareTo(a));

对于函数体只有一行代码的，你可以去掉大括号{}以及return关键字，但是你还可以写得更短点：

代码如下:

Collections.sort(names, (a, b) -> b.compareTo(a));

Java编译器可以自动推导出参数类型，所以你可以不用再写一次类型。接下来我们看看lambda表达式还能作出什么更方便的东西来：

#### 三、函数式接口

Lambda表达式是如何在java的类型系统中表示的呢？每一个lambda表达式都对应一个类型，通常是接口类型。而“函数式接口”是指仅仅只包含一个抽象方法的接口，每一个该类型的lambda表达式都会被匹配到这个抽象方法。因为 默认方法 不算抽象方法，所以你也可以给你的函数式接口添加默认方法。

我们可以将lambda表达式当作任意只包含一个抽象方法的接口类型，确保你的接口一定达到这个要求，你只需要给你的接口添加 @FunctionalInterface 注解，编译器如果发现你标注了这个注解的接口有多于一个抽象方法的时候会报错的。

示例如下：

代码如下:

@FunctionalInterface interface Converter<F, T> { T convert(F from); } Converter<String, Integer> converter = (from) -> Integer.valueOf(from); Integer converted = converter.convert("123"); System.out.println(converted); // 123

需要注意如果@FunctionalInterface如果没有指定，上面的代码也是对的。

译者注 将lambda表达式映射到一个单方法的接口上，这种做法在Java 8之前就有别的语言实现，比如Rhino JavaScript解释器，如果一个函数参数接收一个单方法的接口而你传递的是一个function，Rhino 解释器会自动做一个单接口的实例到function的适配器，典型的应用场景有 org.w3c.dom.events.EventTarget 的addEventListener 第二个参数 EventListener。

#### 四、方法与构造函数引用

前一节中的代码还可以通过静态方法引用来表示：

代码如下:

Converter<String, Integer> converter = Integer::valueOf; Integer converted = converter.convert("123"); System.out.println(converted); // 123

Java 8 允许你使用 :: 关键字来传递方法或者构造函数引用，上面的代码展示了如何引用一个静态方法，我们也可以引用一个对象的方法：

代码如下:

converter = something::startsWith; String converted = converter.convert("Java"); System.out.println(converted); // "J"

接下来看看构造函数是如何使用::关键字来引用的，首先我们定义一个包含多个构造函数的简单类：

代码如下:

class Person { String firstName; String lastName;

Person() {}

Person(String firstName, String lastName) { this.firstName = firstName; this.lastName = lastName; } }

接下来我们指定一个用来创建Person对象的对象工厂接口：

代码如下:

interface PersonFactory<P extends Person> { P create(String firstName, String lastName); }

这里我们使用构造函数引用来将他们关联起来，而不是实现一个完整的工厂：

代码如下:

PersonFactory<Person> personFactory = Person::new; Person person = personFactory.create("Peter", "Parker");

我们只需要使用 Person::new 来获取Person类构造函数的引用，Java编译器会自动根据PersonFactory.create方法的签名来选择合适的构造函数。

#### 五、Lambda 作用域

在lambda表达式中访问外层作用域和老版本的匿名对象中的方式很相似。你可以直接访问标记了final的外层局部变量，或者实例的字段以及静态变量。

#### 六、访问局部变量

我们可以直接在lambda表达式中访问外层的局部变量：

代码如下:

final int num = 1; Converter<Integer, String> stringConverter = (from) -> String.valueOf(from + num);

stringConverter.convert(2); // 3

但是和匿名对象不同的是，这里的变量num可以不用声明为final，该代码同样正确：

代码如下:

int num = 1; Converter<Integer, String> stringConverter = (from) -> String.valueOf(from + num);

stringConverter.convert(2); // 3

不过这里的num必须不可被后面的代码修改（即隐性的具有final的语义），例如下面的就无法编译：

代码如下:

int num = 1; Converter<Integer, String> stringConverter = (from) -> String.valueOf(from + num); num = 3;

在lambda表达式中试图修改num同样是不允许的。

#### 七、访问对象字段与静态变量

和本地变量不同的是，lambda内部对于实例的字段以及静态变量是即可读又可写。该行为和匿名对象是一致的：

代码如下:

class Lambda4 { static int outerStaticNum; int outerNum;

void testScopes() { Converter<Integer, String> stringConverter1 = (from) -> { outerNum = 23; return String.valueOf(from); };

Converter<Integer, String> stringConverter2 = (from) -> {  outerStaticNum = 72;  return String.valueOf(from);  }; } }

#### 八、访问接口的默认方法

还记得第一节中的formula例子么，接口Formula定义了一个默认方法sqrt可以直接被formula的实例包括匿名对象访问到，但是在lambda表达式中这个是不行的。 Lambda表达式中是无法访问到默认方法的，以下代码将无法编译：

代码如下:

Formula formula = (a) -> sqrt( a * 100); Built-in Functional Interfaces

JDK 1.8 API包含了很多内建的函数式接口，在老Java中常用到的比如Comparator或者Runnable接口，这些接口都增加了@FunctionalInterface注解以便能用在lambda上。 Java 8 API同样还提供了很多全新的函数式接口来让工作更加方便，有一些接口是来自Google Guava库里的，即便你对这些很熟悉了，还是有必要看看这些是如何扩展到lambda上使用的。

**Predicate****接口**

Predicate 接口只有一个参数，返回boolean类型。该接口包含多种默认方法来将Predicate组合成其他复杂的逻辑（比如：与，或，非）：

代码如下:

Predicate<String> predicate = (s) -> s.length() > 0;

predicate.test("foo"); // true predicate.negate().test("foo"); // false

Predicate<Boolean> nonNull = Objects::nonNull; Predicate<Boolean> isNull = Objects::isNull;

Predicate<String> isEmpty = String::isEmpty; Predicate<String> isNotEmpty = isEmpty.negate();

**Function** **接口**

Function 接口有一个参数并且返回一个结果，并附带了一些可以和其他函数组合的默认方法（compose, andThen）：

代码如下:

Function<String, Integer> toInteger = Integer::valueOf; Function<String, String> backToString = toInteger.andThen(String::valueOf);

backToString.apply("123"); // "123"

**Supplier** **接口** Supplier 接口返回一个任意范型的值，和Function接口不同的是该接口没有任何参数

代码如下:

Supplier<Person> personSupplier = Person::new; personSupplier.get(); // new Person

**Consumer** **接口** Consumer 接口表示执行在单个参数上的操作。

代码如下:

Consumer<Person> greeter = (p) -> System.out.println("Hello, " + p.firstName); greeter.accept(new Person("Luke", "Skywalker"));

**Comparator** **接口** Comparator 是老Java中的经典接口， Java 8在此之上添加了多种默认方法：

代码如下:

Comparator<Person> comparator = (p1, p2) -> p1.firstName.compareTo(p2.firstName);

Person p1 = new Person("John", "Doe"); Person p2 = new Person("Alice", "Wonderland");

comparator.compare(p1, p2); // > 0 comparator.reversed().compare(p1, p2); // < 0

**Optional** **接口**

Optional 不是函数是接口，这是个用来防止NullPointerException异常的辅助类型，这是下一届中将要用到的重要概念，现在先简单的看看这个接口能干什么：

Optional 被定义为一个简单的容器，其值可能是null或者不是null。在Java 8之前一般某个函数应该返回非空对象但是偶尔却可能返回了null，而在Java 8中，不推荐你返回null而是返回Optional。

代码如下:

Optional<String> optional = Optional.of("bam");

optional.isPresent(); // true optional.get(); // "bam" optional.orElse("fallback"); // "bam"

optional.ifPresent((s) -> System.out.println(s.charAt(0))); // "b"

**Stream** **接口**

java.util.Stream 表示能应用在一组元素上一次执行的操作序列。Stream 操作分为中间操作或者最终操作两种，最终操作返回一特定类型的计算结果，而中间操作返回Stream本身，这样你就可以将多个操作依次串起来。Stream 的创建需要指定一个数据源，比如 java.util.Collection的子类，List或者Set， Map不支持。Stream的操作可以串行执行或者并行执行。

首先看看Stream是怎么用，首先创建实例代码的用到的数据List：

代码如下:

List<String> stringCollection = new ArrayList<>(); stringCollection.add("ddd2"); stringCollection.add("aaa2"); stringCollection.add("bbb1"); stringCollection.add("aaa1"); stringCollection.add("bbb3"); stringCollection.add("ccc"); stringCollection.add("bbb2"); stringCollection.add("ddd1");

Java 8扩展了集合类，可以通过 Collection.stream() 或者 Collection.parallelStream() 来创建一个Stream。下面几节将详细解释常用的Stream操作：

**Filter** **过滤**

过滤通过一个predicate接口来过滤并只保留符合条件的元素，该操作属于中间操作，所以我们可以在过滤后的结果来应用其他Stream操作（比如forEach）。forEach需要一个函数来对过滤后的元素依次执行。forEach是一个最终操作，所以我们不能在forEach之后来执行其他Stream操作。

代码如下:

stringCollection .stream() .filter((s) -> s.startsWith("a")) .forEach(System.out::println);

// "aaa2", "aaa1"

**Sort** **排序**

排序是一个中间操作，返回的是排序好后的Stream。如果你不指定一个自定义的Comparator则会使用默认排序。

代码如下:

stringCollection .stream() .sorted() .filter((s) -> s.startsWith("a")) .forEach(System.out::println);

// "aaa1", "aaa2"

需要注意的是，排序只创建了一个排列好后的Stream，而不会影响原有的数据源，排序之后原数据stringCollection是不会被修改的：

代码如下:

System.out.println(stringCollection); // ddd2, aaa2, bbb1, aaa1, bbb3, ccc, bbb2, ddd1

**Map** **映射** 中间操作map会将元素根据指定的Function接口来依次将元素转成另外的对象，下面的示例展示了将字符串转换为大写字符串。你也可以通过map来讲对象转换成其他类型，map返回的Stream类型是根据你map传递进去的函数的返回值决定的。

代码如下:

stringCollection .stream() .map(String::toUpperCase) .sorted((a, b) -> b.compareTo(a)) .forEach(System.out::println);

// "DDD2", "DDD1", "CCC", "BBB3", "BBB2", "AAA2", "AAA1"

**Match** **匹配**

Stream提供了多种匹配操作，允许检测指定的Predicate是否匹配整个Stream。所有的匹配操作都是最终操作，并返回一个boolean类型的值。

代码如下:

boolean anyStartsWithA = stringCollection .stream() .anyMatch((s) -> s.startsWith("a"));

System.out.println(anyStartsWithA); // true

boolean allStartsWithA = stringCollection .stream() .allMatch((s) -> s.startsWith("a"));

System.out.println(allStartsWithA); // false

boolean noneStartsWithZ = stringCollection .stream() .noneMatch((s) -> s.startsWith("z"));

System.out.println(noneStartsWithZ); // true

**Count** **计数** 计数是一个最终操作，返回Stream中元素的个数，返回值类型是long。

代码如下:

long startsWithB = stringCollection .stream() .filter((s) -> s.startsWith("b")) .count();

System.out.println(startsWithB); // 3

**Reduce** **规约**

这是一个最终操作，允许通过指定的函数来讲stream中的多个元素规约为一个元素，规越后的结果是通过Optional接口表示的：

代码如下:

Optional<String> reduced = stringCollection .stream() .sorted() .reduce((s1, s2) -> s1 + "#" + s2);

reduced.ifPresent(System.out::println); // "aaa1#aaa2#bbb1#bbb2#bbb3#ccc#ddd1#ddd2"

**并行****Streams**

前面提到过Stream有串行和并行两种，串行Stream上的操作是在一个线程中依次完成，而并行Stream则是在多个线程上同时执行。

下面的例子展示了是如何通过并行Stream来提升性能：

首先我们创建一个没有重复元素的大表：

代码如下:

int max = 1000000; List<String> values = new ArrayList<>(max); for (int i = 0; i < max; i++) { UUID uuid = UUID.randomUUID(); values.add(uuid.toString()); }

然后我们计算一下排序这个Stream要耗时多久， 串行排序：

代码如下:

long t0 = System.nanoTime();

long count = values.stream().sorted().count(); System.out.println(count);

long t1 = System.nanoTime();

long millis = TimeUnit.NANOSECONDS.toMillis(t1 - t0); System.out.println(String.format("sequential sort took: %d ms", millis));

// 串行耗时: 899 ms 并行排序：

代码如下:

long t0 = System.nanoTime();

long count = values.parallelStream().sorted().count(); System.out.println(count);

long t1 = System.nanoTime();

long millis = TimeUnit.NANOSECONDS.toMillis(t1 - t0); System.out.println(String.format("parallel sort took: %d ms", millis));

// 并行排序耗时: 472 ms 上面两个代码几乎是一样的，但是并行版的快了50%之多，唯一需要做的改动就是将stream()改为parallelStream()。

**Map**

前面提到过，Map类型不支持stream，不过Map提供了一些新的有用的方法来处理一些日常任务。

代码如下:

Map<Integer, String> map = new HashMap<>();

for (int i = 0; i < 10; i++) { map.putIfAbsent(i, "val" + i); }

map.forEach((id, val) -> System.out.println(val)); 以上代码很容易理解， putIfAbsent 不需要我们做额外的存在性检查，而forEach则接收一个Consumer接口来对map里的每一个键值对进行操作。

下面的例子展示了map上的其他有用的函数：

代码如下:

map.computeIfPresent(3, (num, val) -> val + num); map.get(3); // val33

map.computeIfPresent(9, (num, val) -> null); map.containsKey(9); // false

map.computeIfAbsent(23, num -> "val" + num); map.containsKey(23); // true

map.computeIfAbsent(3, num -> "bam"); map.get(3); // val33

接下来展示如何在Map里删除一个键值全都匹配的项：

代码如下:

map.remove(3, "val3"); map.get(3); // val33

map.remove(3, "val33"); map.get(3); // null

另外一个有用的方法：

代码如下:

map.getOrDefault(42, "not found"); // not found

对Map的元素做合并也变得很容易了：

代码如下:

map.merge(9, "val9", (value, newValue) -> value.concat(newValue)); map.get(9); // val9

map.merge(9, "concat", (value, newValue) -> value.concat(newValue)); map.get(9); // val9concat

Merge做的事情是如果键名不存在则插入，否则则对原键对应的值做合并操作并重新插入到map中。

#### 九、Date API

Java 8 在包java.time下包含了一组全新的时间日期API。新的日期API和开源的Joda-Time库差不多，但又不完全一样，下面的例子展示了这组新API里最重要的一些部分：

**Clock** **时钟**

Clock类提供了访问当前日期和时间的方法，Clock是时区敏感的，可以用来取代 System.currentTimeMillis() 来获取当前的微秒数。某一个特定的时间点也可以使用Instant类来表示，Instant类也可以用来创建老的java.util.Date对象。

代码如下:

Clock clock = Clock.systemDefaultZone(); long millis = clock.millis();

Instant instant = clock.instant(); Date legacyDate = Date.from(instant); // legacy java.util.Date

**Timezones** **时区**

在新API中时区使用ZoneId来表示。时区可以很方便的使用静态方法of来获取到。 时区定义了到UTS时间的时间差，在Instant时间点对象到本地日期对象之间转换的时候是极其重要的。

代码如下:

System.out.println(ZoneId.getAvailableZoneIds()); // prints all available timezone ids

ZoneId zone1 = ZoneId.of("Europe/Berlin"); ZoneId zone2 = ZoneId.of("Brazil/East"); System.out.println(zone1.getRules()); System.out.println(zone2.getRules());

// ZoneRules[currentStandardOffset=+01:00] // ZoneRules[currentStandardOffset=-03:00]

**LocalTime** **本地时间**

LocalTime 定义了一个没有时区信息的时间，例如 晚上10点，或者 17:30:15。下面的例子使用前面代码创建的时区创建了两个本地时间。之后比较时间并以小时和分钟为单位计算两个时间的时间差：

代码如下:

LocalTime now1 = LocalTime.now(zone1); LocalTime now2 = LocalTime.now(zone2);

System.out.println(now1.isBefore(now2)); // false

long hoursBetween = ChronoUnit.HOURS.between(now1, now2); long minutesBetween = ChronoUnit.MINUTES.between(now1, now2);

System.out.println(hoursBetween); // -3 System.out.println(minutesBetween); // -239

LocalTime 提供了多种工厂方法来简化对象的创建，包括解析时间字符串。

代码如下:

LocalTime late = LocalTime.of(23, 59, 59); System.out.println(late); // 23:59:59

DateTimeFormatter germanFormatter = DateTimeFormatter .ofLocalizedTime(FormatStyle.SHORT) .withLocale(Locale.GERMAN);

LocalTime leetTime = LocalTime.parse("13:37", germanFormatter); System.out.println(leetTime); // 13:37

**LocalDate** **本地日期**

LocalDate 表示了一个确切的日期，比如 2014-03-11。该对象值是不可变的，用起来和LocalTime基本一致。下面的例子展示了如何给Date对象加减天/月/年。另外要注意的是这些对象是不可变的，操作返回的总是一个新实例。

代码如下:

LocalDate today = LocalDate.now(); LocalDate tomorrow = today.plus(1, ChronoUnit.DAYS); LocalDate yesterday = tomorrow.minusDays(2);

LocalDate independenceDay = LocalDate.of(2014, Month.JULY, 4); DayOfWeek dayOfWeek = independenceDay.getDayOfWeek();

System.out.println(dayOfWeek); // FRIDAY 从字符串解析一个LocalDate类型和解析LocalTime一样简单：

代码如下:

DateTimeFormatter germanFormatter = DateTimeFormatter .ofLocalizedDate(FormatStyle.MEDIUM) .withLocale(Locale.GERMAN);

LocalDate xmas = LocalDate.parse("24.12.2014", germanFormatter); System.out.println(xmas); // 2014-12-24

**LocalDateTime** **本地日期时间**

LocalDateTime 同时表示了时间和日期，相当于前两节内容合并到一个对象上了。LocalDateTime和LocalTime还有LocalDate一样，都是不可变的。LocalDateTime提供了一些能访问具体字段的方法。

代码如下:

LocalDateTime sylvester = LocalDateTime.of(2014, Month.DECEMBER, 31, 23, 59, 59);

DayOfWeek dayOfWeek = sylvester.getDayOfWeek(); System.out.println(dayOfWeek); // WEDNESDAY

Month month = sylvester.getMonth(); System.out.println(month); // DECEMBER

long minuteOfDay = sylvester.getLong(ChronoField.MINUTE_OF_DAY); System.out.println(minuteOfDay); // 1439

只要附加上时区信息，就可以将其转换为一个时间点Instant对象，Instant时间点对象可以很容易的转换为老式的java.util.Date。

代码如下:

Instant instant = sylvester .atZone(ZoneId.systemDefault()) .toInstant();

Date legacyDate = Date.from(instant); System.out.println(legacyDate); // Wed Dec 31 23:59:59 CET 2014

格式化LocalDateTime和格式化时间和日期一样的，除了使用预定义好的格式外，我们也可以自己定义格式：

代码如下:

DateTimeFormatter formatter = DateTimeFormatter .ofPattern("MMM dd, yyyy - HH:mm");

LocalDateTime parsed = LocalDateTime.parse("Nov 03, 2014 - 07:13", formatter); String string = formatter.format(parsed); System.out.println(string); // Nov 03, 2014 - 07:13

和java.text.NumberFormat不一样的是新版的DateTimeFormatter是不可变的，所以它是线程安全的。

#### 十、Annotation 注解

在Java 8中支持多重注解了，先看个例子来理解一下是什么意思。 首先定义一个包装类Hints注解用来放置一组具体的Hint注解：

代码如下:

@interface Hints { Hint[] value(); }

@Repeatable(Hints.class) @interface Hint { String value(); }

Java 8允许我们把同一个类型的注解使用多次，只需要给该注解标注一下@Repeatable即可。

例 1: 使用包装类当容器来存多个注解（老方法）

代码如下:

@Hints({@Hint("hint1"), @Hint("hint2")}) class Person {}

例 2：使用多重注解（新方法）

代码如下:

@Hint("hint1") @Hint("hint2") class Person {}

第二个例子里java编译器会隐性的帮你定义好@Hints注解，了解这一点有助于你用反射来获取这些信息：

代码如下:

Hint hint = Person.class.getAnnotation(Hint.class); System.out.println(hint); // null

Hints hints1 = Person.class.getAnnotation(Hints.class); System.out.println(hints1.value().length); // 2

Hint[] hints2 = Person.class.getAnnotationsByType(Hint.class); System.out.println(hints2.length); // 2

即便我们没有在Person类上定义@Hints注解，我们还是可以通过 getAnnotation(Hints.class) 来获取 @Hints注解，更加方便的方法是使用 getAnnotationsByType 可以直接获取到所有的@Hint注解。 另外Java 8的注解还增加到两种新的target上了：

代码如下:

@Target({ElementType.TYPE_PARAMETER, ElementType.TYPE_USE}) @interface MyAnnotation {}

关于Java 8的新特性就写到这了，肯定还有更多的特性等待发掘。JDK 1.8里还有很多很有用的东西，比如Arrays.parallelSort, StampedLock和CompletableFuture等等。

##  Java代理的几种实现方式 



第一种:静态代理,只能静态的代理某些类或者某些方法,不推荐使用,功能比较弱,但是编码简单

第二种:动态代理,包含Proxy代理和CGLIB动态代理

### Proxy代理是JDK内置的动态代理

​         特点:面向接口的,不需要导入三方依赖的动态代理,可以对多个不同的接口进行增强,通过反射读取注解时,只能读取到接口上的注解

​         原理:面向接口,只能对实现类在实现接口中定义的方法进行增强

定义接口和实现

```
package com.proxy;

public interface UserService {
    public String getName(int id);

    public Integer getAge(int id);
}
```

```
package com.proxy;

public class UserServiceImpl implements UserService {
    @Override
    public String getName(int id) {
        System.out.println("------getName------");
        return "riemann";
    }

    @Override
    public Integer getAge(int id) {
        System.out.println("------getAge------");
        return 26;
    }
}
```

```
package com.proxy;

import java.lang.reflect.InvocationHandler;
import java.lang.reflect.Method;

public class MyInvocationHandler implements InvocationHandler {

    public Object target;

    MyInvocationHandler() {
        super();
    }

    MyInvocationHandler(Object target) {
        super();
        this.target = target;
    }

    @Override
    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
        if ("getName".equals(method.getName())) {
            System.out.println("++++++before " + method.getName() + "++++++");
            Object result = method.invoke(target, args);
            System.out.println("++++++after " + method.getName() + "++++++");
            return result;
        } else {
            Object result = method.invoke(target, args);
            return result;
        }
    }
}
```

```
package com.proxy;

import java.lang.reflect.InvocationHandler;
import java.lang.reflect.Proxy;

public class Main1 {
    public static void main(String[] args) {
        UserService userService = new UserServiceImpl();
        InvocationHandler invocationHandler = new MyInvocationHandler(userService);
        UserService userServiceProxy = (UserService) Proxy.newProxyInstance(userService.getClass().getClassLoader(),
                userService.getClass().getInterfaces(),invocationHandler);
        System.out.println(userServiceProxy.getName(1));
        System.out.println(userServiceProxy.getAge(1));
    }
}
```



### CGLIB动态代理

​        特点:面向父类的动态代理,需要导入第三方依赖

​        原理:面向父类,底层通过子类继承父类并重写方法的形式实现增强

Proxy和CGLIB是非常重要的代理模式,是springAOP底层实现的主要两种方式

CGLIB的核心类：
net.sf.cglib.proxy.Enhancer – 主要的增强类
net.sf.cglib.proxy.MethodInterceptor – 主要的方法拦截类，它是Callback接口的子接口，需要用户实现
net.sf.cglib.proxy.MethodProxy – JDK的java.lang.reflect.Method类的代理类，可以方便的实现对源对象方法的调用,如使用：
Object o = methodProxy.invokeSuper(proxy, args);//虽然第一个参数是被代理对象，也不会出现死循环的问题。

net.sf.cglib.proxy.MethodInterceptor接口是最通用的回调（callback）类型，它经常被基于代理的AOP用来实现拦截（intercept）方法的调用。这个接口只定义了一个方法
public Object intercept(Object object, java.lang.reflect.Method method,
Object[] args, MethodProxy proxy) throws Throwable;

第一个参数是代理对像，第二和第三个参数分别是拦截的方法和方法的参数。原来的方法可能通过使用java.lang.reflect.Method对象的一般反射调用，或者使用 net.sf.cglib.proxy.MethodProxy对象调用。net.sf.cglib.proxy.MethodProxy通常被首选使用，因为它更快。

```
package com.proxy.cglib;

import net.sf.cglib.proxy.MethodInterceptor;
import net.sf.cglib.proxy.MethodProxy;
import java.lang.reflect.Method;
 
public class CglibProxy implements MethodInterceptor {
    @Override
    public Object intercept(Object o, Method method, Object[] args, MethodProxy methodProxy) throws Throwable {
        System.out.println("++++++before " + methodProxy.getSuperName() + "++++++");
        System.out.println(method.getName());
        Object o1 = methodProxy.invokeSuper(o, args);
        System.out.println("++++++before " + methodProxy.getSuperName() + "++++++");
        return o1;
    }
}

```

```
package com.proxy.cglib;
 
import com.test3.service.UserService;
import com.test3.service.impl.UserServiceImpl;
import net.sf.cglib.proxy.Enhancer;
 
public class Main2 {
    public static void main(String[] args) {
        CglibProxy cglibProxy = new CglibProxy();
 
        Enhancer enhancer = new Enhancer();
        enhancer.setSuperclass(UserServiceImpl.class);
        enhancer.setCallback(cglibProxy);
 
        UserService o = (UserService)enhancer.create();
        o.getName(1);
        o.getAge(1);
    }
}
```



# Java集合

## 常用的集合类

**Map和Collection接口是所有集合的父接口**

<img src="https://uploadfiles.nowcoder.com/files/20210427/115285789_1619511097912/20201126123247321.PNG" alt="在这里插入图片描述" style="zoom: 80%;" />

<img src="https://uploadfiles.nowcoder.com/files/20210427/115285789_1619511098047/20201126123312505.PNG" alt="在这里插入图片描述" style="zoom: 80%;" />



## List，Set，Map三者的区别？

- `List`：**有序集合**（有序指存入的顺序和取出的顺序相同），**可存储重复元素，可存储多个`null`**。 
- `Set`：**无序集合**（元素存入和取出顺序不一定相同），**不可存储重复元素，只能存储一个`null`。** 
- `Map`：使用键值对的方式对元素进行存储，`key`是无序的，且是唯一的。`value`值不唯一。



## 常用集合框架底层数据结构

- `List`：
  1. `ArrayList`：数组 
  2. `LinkedList`：双向[链表]() 
- `Set`：
  1. `HashSet`：底层基于`HashMap`实现，`HashSet`存入读取元素的方式和`HashMap`中的`Key`是一致的。 
  2. `TreeSet`：[红黑树]() 
- `Map`：
  1. `HashMap`： JDK1.8之前`HashMap`由数组+[链表]()组成的， JDK1.8之后有数组+[链表]()/[红黑树]()组成，当[链表]()长度大于8时，[链表]()转化为[红黑树]()，当长度小于6时，从[红黑树]()转化为[链表]()。这样做的目的是能提高`HashMap`的性能，因为[红黑树]()的查找元素的时间复杂度远小于[链表]()。 
  2. `HashTable`：数组+[链表]() 
  3. `TreeMap`：[红黑树]()



## 哪些集合类是线程安全的

- `Vector`：动态数组，相当于有同步机制(比如锁)的`ArrayList`

- `Stack`：栈

- `HashTable`: key-value结构,操作几乎和HashMap一致，

  ​					  主要的区别在于HashTable为了实现多线程安全		

  ​					  在几乎所有的方法上都加上了synchronized锁，

  ​                      而加锁的结果就是HashTable操作的效率十分低下

- `enumeration`：枚举



## Java集合的快速失败机制 “fail-fast”和安全失败机制“fail-safe”是什么？

### 快速失败: fail-fast

Java的快速失败机制是Java集合框架中的一种错误检测机制，当多个线程同时对集合中的内容进行修改时可能就会抛出并发修改异常`ConcurrentModificationException`。其实不仅仅是在多线程状态下，**在单线程中用增强`for`循环中一边遍历集合一边修改集合的元素也会抛出`ConcurrentModificationException`异常**。看下面代码：

* **iteritor会维护一个expectedModMount变量, 而集合改变时, 集合的modeCount会自增, 此时expectedModMount != modCount, 便会抛出`ConcurrentModificationException`异常. **

* **同时在多线程中不同的Thread的迭代器会维护各自的expectedModMount, 而modCount是不同线程共享的ArrayList的变量, 如果一个线程修改了ArrayList的modCount导致另一个线程迭代的时候expectedModMount != modCount就会抛出异常**

```java
public class Main{
    public static void main(String[] args) {
    List<Integer> list = new ArrayList<>();
        for(Integer i : list){
            list.remove(i);  //运行时抛出ConcurrentModificationException异常
        }
    }
}
```

### 如何边遍历边移除 Collection 中的元素？

* 遍历集合时如果直接调用`remove()`方法抛出`ConcurrentModificationException`异常，所以正确的做法是用迭代器的`remove()`方法，便可正常运行。

```java
public class Main{
    public static void main(String[] args) {
    List<Integer> list = new ArrayList<>();
    Iterator<Integer> it = list.iterator();
        while(it.hasNext()){
            it.remove();
        }
    }
}
```



### 什么是CopyOnWrite容器？

#### 安全失败: fail-safe

采用安全失败机制的集合容器，不是直接在集合内容上修改的，而是**先复制原有集合内容，在拷贝的集合上进行操作**，修改之后，再将原容器的引用指向新的容器**。读数据是迭代器遍历，拿到的是开始遍历那一刻的集合拷贝，在遍历期间原集合发生了修改，**迭代器是无法访问到修改后的内容**。`java.util.concurrent`包下的容器都是**安全失败**，可以在多线程下并发使用。

CopyOnWrite容器即写时复制的容器。通俗的理解是当我们往一个容器添加元素的时候，不直接往当前容器添加，而是先将当前容器进行Copy，复制出一个新的容器，然后新的容器里添加元素，添加完元素之后，再将原容器的引用指向新的容器。这样做的好处是我们可以对CopyOnWrite容器进行并发的读，而不需要加锁，因为当前容器不会添加任何元素。所以CopyOnWrite容器也是一种**读写分离**的思想，读和写不同的容器。**适用于读多写少的场景**。

#### CopyOnWriteArrayList的使用原理

读不需要加锁，遍历迭代器读取。

写数据的操作，使用Reentrantlock加锁，先复制原有集合内容，在拷贝的集合上进行写数据。写完后将引用指向新的拷贝集合。

#### CopyOnWriteArrayList的缺陷和使用场景

CopyOnWriteArrayList 有几个缺点：

- 由于写操作的时候，需要**拷贝数组，会消耗内存**，如果原数组的内容比较多的情况下，可能导致young gc或者full gc
- **不能用于实时读**的场景，像**拷贝数组、新增元素都需要时间**，所以调用一个set操作后，**读取到数据可能还是旧的**,虽然CopyOnWriteArrayList 能做到最终一致性,但是还是没法满足实时性要求

### **`java.util.concurrent`下的集合:**

阻塞队列（队列无数据，操作队列产生异常或返回null，不具备等待/阻塞的特色）
**ConcurrentHashMap**：在 JDK1.7 的时候，ConcurrentHashMap（分段锁） 对整个数组进行了分割分段(Segment数组+entry组成)，每一把锁只锁一个segment对象(继承ReentrantLock)，多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。 到了 JDK1.8 的时候已经摒弃了 Segment 的概念，而是直接用 Node 数组+链表+红黑树的数据结构来实现，并发控制使用 synchronized 和 CAS 来操作，发生哈希冲突时使用synchronized，没有发生时使用cas。 hashtable使用synchronized对整个hashentry数组上锁。

CAS:CAS算法是乐观锁的一种实现。CAS有3个操作数，内存值V，旧的预期值A，要修改的新值B。当且仅当预期值A和内存值V相同时，将内存值V修改为B。比较并交换的操作是原子的，不可以被中断。
CAS是一种系统原语,原语属于操作系统用于范畴，原语的执行必须是连续的,在执行过程中不允许中断。

ABA:线程1对变量修改多次后将变量改回初始值，此时线程2使用cas修改变量时会判定变量没有发生改变，修改成功。隐患(堆栈操作)。
对变量增加一个版本号来解决该问题。

**ConcurrentSkipListMap**：支持排序。

**ConcurrentSkipListSet**：支持排序且不允许重复元素。

注：上面两个，排序的实现要求集合中的对象实现Comparable接口，不重复的实现为重写hashCode和equals方法

**ConcurrentLinkedQueue**：队列操作(只操作队头)，poll() / peek() / element()

**ConcurrentLinkedDeque**：双端队列(支持操作队头和队尾)，pollFirst() / pollLast()

**CopyOnWriteArrayList**：适用于读操作>>写操作的情况。在写时拷贝，也就是如果需要对CopyOnWriteArrayList的内容进行改变，首先会拷贝一份新的List并且在新的List上进行修改，最后将原List的引用指向新的List。线程安全地遍历，因为如果另外一个线程在遍历的时候修改List的话，实际上会拷贝出一个新的List上修改，而不影响当前正在被遍历的List。ArrayList非线程安全

**CopyOnWriteArraySet**：HashSet非线程安全

 

阻塞队列（取空队列需要等待直到有元素，塞满队列需要等待直到有空间）
**ArrayBlockingQueue**：有界阻塞队列

**LinkedBlockingQueue**：无界阻塞队列，基于单向链表的实现

**PriorityBlockingQueue**：无界有序的阻塞队列，基于数组

**SynchronousQueue**：同步队列，插入需等待移除，移除需等待插入 

**DelayQueue**：延时执行任务的队列，集合元素需实现java.util.concurrent.Delayed接口

**LinkedTransferQueue**：与SynchronousQueue功能类似，但有嗅探功能，能尝试性的添加数据（tryTransfer()方法）



## comparable 和 comparator的区别？

Comparable可以认为是一个内比较器，实现了Comparable接口的类有一个特点，就是这些类是可以和自己比较的，排序规则定好。

Comparator可以认为是是一个外比较器，使用情况 ：1、（没有实现Comparable接口），但是又想对两个对象进行比较
2、一个对象实现了Comparable接口，但是开发者认为compareTo方法中的比较方式并不是自己想要的那种比较方式。

Comparator将比较规则与数据分离，相比于Comparable耦合度更低。

## List集合

### Array 和 ArrayList 有何区别？

- `Array`可以包含基本类型和对象类型，`ArrayList`只能包含对象类型。 
- `Array`大小是固定的，`ArrayList`的大小是动态变化的。(`ArrayList`的扩容是个常见面试题) 
- 相比于`Array`，`ArrayList`有着更多的内置方法，如`addAll()`，`removeAll()`。 
- 对于基本类型数据，`ArrayList` 使用自动装箱来减少编码工作量；而当处理固定大小的基本数据类型的时候，这种方式相对比较慢，这时候应该使用`Array`。

### 遍历一个 List 有哪些不同的方式？

元素在内存中的存储方式，主要有两种：

1. 顺序存储（Random Access）：相邻的数据元素在内存中的位置也是相邻的，可以根据元素的位置（如`ArrayList`中的下表）读取元素。 
2. 链式存储（Sequential Access）：每个数据元素包含它下一个元素的内存地址，在内存中不要求相邻。例如`LinkedList`。 

主要的遍历方式主要有三种：

1. `for`循环遍历：遍历者自己在集合外部维护一个计数器，依次读取每一个位置的元素。 
2. `Iterator`遍历：基于顺序存储集合的`Iterator`可以直接按位置访问数据。基于链式存储集合的`Iterator`，需要保存当前遍历的位置，然后根据当前位置来向前或者向后移动指针。 
3. `foreach`遍历：`foreach` 内部也是采用了`Iterator`的方式实现，但使用时不需要显示地声明`Iterator`。 

那么对于以上三种遍历方式应该如何选取呢？

在Java集合框架中，提供了一个`RandomAccess`接口，该接口没有方法，只是一个标记。通常用来标记`List`的实现是否支持`RandomAccess`。所以在遍历时，可以先判断是否支持`RandomAccess`（`list instanceof RandomAccess`），如果支持可用 `for`循环遍历，否则建议用`Iterator`或 `foreach`遍历。

### ArrayList的扩容机制

`ArrayList`的初始容量为10，**扩容时是旧的容量值加上旧的容量数值进行右移一位（位运算，相当于除以2，位运算的效率更高）**，所以每次扩容都是旧的容量的1.5倍。

### ArrayList 和 LinkedList 的区别是什么？

- 是否线程安全：`ArrayList`和`LinkedList`都是不保证线程安全的 
- 底层实现：`ArrayList`的底层实现是数组，`LinkedList`的底层是双向[链表]()。 
- 内存占用：`ArrayList`会存在一定的空间浪费，因为每次扩容都是之前的1.5倍，而`LinkedList`中的每个元素要存放直接后继和直接前驱以及数据，所以对于每个元素的存储都要比`ArrayList`花费更多的空间。 
- 应用场景：`ArrayList`的底层数据结构是数组，所以在插入和删除元素时的时间复杂度都会受到位置的影响，平均时间复杂度为O(n)，在读取元素的时候可以根据下标直接查找到元素，不受位置的影响，平均时间复杂度为o(1)，**所以`ArrayList`更加适用于多读，少增删的场景**。`LinkedList`的底层数据结构是双向[链表]()，所以插入和删除元素不受位置的影响，平均时间复杂度为o(1)，如果是在指定位置插入则是O(n)，因为在插入之前需要先找到该位置，读取元素的平均时间复杂度为o(n)。**所以`LinkedList`更加适用于多增删，少读写的场景**。

### ArrayList 和 Vector 的区别是什么？

- 相同点
  1. 都实现了`List`接口 
  2. 底层数据结构都是数组 
- 不同点
  1. 线程安全：`Vector`使用了`Synchronized`来实现线程同步，所以是线程安全的，而`ArrayList`是线程不安全的。 
  2. 性能：由于`Vector`使用了`Synchronized`进行加锁，所以性能不如`ArrayList`。 
  3. 扩容：`ArrayList`和`Vector`都会根据需要动态的调整容量，但是`ArrayList`每次扩容为旧容量的1.5倍，而`Vector`每次扩容为旧容量的2倍。

### 简述 ArrayList、Vector、LinkedList 的存储性能和特性？

- `ArrayList`底层数据结构为数组，对元素的读取速度快，而增删数据慢，线程不安全。 
- `LinkedList`底层为双向[链表]()，对元素的增删数据快，读取慢，线程不安全。 
- `Vector`的底层数据结构为数组，用`Synchronized`来保证线程安全，性能较差，但线程安全。



## Set集合

### 说一下 HashSet 的实现原理

`HashSet`的底层是`HashMap`，默认构造函数是构建一个初始容量为16，负载因子为0.75 的`HashMap`。`HashSet`的值存放于`HashMap`的`key`上，`HashMap`的`value`统一为静态常量`PRESENT`，而不是null。

``` java
 private static final Object PRESENT = new Object();
```

如果HashMap底层的value保存的都是null的话，那么remove不管是成功或者是失败都会返回null，如果是add操作，add成功和失败都返回true，无法区分remove是成功还是失败了

## HashSet如何检查重复

hashcode、equals

## Map集合

### HashMap和HashSet

`HashSet` 底层就是基于 `HashMap` 实现的

|               `HashMap`                |                          `HashSet`                           |
| :------------------------------------: | :----------------------------------------------------------: |
|           实现了 `Map` 接口            |                       实现 `Set` 接口                        |
|               存储键值对               |                          仅存储对象                          |
|     调用 `put()`向 map 中添加元素      |             调用 `add()`方法向 `Set` 中添加元素              |
| `HashMap` 使用键（Key）计算 `hashcode` | `HashSet` 使用成员对象来计算 `hashcode` 值，对于两个对象来说 `hashcode` 可能相同，所以`equals()`方法用来判断对象的相等性 |

### HashMap 和 Hashtable 的区别

1. **线程是否安全**： HashMap 是非线程安全的， HashTable 是线程安全的,因为 HashTable 内部的方法基本都经过synchronized 修饰。（如果你要保证线程安全的话就使用 ConcurrentHashMap 吧！）； 
2. **效率**： 因为线程安全的问题， HashMap 要比 HashTable 效率高⼀点。另外， HashTable 基本被淘汰，不要在代码中使用它；
3. **Null key 和 Null value 的支持**： HashMap 可以存储 null 的 key 和 value，但 null 作为 键只能有⼀个，null 作为值可以有多个；HashTable 不允许有 null 键和 null 值，否则会抛出 NullPointerException 。 
4. **初始容量大小和每次扩充容量大小的不同** ： ① 创建时如果不指定容量初始值， Hashtable 默认的初始大小为 11，之后每次扩充，容量变为原来的 2n+1。 HashMap 默认的初始化大小为 16。之后每次扩充，容量变为原来的 2 倍。② 创建时如果给定了容量初始值，那么 Hashtable 会直接使用你给定的大小，而 HashMap 会将其扩充为 2 的幂次方大小（ HashMap 中的 tableSizeFor() 方法保证，下面给出了源代码）。也就是说 HashMap 总是使用 2 的幂作为哈希表的大小,后面会介绍到为什么是 2 的幂次方。 
5. **底层数据结构**： JDK1.8 以后的 HashMap 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么 会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时 间。Hashtable 没有这样的机制

### HashMap的底层实现

- JDK1.7的底层数据结构(数组+[链表](https://www.nowcoder.com/jump/super-jump/word?word=链表))
- JDK1.8的底层数据结构(数组+[链表](https://www.nowcoder.com/jump/super-jump/word?word=链表)、红黑树)

HashMap中有一个**实现了Map.Entry接口的Node内部类**，Node类包含**hash、key(泛型)、value(泛型)、next(Node型)属性**。

**HashMap 通过 key 的 hashCode 经过扰动函数处理过后得到 hash 值，然后通过` (n - 1) & hash `判断当前元素 存放的位置（这里的 n 指的是数组的长度），如果当前位置存在元素的话，就判断该元素与要存 ⼊的元素的 hash 值以及 key 是否相同，如果相同的话，直接覆盖，不相同就通过拉链法解决冲突。**当链表长度大于阈值8时, 链表会转成红黑树, 而长度小于6时, 会由红黑树退化成链表.

扰动函数:

1.8 先将hash值右移16位, 再进行异或(高16位与低16位异或), 如果不进行扰动处理，因为hash值有32位，直接对数组的长度求余，起作用只是hash值的几个低位。

``` java
static final int hash(Object key) {
        int h;
        return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
    }
```

#### HashMap在JDK1.7和JDK1.8中有哪些不同点：

|                              | JDK1.7                                                  | JDK1.8                                                       | JDK1.8的优势                                                 |
| ---------------------------- | ------------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 底层结构                     | 数组+[链表]()                                           | 数组+[链表]()/[红黑树]()([链表]()大于8)                      | 避免单条[链表]()过长而影响查询效率，提高查询效率             |
| hash值计算方式               | 9次扰动 = 4次位运算 + 5次异或运算                       | 2次扰动 = 1次位运算 + 1次异或运算                            | **可以均匀地把之前的冲突的节点分散到新的桶（高位低位均参与运算）** |
| 插入数据方式                 | 头插法（先讲原位置的数据移到后1位，再插入数据到该位置） | 尾插法（直接插入到[链表]()尾部/[红黑树]()）                  | 解决多线程造成死循环的问题(循环链表)                         |
| **扩容后存储位置的计算方式** | 重新进行hash计算                                        | 原位置或原位置+旧容量(**hash值和新增参与的位运算&结果为0或1**) | 省去了重新计算hash值的时间                                   |

### HashMap 的长度为什么是2的幂次方

hashmap为了存取高效, 需要尽量减少碰撞, 而将hash值取模后的结果作为索引就是实现了这个思想, 同时只有当数组长度为2的幂次方时, hash%length==hash&(length-1), 使用位运算效率比取余更高

>  **索引计算方法**
>
>  - 首先，计算对象的 hashCode()
>  - 再进行调用 HashMap 的 hash() 方法进行二次哈希
>  - 二次 hash() 是为了综合高位数据，让哈希分布更为均匀
>  - 最后 & (capacity – 1) 得到索引

#### HashMap为什么不直接使用hashCode()处理后的哈希值直接作为table的下标？

`hashCode()`处理后的哈希值范围太大，不可能在内存建立这么大的数组。 -2^31 到 2^31-1

### HashMap的put方法的具体流程？

<img src="https://uploadfiles.nowcoder.com/files/20210427/115285789_1619511098106/20201126123552911.png" alt="在这里插入图片描述" style="zoom: 1000%;" />

### HashMap的扩容操作是怎么实现的？

- 初始值为16，负载因子为0.75，阈值为负载因子*容量
- `resize()`方法是在`hashmap`中的键值对大于阀值时或者初始化时，就调用`resize()`方法进行扩容。
- 每次扩容，容量都是之前的两倍
- jdk1.8扩容后计算元素的新位置，有个判断`e.hash & oldCap`是否为零，**若等于0，则位置不变，若等于1，位置变为原位置加旧容量**。

### HashMap默认负载因子为什么选择0.75？

* 如果负载因子过高，空间利用率提高，但是会使得哈希冲突的概率增加；

* 如果负载因子过低，会频繁扩容，哈希冲突概率降低，但是会使得空间利用率变低。
* 具体为什么是0.75，不是0.74或0.76，这是一个基于数学分析（泊松分布）和行业规定一起得到的一个结论。

### 为什么要将[链表]()中转[红黑树]()的阈值设为8？为什么不一开始直接使用[红黑树]()？

可能有很多人会问，既然[红黑树]()性能这么好，为什么不一开始直接使用[红黑树]()，而是先用[链表]()，[链表]()长度大于8时，才转换为[红黑树]()。

- 因为[红黑树]()的节点所占的空间是普通[链表]()节点的两倍，但查找的时间复杂度低，所以只有当节点特别多时，[红黑树]()的优点才能体现出来。至于为什么是8，是通过数据分析统计出来的一个结果，[链表]()长度到达8的概率是很低的，综合[链表]()和[红黑树]()的性能优缺点考虑将大于8的[链表]()转化为[红黑树]()。 
- [链表]()转化为[红黑树]()除了**[链表]()长度大于8，还要`HashMap`中的数组长度大于64**。也就是如果`HashMap`长度小于64，[链表]()长度大于8是不会转化为[红黑树]()的，而是直接扩容。

树化意义

- 红黑树用来避免 DoS 攻击，防止**链表超长时性能下降**，树化应当是偶然情况，是保底策略
- hash表的查找，更新的时间复杂度是 O(1)，而红黑树的查找，更新的时间复杂度是 O(log2⁡n )，TreeNode 占用空间也比普通 Node 的大，如非必要，尽量还是使用链表
- hash 值如果足够随机，则在 hash 表内按泊松分布，在负载因子 0.75 的情况下，长度超过 8 的链表出现概率是 0.00000006，树化阈值选择 8 就是为了让树化几率足够小

树化规则

- 当链表长度超过树化阈值 8  时，先尝试扩容来减少链表长度，如果数组容量已经 >=64，才会进行树化

退化规则

- 情况1：在扩容时如果拆分树时，树元素个数 <= 6 则会退化链表
- 情况2：remove 树节点时，若root、root.left、root.right、root.left.left 有一个为 null ，也会退化为链表

### HashMap是怎么解决哈希冲突的？

- 拉链法

  `HasMap`中的数据结构为数组+[链表]()/[红黑树]()，当不同的`key`计算出的hash值相同时，就用[链表]()的形式将Node结点（冲突的`key`及`key`对应的`value`）挂在数组后面。

- hash函数

  `key`的hash值经过两次扰动，`key`的`hashCode`值与`key`的`hashCode`值的右移16位进行异或，然后对数组的长度取余（实际为了提高性能用的是位运算，但目的和取余一样），这样做可以让`hashCode`取值出的高位也参与运算，进一步降低hash冲突的概率，使得数据分布更平均。

- [红黑树]()

  在拉链法中，如果hash冲突特别严重，则会导致数组上挂的[链表]()长度过长，性能变差，因此在[链表]()长度大于8时，将[链表]()转化为[红黑树]()，可以提高遍历[链表]()的速度。

  

### 为什么HashMap中String、Integer这样的包装类适合作为Key？

- 这些包装类都是`final`修饰，是不可变性的， 保证了`key`的不可更改性，不会出现放入和获取时哈希值不同的情况。这样`key`对应的`hashCode()`值可以被缓存起来，性能更好
- 它们内部已经重写过`hashcode()`,`equal()`等方法。 

### 如果使用Object作为HashMap的Key，应该怎么办呢？

- 重写`hashCode()`方法，因为需要计算hash值确定存储位置 
- 重写`equals()`方法，因为需要保证`key`的唯一性。 

### HashMap 多线程导致死循环问题

由于JDK1.7的`hashMap`遇到hash冲突采用的是头插法，在多线程情况的Rehash 会造成元素之间会形成⼀个循环链表。但JDK1.8已经改成了尾插法，不存在这个问题了。但需要注意的是JDK1.8中的`HashMap`仍然是不安全的，在多线程情况下使用仍然会出现线程安全问题。

### HashMap、ConcurrentHashMap及Hashtable 的区别

`CocurrentHashMap`可以看作**线程安全且高效**的`HashMap`，相比于`HashMap`具有线程安全的优势，相比于`HashTable`具有效率高的优势。

|                              | HashMap(JDK1.8)                    | ConcurrentHashMap(JDK1.8)                                    | Hashtable                      |
| ---------------------------- | ---------------------------------- | ------------------------------------------------------------ | ------------------------------ |
| 底层实现                     | 数组+[链表]()/[红黑树]()           | 数组+[链表]()/[红黑树]()                                     | 数组+[链表]()                  |
| 线程安全                     | 不安全                             | 安全(`Synchronized`修饰Node节点)                             | 安全(`Synchronized`修饰整个表) |
| 效率                         | 高                                 | 较高                                                         | 低                             |
| 扩容                         | 初始16，每次扩容成2n               | 初始16，每次扩容成2n                                         | 初始11，每次扩容成2n+1         |
| 是否支持Null key和Null Value | 可以有一个Null key，Null Value多个 | 不支持                                                       | 不支持                         |
| 锁                           | 无                                 | JDK1.7底层是`ReentrantLock`+`Segment`+`HashEntry`，JDK1.8底层是`synchronized`+`CAS`+[链表]()/[红黑树]() | 使用synchronized全表锁         |

# Java虚拟机

## 1.Java内存Java运行时数据内存区域，并说一下每个部分都存哪些内容？

回答：Java的运行时区主要包含堆、方法区、虚拟机栈、本地方法栈、程序计数器，其中堆和方法区是所有线程所共有的。而且虚拟机栈、程序计数器和本地方法栈是线程所私有的。

堆：存放对象实例(字符串常量池在堆中)

方法区：用来存储已经被**虚拟机加载的类型信息、常量、静态变量、即时编译器编译后的代码缓存**等数据。

虚拟机栈：（生命周期与线程相同）Java中**每个方法执行的时候，Java虚拟机都会同步创建一个栈帧**，用于存储**局部变量表、操作数栈、动态链接（指向运行时常量池中该栈帧所属方法的引用）、方法出口**等信息。

程序计数器：保存下一条需要执行的字节码指令，是程序控制流的指示器**，分支、循环、跳转、异常处理、线程恢复等**基础功能都是依赖程序计数器。

本地方法栈：与虚拟机栈类似

### 什么是动态链接

简介：每一个栈帧内部**都包含一个指向运行时常量池中该栈帧所属方法的引用**，包含这个引用的**目的就是为了支持当前方法的代码能够实现动态链接**。比如: invokedynamic指令

作用：在Java源文件被编译到[字节码](https://so.csdn.net/so/search?q=字节码&spm=1001.2101.3001.7020)文件中时，所有的变量和方法引用都作为符号引用保存在class文件的常量池里，比如:描述一个方法调用了另外的其他方法时，就是通过常量池中指向方法的符号引用来表示的，**动态链接的作用就是为了将这些符号引用转换为调用方法的直接引用(动态链接指向运行时常量池中当前方法，方法信息中存放了当前方法调用的其他方法，再次获取其他方法的符号引用。)**。

### 追问1：程序计数器可以为空吗？

如果正在执行的是 Native 方法，这个计数器的值则为空 (Undefined)。

程序计数器存放的是Java字节码的地址，而native方法的方法体是非Java的，所以程序计数器的值才未定义。

### 追问2：堆中又怎么细分的？

<img src="https://img-blog.csdnimg.cn/20210228210659226.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ1Nzg4MDQz,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:67%;" />

回答：堆中可以细分为新生代和老年代，其中新生代又分为Eden区，From Sur[vivo]()r和To Sur[vivo]()r区，比例是8:1:1。 

### 追问3：哪些区域会造成OOM，处理？

回答：除了程序计数器不会产生OOM(Out Of Memory)，,翻译成中文就是“内存用完了”，其余的均可以产生OOM。

### class字节码文件

java文件被编译成 Class文件，`Class文件中除了包含类的版本、字段、方法、接口等描述信息外，还有一项就是常量池`。

因为JVM针对各种操作系统和平台都进行了定制，无论在什么平台，都可以通过javac命令将一个.java文件编译成固定格式的字节码（.class文件）供JVM使用。之所以被称为字节码，是因为**.class文件是由十六进制值组成的，JVM以两个十六进制值为一组，就是以字节为单位进行读取**

#### 组成结构是什么？

JVM对字节码的规范是有要求的，要求每一个字节码文件都要有十部分固定的顺序组成，如下图：
![](C:/Users/y/Desktop/春招笔记/images/bytecode2.png)

1. 魔数

所有的.class文件的前4个字节都是魔数，魔数以一个固定值：0xCAFEBABE，放在文件的开头，JVM就可以根据这个文件的开头来判断这个文件是否可能是一个.class文件，如果是以这个开头，才会往后执行下面的操作，这个魔数的固定值是Java之父James Gosling指定的，意为CafeBabe（咖啡宝贝）

2. 版本号

版本号是魔术之后的4个字节，前两个字节表示次版本号（Minor Version），后两个字节表示主版本号（Major Version），上面的0000 0032，次版本号0000转为十进制是0，主版本号0032 转为十进制50，对应下图的版本映射关系，可以看到对应的java版本号是1.6

![image.png](C:/Users/y/Desktop/春招笔记/images/bytecodeversion.png)

3. 常量池

紧接着主版本号之后的字节为常量池入口，常量池中有两类常量：**字面量和符号引用**，字面量是代码中申明为Final的常量值，符号引用是如类和接口的全局限定名、字段的名称和描述符、方法的名称和描述符。常量池整体分为两个部分：常量池计数器以及常量池数据区
![](C:/Users/y/Desktop/春招笔记/images/changlangchi.png)

4. 访问标志

常量池结束后的两个字节，描述的是类还是接口，以及是否被Public、Abstract、Final等修饰符修饰，JVM规范规定了9种访问标示（Access_Flag）JVM是通过按位或操作来描述所有的访问标示的，比如类的修饰符是Public Final，则对应的访问修饰符的值为ACC_PUBLIC | ACC_FINAL，即0x0001 | 0x0010=0x0011
![](C:/Users/y/Desktop/春招笔记/images/access_flag.png)


5. 当前类索引

访问标志后的两个字节，描述的是当前类的全限定名，这两个字节保存的值是常量池中的索引值，根据索引值就能在常量池中找到这个类的全限定名
​


6. 父类索引

当前类名后的两个字节，描述的父类的全限定名，也是保存的常量池中的索引值

7. 接口索引

父类名称后的两个字节，是接口计数器，描述了该类或者父类实现的接口数量，紧接着的n个字节是所有接口名称的字符串常量的索引值

8. 字段表

用于描述类和接口中声明的变量，包含类级别的变量和实例变量，但是不包含方法内部声明的局部变量，字段表也分为两个部分，第一部分是两个字节，描述字段个数，第二部分是每个字段的详细信息fields_info


9. 方法表

字段表结束后为方法表，方法表也分为两个部分，第一个部分是两个字节表述方法的个数，第二部分是每个方法的详细信息
方法的访问信息比较复杂，包括方法的访问标志、方法名、方法的描述符和方法的属性：


10. 附加属性

字节码的最后一部分，该项存放了在该文件中类或接口所定义属性的基本信息。

### 常量池

#### class常量池

Class文件被Java虚拟机加载进来后，存放在方法区 **各种字面量** (final常量、基本数据类型)和 **符号引用**（字段名、方法名） 。

#### 运行时常量池

运行时常量池是方法区的一部分。运行时常量池是当Class文件被加载到内存后，Java虚拟机会 将**Class文件常量池里的内容转移到运行时常量池里**(运行时常量池也是每个类都有一个)。运行时常量池相对于Class文件常量池的另外一个重要特征是具备**动态性**，Java语言并不要求常量一定只有编译期才能产生，也就是并非预置入Class文件中常量池的内容才能进入方法区运行时常量池，**运行期间也可能将新的常量放入池中**，比如**String中的intern()方法**(字符串不在常量池中则加到常量池中)。

#### 字符串常量池

String类是我们使用频率非常高的一种对象类型。JVM为了提升性能和减少内存开销，避免字符串的重复创建，其维护了一块特殊的内存空间，字符串常量池。



- 存放区域

在JDK1.7之前运行时常量池逻辑包含字符串常量池存放在方法区, 此时hotspot虚拟机对方法区的实现为永久代
在JDK1.7 字符串常量池被从方法区拿到了堆中, 这里没有提到运行时常量池,也就是说 字符串常量池被单独拿到堆,运行时常量池剩下的东西还在方法区, 也就是hotspot中的永久代
在JDK1.8 hotspot移除了永久代用元空间(Metaspace)取而代之, 这时候字符串常量池还在堆, 运行时常量池还在方法区, 只不过方法区的实现从永久代变成了元空间(Metaspace)

方法区两种实现：永久代与元空间，永久代实际存放在堆内存，需要指定大小，容易发送OOM，元空间使用本地内存，仅受本地内存的限制。

#### java共享内存

**MappedByteBuffer**为开发人员在java中实现共享[内存](https://so.csdn.net/so/search?q=内存&spm=1001.2101.3001.7020)提供了良好的方法，该缓冲区实际上**是一个磁盘文件的内存映像**，二者的变化会保持同步，即**内存数据发生变化过后会立即反应到磁盘文件中**，这样会有效的保证共享内存的实现。

应用场景：  

1、永久对象的配置
2、共享数据的查询

## 2.Java对象的创建过程

### 1. 类加载检查

虚拟机遇到一条 new 指令时，首先将去检查这个指令的参数是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程。

1、加载

双亲委派模型加载，加载过程为读取一个class文件，将其转化为某种静态数据结构存放在方法区内并在堆中生成一个相应的java.lang.Class类型对象供用户调用

2、验证

- **格式验证：**验证是否符合class文件规范

- **语义验证：**检查一个被标记为final的类型是否包含子类；检查一个类中的final方法是否被子类进行重写；确保父类和子类之间没有不兼容的一些方法声明（比如方法签名相同，但方法的返回值不同）

- **操作验证：**在操作数栈中的数据必须进行正确的操作，对常量池中的各种符号引用执行验证（通常在解析阶段执行，检查是否可以通过符号引用中描述的全限定名定位到指定类型上，以及类成员信息的访问修饰符是否允许访问等）

3、准备

为**类中的所有静态变量分配内存空间，并为其设置一个初始值**（由于还没有产生对象，实例变量不在此操作范围内）

被final修饰的static变量（常量），会直接赋值；

4、解析

将常量池中的符号引用转为直接引用（得到类或者字段、方法在内存中的指针或者偏移量，以便直接调用该方法），这个可以在初始化之后再执行。 解析需要静态绑定的内容。 // 所有不会被重写的方法和域都会被静态绑定

**以上2、3、4三个阶段又合称为链接阶段**，链接阶段要做的是将加载到JVM中的二进制字节流的类数据信息合并到JVM的运行时状态中。

5、初始化（先父后子）

- 4.1 为静态变量赋值
- 4.2 执行static代码块

> 注意：static代码块只有jvm能够调用 如果是多线程需要同时初始化一个类，仅仅只能允许其中一个线程对其执行初始化操作，其余线程必须等待，只有在活动线程执行完对类的初始化操作之后，才会通知正在等待的其他线程。

因为子类存在对父类的依赖，所以类的加载顺序是先加载父类后加载子类，初始化也一样。不过，父类初始化时，子类静态变量的值也有有的，是默认值。

最终，方法区会存储当前类信息，包括类的静态变量、类初始化代码（定义静态变量时的赋值语句 和 静态初始化代码块）、实例变量定义、实例初始化代码（定义实例变量时的赋值语句实例代码块和构造方法）和实例方法，还有父类的类信息引用。



### 2. 分配内存

在**类加载检查**通过后，接下来虚拟机将为新生对象**分配内存**。对象所需的内存大小在类加载完成 后便可确定，为对象分配空间的任务等同于把一块确定大小的内存从 Java 堆中划分出来。分配方式有 **“指针碰撞”** 和 **“空闲列表”** 两种，选择哪种分配方式由 Java 堆是否规整决定，而 Java 堆是否规整⼜由所采用的垃圾收集器是否带有压缩整理功能决定。

​	**内存分配的两种方式**：

* **指针碰撞**

  * 内存规整 标记-整理

    用过的内存全部整合到一边, 没有用过的内存放在另一边, 中间有个分界值指针, 向着没有用过的内存方向将该指针移动对象内存大小

* **空闲列表**

  * 内存不规整 标记-清除

    虚拟机会维护一个列表, 该列表中会记录哪些内存块是可用的, 在分配的时候, 找一块足够大的内存块来划分给对象实例, 最后更新列表记录

​	**内存分配并发问题**:

​	在创建对象的时候有一个很重要的问题，就是线程安全，因为在实际开发过程中，创建对象是很频繁的事情，作为虚拟机来说，必须要保证线程是安全的，通常来讲，虚拟机采用两种方式来保 证线程安全：

* **CAS+失败重试**： CAS 是乐观锁的一种实现方式。所谓乐观锁就是，每次不加锁而是假设 没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。虚拟机采用 CAS 配上失败重试的方式保证更新操作的原子性。
* **TLAB**： 为每一个线程预先在 Eden 区分配一块儿内存，JVM 在给线程中的对象分配内存 时，首先在 TLAB 分配，当对象大于 TLAB 中的剩余内存或 TLAB 的内存已用尽时，再采用 上述的 CAS 进行内存分配

### 3. 初始化零值

内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这一步操 作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用，程序能访问到这些字段的 数据类型所对应的零值。

### 4. 设置对象头

初始化零值完成之后，**虚拟机要对对象进行必要的设置**，例如这个对象是哪个类的实例、如何才 能找到类的元数据信息、对象的哈希码、对象的 GC 分代年龄等信息。 这些信息存放在对象头 中。 另外，根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方 式。

#### 对象包含哪些区域？

**对象头区域此处存储的信息包括两部分：**

1、对象自身的运行时数据( MarkWord )

存储 hashCode、GC 分代年龄、锁类型标记、偏向锁线程 ID 、 CAS 锁指向线程 LockRecord 的指针等， synconized 锁的机制与这个部分( markwork )密切相关，用 markword 中最低的三位代表锁的状态，其中一位是偏向锁位，另外两位是普通锁位。

2、对象类型指针( Class Pointer )

对象指向它的类元数据的指针、 JVM 就是通过它来确定是哪个 Class 的实例。

**实例数据区域** 

 此处存储的是对象真正有效的信息，比如对象中所有字段的内容

**对齐填充区域**

 JVM 的实现 HostSpot 规定对象的起始地址必须是 8 字节的整数倍，换句话来说，现在 64 位的 OS 往外读取数据的时候一次性读取 64bit 整数倍的数据，也就是 8 个字节，所以 HotSpot 为了高效读取对象，就做了"对齐"，如果一个对象实际占的内存大小不是 8byte 的整数倍时，就"补位"到 8byte 的整数倍。所以对齐填充区域的大小不是固定的。

### 5. 执行init方法

在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从 Java 程序的视 角来看，对象创建才刚开始， `<init>` 方法还没有执行，所有的字段都还为零。所以一般来说， 执行 new 指令之后会接着执行`<init>`方法，把对象按照程序员的意愿进行初始化，这样一个真 正可用的对象才算完全产生出来。



### 类加载机制

#### java类加载器有哪些？

| 名称                     | 加载哪的类            | 说明                           |
| ------------------------ | --------------------- | ------------------------------ |
| Bootstrap ClassLoader    | JAVA_HOME/jre/lib     | 无法直接访问                   |
| Extension ClassLoader    | JAVA_HOME/jre/lib/ext | 上级为 Bootstrap，显示为  null |
| Application  ClassLoader | classpath             | 上级为 Extension               |
| 自定义类加载器           | 自定义                | 上级为 Application             |






#### 双亲委派机制是什么？

双亲委派模式是在Java 1.2后引入的，责任链模式，其工作原理的是，如果一个类加载器收到了类加载请求，它并不会自己先去加载，而是把这个**请求委托给父类的加载器去执行**，如果父类加载器还存在其父类加载器，则**进一步向上委托，依次递归**，**请求最终将到达顶层的启动类加载器**，如果父类加载器可以完成类加载任务，就成功返回，倘若父类加载器无法完成此加载任务，子加载器才会尝试自己去加载，这就是双亲委派模式。

双亲委派的好处

   - **能够有效确保一个类的全局唯一性**，当程序中出现多个限定名相同的类时，类加载器在执行加载时，始终只会加载其中的某一个类。
   - 每一个类都会被尽可能的加载（从引导类加载器往下，每个加载器都可能会根据优先次序尝试加载它）
   - 有效避免了某些恶意类的加载（比如自定义了Java.lang.Object类，一般而言在双亲委派模型下会加载系统的Object类而不是自定义的Object类）

#### 如何破坏双亲委派模型

1. 双亲委派模型的第一次“被破坏”是**重写自定义加载器的loadClass()**,jdk不推荐。一般都只是重写findClass()，这样可以保持双亲委派机制.而loadClass方法加载规则由自己定义，就可以随心所欲的加载类
1. 双亲委派模型的第二次“被破坏”是ServiceLoader和Thread.setContextClassLoader()。即线程上下文类加载器（contextClassLoader）。双亲委派模型很好地解决了各个类加载器的基础类统一问题(越基础的类由越上层的加载器进行加载)，基础类之所以被称为“基础”，是因为它们总是作为被调用代码调用的API。但是，如果基础类又要调用用户的代码，那该怎么办呢？线程上下文类加载器就出现了。
   1. **SPI机制**
   
      JDK内置的一种 服务发现机制，可以用来启用框架扩展和替换组件，SPI 的本质是将**接口实现类的全限定名配置在文件中**，并由服务加载器读取配置文件，加载实现类。比如**JDBC接口**，java.sql.Driver接口，其他不同厂商可以针对同一接口做出不同的实现
   
      a).DriverManager类会加载每个Driver接口的实现类并管理它们，但是**DriverManager类自身是 jre/lib/rt.jar 里的类**，是由**bootstrap classloader**加载的
      b).根据类加载机制，某个**类需要引用其它类的时候，虚拟机将会用这个类的classloader去加载被引用的类**
      c).boostrap classloader显然是无法加载到MySQL driver的（ClassNotFoundException）
      因此只能在DriverManager里强行指定**一个上下文加载器**来加载Driver实现类，而这就会打破双亲委派模型
   
   1. 线程上下文类加载器默认情况下就是AppClassLoader，那为什么不直接通过getSystemClassLoader()获取类加载器来加载classpath路径下的类的呢？其实是可行的，但这种直接使用getSystemClassLoader()方法获取AppClassLoader加载类有一个缺点，那就是代码部署到不同服务时会出现问题，如把代码部署到Java Web应用服务或者EJB之类的服务将会出问题，因为这些服务使用的线程上下文类加载器并非AppClassLoader，而是Java Web应用服自家的类加载器，类加载器不同。，所以我们应用该少用getSystemClassLoader()。总之不同的服务使用的可能默认ClassLoader是不同的，但使用线程上下文类加载器总能获取到与当前程序执行相同的ClassLoader，从而避免不必要的问题
1. 双亲委派模型的第三次“被破坏”是由于用户对程序动态性的追求导致的，这里所说的“动态性”指的是当前一些非常“热门”的名词：代码热替换、模块热部署等，简答的说就是机器不用重启，只要部署上就能用。

### 对象的访问定位方法有几种，各有什么优缺点？

回答：Java虚拟机中对象的访问方式有**①使用句柄**和**②直接指针**两种。

1. **句柄：** 如果使用句柄的话，那么Java堆中将会划分出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而句柄中包含了**对象实例数据与类型数据各自的具体地址信息**； 
2. **直接指针：** 如果使用直接指针访问，那么 Java **堆对象的布局中就必须考虑如何放置访问类型数据的相关信息**，而reference 中存储的直接就是对象的地址。 

总结：使用句柄最大的好处就是reference中存储的是稳定句柄地址，在对象移动时只会改变句柄中的实例数据指针，而reference本身不需要被修改。使用直接指针访问方式最大的好处就是速度快，它节省了一次指针定位的时间开销。

![img](C:\Users\50131\Desktop\春招笔记\images\592743-20160319235555303-769658219.jpg)

### 执行 java 命令的过程

1. 创建 JVM，调用类加载子系统加载 class，将类的信息存入**方法区**
2. 创建 main 线程，使用的内存区域是 JVM **虚拟机栈**，开始执行 main 方法代码
  3. 如果遇到了未见过的类，会继续触发类加载过程，同样会存入**方法区**
  4. 需要创建对象，会使用**堆内存**来存储对象
  5. 不再使用的对象，会由**垃圾回收器**在内存不足时回收其内存
  6. 调用方法时，方法内的**局部变量、方法参数**所使用的是 JVM虚拟机栈中的**栈帧内存**
  7. 调用方法时，先要到**方法区**获得到该方法的字节码指令，由解释器将字节码指令解释为机器码执行
  8. 调用方法时，会将要执行的指令行号读到**程序计数器**，这样当发生了线程切换，恢复时就可以从中断的位置继续
  9. 对于非 java 实现的方法调用，使用内存称为**本地方法栈**（见说明）
  10. 对于热点方法调用，或者频繁的循环代码，由 **JIT 即时编译器**将这些代码编译成机器码缓存，提高执行性能



## 3.如何判断对象已死？

回答：Java中判断对象死亡的方法有引用计数法和可达性分析。

- **引用计数法**：对象中添加一个引用计数器，每当有一个地方引用它，计数器就加1；当引用失效，计数器就减1；任何时候计数器为0的对象就是不可能再被使用的。

- **可达性分析**：通过一系列的GC Roots的根对象作为起始节点，从这些节点开始，根据引用关系向下搜索，如果某个对象到GC Roots间没有任何引用链相连，则说明该对象可以被回收。

### 追问1：GCroot可以是哪些？

GC Root表示：**当前时刻存活的对象**

回答：在Java中可以作为GC Roots的比较多，分别有

(1)在**虚拟机栈中引用的对象**，比如各个线程被调用的方法堆栈中使用到的**参数、局部变量、临时变量**等。

(2)在方法区中**类静态属性引用的对象**，比如Java类的引用类型静态变量。

(3)在方法区中**常量引用的对象**，比如字符串常量池里的引用。

(4)在本地方法栈中JNI引用的对象。

(5)Java虚拟机内部的引用，如基本数据类型对应的Class对象，一些常驻的异常对象。

(6)所有被**同步锁持有的对象**。

### 追问2：被标志为GC的对象一定会被GC掉吗？

回答：不一定，还有逃脱的可能。真正宣告一个对象死亡至少经历两次标记的过程。

如果对象进行可达性分析后没有与GC Roots相连，那么这是第一次标记，之后会在进行一次筛选，筛选的条件是是否有必要执行finalize()方法。



## 4.垃圾回收[算法]()有哪些？详细叙述一下。

回答：垃圾回收[算法]()主要有三种，分别标记清除、标记整理和标记复制。

标记清除：[算法]()分为“标记”和“清除”两个阶段：首先**标记出所有需要回收的对象**，在标记完成后**统一回收所有被标记的对象**。它的主要不足是空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致以后在程序运行过程中需要分配较大对象时，无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。

标记复制：将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。这样使得每次都是对整个半区进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要按顺序分配内存即可，实现简单，运行高效。只是这种[算法]()的代价是将内存缩小为了原来的一半。

标记整理：首先标记出所有需要回收的对象，在标记完成后，后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。

### 标记-三色标记法

我们把遍历对象图过程中遇到的对象，按“是否访问过”这个条件标记成以下三种颜色：

- 白色：尚未被GC访问过的对象，如果全部标记已完成依旧为白色的，称为不可达对象，既垃圾对象。

- 黑色：本对象已经被GC访问过，且本对象的子引用对象也已经被访问过了。

- 灰色：本对象已访问过，但是本对象的子引用对象还没有被访问过，全部访问完会变成黑色，属于中间态。

1、在GC并发标记刚开始时，所以对象均为白色集合。

2、将所有GCRoots直接引用的对象标记为灰色集合。

3、判断若灰色集合中的对象不存在子引用，则将其放入黑色集合，若存在子引用对象，则将其所有的子引用对象放入灰色集合，当前对象放入黑色集合

4、按照步骤三，以此类推，直至灰色集合中的所有对象变成黑色后，本轮标记完成，且当前白色集合内的对象称为不可达对象，既垃圾对象。

### 追问1：新生代和老年代一般使用什么[算法]()？

回答：新生代一般使用标记复制和标记整理[算法]()，老年代一般使用标记清除[算法]()。

### 追问2：为什么新生代不使用标记清除[算法]()？

在新生代中，每次垃圾收集时都发现有大批对象死去，**只有少量存活**，那就选用复制[算法]()，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用“标记—清理”或者“标记—整理”[算法]()来进行回收。

### MinorGC回收整体流程

当Eden区不够分配内存时，发动一次MinorGC，发动之前检查老年代剩余内存空间是否大于新生代所有对象的大小，大于则发动MinorGC，不大于则查看分配担保参数，如果不允许，发动FullGC，如果允许，检查老年代内存空间是否大于历代晋升老年代对象的平均大小，如果小于，发动FullGC，如果大于发动MinorGC，如果担保失败，发动FullGC。经过扫描与标记，存活的对象被复制到survive区域，不存活的对象被回收， 并且存活的对象年龄都增大一岁，当对象年龄达到界限值(**可以通过参数 -XX:MaxTenuringThreshold 来设置,动态年龄判定见下**)进入老年代。

#### Full GC / Major GC发动时机(回收整个堆)

1).调用System.gc()方法会建议JVM进行Major GC，因为是建议并不一定会进行，但是大多数情况下还是会进行Major GC，强烈影响系建议能不使用此方法就别使用，让虚拟机自己去管理它的内存，可通过通过-XX:+ DisableExplicitGC来禁止RMI调用System.gc()。

2).**老年代空间不足**，JVM会进行Major GC，如果Major GC完后空间还是不足，就会抛出java.lang.OutOfMemoryError: Java heap space异常。

3).**方法区空间不足**，JVM会进行Major GC，如果Major GC完后空间还是不足，就会抛出java.lang.OutOfMemoryError: PermGen space异常。

4).java虚拟机中有分配担保机制，当新生代的复制空间存储不下存活对象时就会触发担保机制，会把剩下无法存储的对象直接存放进老年代，如果此时老年代空间也无法存储，就会担保失败，担保失败后JVM会进行Major GC。

## 5.垃圾回收器有哪些？

回答：垃圾回收器可以在新生代和老年代都有，在新生代有Serial、ParNew、Parallel Scavenge；老年代有CMS、Serial Old、Parallel Old;还有不区分新老的G1[算法]()。

### 追问1：CMS垃圾回收器的过程是什么样的？会带来什么问题？

回答：CMS内容管理系统回收过程可以分为4个步骤。

（1）初试标记：初试标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快，但需要**暂停**所有其他的工作线程。

（2）并发标记：GC和用户线程一起工作，执行GC Roots跟踪标记过程，**不需要暂停**工作线程。

（3）重新标记(**Stop-the-world**)：在并发标记过程中用户线程继续运作，导致在垃圾回收过程中部分对象的状态发生了变化，未来确保这部分对象的状态的正确性，需要对其重新标记并**暂停**工作线程。

（4）并发清除：清理删除掉标记阶段判断的已经死亡的对象，这个过程用户线程和垃圾回收线程**同时**发生。

带来的问题：

（1）CMS收集器对处理器资源非常敏感。

（2）CMS无法处理“浮动垃圾”。

（3）CMS是基于标记-清除算法产生**大量的空间碎片**。

### 追问2：G1垃圾回收器的改进是什么？相比于CMS突出的地方是什么？

回答：G1垃圾回收器将堆内存划分为大小固定的几个独立区域，并维护一个优先级列表，在垃圾回收过程中根据系统允许的最长垃圾回收时间，优先回收垃圾最多的区域。（G1[算法]()是可控STW的一种[算法]()，GC收集器和我们GC调优的目标就是尽可能的减少STW的时间和次数。）

G1突出的地方：

基于标记整理[算法]()，不产生垃圾碎片。

依靠硬件优势，其他垃圾回收器需要停止用户线程来执行GC线程，G1可以让他们并发执行。

建立可预测的停顿模型，可以精确的控制停顿时间，在不牺牲吞吐量的前提下实现短停顿垃圾回收。

### 追问3：现在jdk默认使用的是哪种垃圾回收器？

回答：(被问到过好几次)

jdk1.7 默认垃圾收集器Parallel Scavenge（新生代）+Parallel Old（老年代）

jdk1.8 默认垃圾收集器Parallel Scavenge（新生代）+Parallel Old（老年代）

jdk1.9 默认垃圾收集器G1



## 6.内存分配策略是什么样的？

1.对象优先在Eden区分配
大多数情况下，对象在新生代Eden区分配，当Eden区没有足够的空间，将触发一次Minor GC（新生代GC）。

2.大对象直接进入老年代
大对象指的是需要大量连续内存空间的Java对象，比如很大的字符串以及数组。为了避免安置不下大对象提前触发GC,**避免Eden和两个Survivor区发生大量的内存复制（复制算法）**，直接放置于老年代。
通过-XX:PertenureSizeThreshold参数，可以指定大于这个值的对象直接进入老年代。

3.长期存活对象进入老年代
既然虚抑机采用了分代收集的思想来管理内存，那么内存回收时就必须能识别哪此时象应放在新生代，哪些对象应放在老年代中。为了做到这点，虚拟机给每个对象定义了一个对象年龄(Age) 计数器。  如果对象在Eden 出生并经过第一次Minor GC后仍然存后，并且能被Survivor容纳的话，将被移动到Survivor空间中，并且对象年龄设为1，对象在Survivor 区中每经过一次MinorGC,  年龄就增加1岁，当它的年龄增加到一定程度(默认为15 岁)，就将会被晋升到老年代中。对象晋升老年代的年龄阈值，可以通过参数-XX:MaxTenuringThreshold 设置。

4.动态对象年龄判断
为了能更好地适应不同程序的内存状况，虚拟机并不是永远地要求对象的年龄必须达到了MaxTenuringThreshold 才能晋升老年代，如果在Survivor 空间中相同年龄所有对象大小的总和大于Survivor 空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代，无须等到-XX:MaxTenuringThreshold 中要求的年龄。

> Hotspot 遍历所有对象时，按照年龄从小到大对其所占用的大小进行累积，当累积的某个年龄大小超过了 survivor 区的⼀半时，取这个年龄和
> MaxTenuringThreshold 中更小的⼀个值，作为新的晋升年龄阈值  

5.空间分配担保
在发生Minor GC之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果这个条件成立，那么MinorGC可以确保是安全的。如果不成立，则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。如果允许，那么会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次MinorGC，尽管这次MinorGC是有风险的；如果小于，或者HandlePromotionFail 设置不允许冒险，那这时也要改为进行一次Full GC。

### 追问1：内存溢出与内存泄漏的区别？

内存溢出：实实在在的内存空间不足导致；

内存泄漏：该释放的对象没有释放，多见于自己使用容器保存元素的情况下。

## 7.jvm调优了解过吗？常用的命令和工具有哪些？

回答：Linux中有top、vmstat、pidstat，jdk中的jstat、jstack、jps、jmap等。（建议详细去看看这些命令的区别和作用，都可能会被问到）

top指令监控系统性能。**实时显示系统中各个进程的资源占用情况**

1）jps命令用于查询正在运行的JVM进程

2）jstat可以实时显示本地或远程JVM进程中类装载、内存、垃圾收集、JIT编译等数据

3）vmstat查看总体的CPU使用情况 

4）jmap用于显示当前Java堆和永久代的详细信息

5）jstack用于生成当前JVM的所有线程快照，线程快照是虚拟机每一条线程正在执行的方法,目的是定位线程出现长时间停顿的原因。

7）pidstat实时查看一个进程的CPU使用情况及上下文切换情况

### JVM调优(性能)的原则有哪些？

1. 多数的Java应用不需要在服务器上进行GC优化，虚拟机内部已有很多优化来保证应用的稳定运行，所以不要为了调优而调优，不当的调优可能适得其反
1. 在应用上线之前，先考虑将机器的JVM参数设置到最优（适合）
1. 在进行GC优化之前，需要确认项目的架构和代码等已经没有优化空间。我们不能指望一个系统架构有缺陷或者代码层次优化没有穷尽的应用，通过GC优化令其性能达到一个质的飞跃
1. GC优化是一个系统而复杂的工作，没有万能的调优策略可以满足所有的性能指标。GC优化必须建立在我们深入理解各种垃圾回收器的基础上，才能有事半功倍的效果
1. 处理吞吐量和延迟问题时，垃圾处理器能使用的内存越大，即java堆空间越大垃圾收集效果越好，应用运行也越流畅。这称之为GC内存最大化原则
1. 在这三个属性（吞吐量、延迟、内存）中选择其中两个进行jvm调优，称之为GC调优3选2



### 什么情况下需要JVM调优？


- Heap内存（老年代）持续上涨达到设置的最大内存值
- Full GC 次数频繁
- GC 停顿（Stop World）时间过长（超过1秒，具体值按应用场景而定）
- 应用出现OutOfMemory 等内存异常
- 应用出现OutOfDirectMemoryError等内存异常（ failed to allocate 16777216 byte(s) of direct memory (used: 1056964615, max: 1073741824)）
- 应用中有使用本地缓存且占用大量内存空间
- 系统吞吐量与响应性能不高或下降
- 应用的CPU占用过高不下或内存占用过高不下



### JVM调优指标？

1. **吞吐量：**用户代码时间 / （用户代码执行时间 + 垃圾回收时间）。是评价垃圾收集器能力的重要指标之一，是不考虑垃圾收集引起的停顿时间或内存消耗，垃圾收集器能支撑应用程序达到的最高性能指标。吞吐量越高算法越好。
2. **低延迟：**STW越短，响应时间越好。评价垃圾收集器能力的重要指标，度量标准是缩短由于垃圾收集引起的停顿时间或完全消除因垃圾收集所引起的停顿，避免应用程序运行时发生抖动。暂停时间越短算法越好
3. 在设计（或使用）GC 算法时，我们必须确定我们的目标：一个 GC 算法只可能针对两个目标之一（即只专注于最大吞吐量或最小暂停时间），或尝试找到一个二者的折衷
4. MinorGC尽可能多的收集垃圾对象。我们把这个称作MinorGC原则，遵守这一原则可以降低应用程序FullGC 的发生频率。FullGC 较耗时，是应用程序无法达到延迟要求或吞吐量的罪魁祸首
5. 堆大小调整的着手点、分析点：
   1. 统计Minor GC 持续时间
   1. 统计Minor GC 的次数
   1. 统计Full GC的最长持续时间
   1. 统计最差情况下Full GC频率
   1. 统计GC持续时间和频率对优化堆的大小是主要着手点
   1. 我们按照业务系统对延迟和吞吐量的需求，在按照这些分析我们可以进行各个区大小的调整
6. 一般来说吞吐量优先的垃圾回收器：-XX:+UseParallelGC  -XX:+UseParallelOldGC，即常规的（PS/PO）
7. 响应时间优先的垃圾回收器：CMS、G1

#### JVM参数有哪些？

1. **Xms** 是指**堆初始值**。一般来讲，大点，程序会启动的快一点，但是也可能会导致机器暂时间变慢
2. **Xmx** 是指**堆最大堆值**。如果程序运行需要占用更多的内存，超出了这个设置值，就会抛出OutOfMemory异常
3. **Xmn**是指新生代大小
4. **Xss** 是指设定每个线程的栈内存的大小。这个就要依据你的程序，看一个线程大约需要占用多少内存，可能会有多少线程同时运行等
5. **-XX:NewRatio**，设置新生代与老年代比值
6. **-XX:MaxTenuringThreshold**，新生代中对象存活的最大年龄，默认15。(若对象在eden区，经历一次MinorGC后还活着，则被移动到Survior区，年龄加1。以后，对象每次经历MinorGC，年龄都加1。达到阀值，则移入老年代)
7. **-XX:SurvivorRatio**，Eden区与Subrvivor区大小的比值
8. **-XX:+UseParNewGC**，新生代使用并行，老年代使用串行；
9. **-XX:+PrintGCDetails**，开启详细GC日志模式



#### 线上虚拟机cpu占用高、内存占用高如何调优？

1. CPU占用过高排查流程
   1. 利用 **top 命令**可以查出**占 CPU 最高的的进程**pid ，如果pid为 9876
   1. 然后查看**该进程下占用最高的线程id**【top -Hp 9876】
   1. 假设占用率最高的线程 ID 为 6900，将其转换为 16 进制形式 (因为 java native 线程以 16 进制形式输出) 【printf '%x\n' 6900】
   1. 利用 **jstack 打印出 java 线程调用栈信息**【jstack 9876 | grep '0x1af4' -A 50 --color】，这样就可以更好定位问题
2. 内存占用过高排查流程
   1. 查找进程id: 【top -d 2 -c】
   1. 查看**JVM堆内存**分配情况：jmap -heap pid
   1. 查看**占用内存比较多的对象** jmap -histo pid | head -n 100
   1. 查看**占用内存比较多的存活对象** jmap -histo:live pid | head -n 100

#### 追问2：jstack和jps的区别是什么？

 jstack：（Stack Trace for Java）命令用于生成虚拟机当前时刻的线程快照。

> 线程快照就是当前虚拟机内每一条线程正在执行的方法堆栈的集合，生成线程快照的主要目的是定位线程出现长时间停顿的原因，如线程间死锁、死循环、请求外部资源导致的长时间等待等都是导致线程长时间停顿的常见原因。
>
> 在代码中可以用java.lang.Thread类的getAllStackTraces（）方法用于获取虚拟机中所有线程的StackTraceElement对象。使用这个方法可以通过简单的几行代码就完成jstack的大部分功能，在实际项目中不妨调用这个方法做个管理员页面，可以随时使用浏览器来查看线程堆栈。

 jps ： 列出当前机器上正在运行的虚拟机进程

> -p :仅仅显示VM 标示，不显示jar,class, main参数等信息.
>
> -m:输出主函数传入的参数. 下的hello 就是在执行程序时从命令行输入的参数
>
> -l: 输出应用程序主类完整package名称或jar完整名称.
>
> -v: 列出jvm参数, -Xms20m -Xmx50m是启动程序指定的jvm参数

[JVM性能调优监控工具jps、jstack、jmap、jhat、jstat、hprof使用详解 - 云+社区 - 腾讯云 (tencent.com)](

### JVM调优工具有哪些？

1. MAT
   1. 提示可能的内存泄露的点
   
      
   
2. jvisualvm

3. jconsole

4. Arthas

5. show-busy-java-threads
   1. [https://github.com/oldratlee/useful-scripts/blob/master/docs/java.md#-show-busy-java-threads](https://github.com/oldratlee/useful-scripts/blob/master/docs/java.md#-show-busy-java-threads)​
      #### 





# MySQL数据库

## MySQL常用的存储引擎有什么？它们有什么区别？

- InnoDB

  InnoDB是MySQL的默认存储引擎，支持事务、行锁和外键等操作。

- MyISAM

  MyISAM是MySQL5.1版本前的默认存储引擎，MyISAM的并发性比较差，不支持事务和外键等操作，默认的锁的粒度为表级锁。

|          | InnoDB                             | MyISAM                                       |
| -------- | ---------------------------------- | -------------------------------------------- |
| 外键     | 支持                               | 不支持                                       |
| 事务     | 支持                               | 不支持                                       |
| 锁       | 支持表锁和行锁                     | 支持表锁                                     |
| 可恢复性 | 根据事务日志进行恢复               | 无事务日志                                   |
| 表结构   | 数据和索引是集中存储的，.ibd和.frm | 数据和索引是分开存储的，数据`.MYD`，索引`.MY |
| 索引     | 聚簇索引                           | 非聚簇索引                                   |

## 数据库的三大范式

- **第一范式：要求任何一张表必须有主键，每一个字段原子性不可再分。**
- **第二范式：建立在第一范式的基础之上，要求所有非主键字段完全依赖主键，不要产生部分依赖。**
- **第三范式：建立在第二范式的基础之上，要求所有非主键字段直接依赖主键，不要产生传递依赖。**

### 一、第一范式

1NF是对属性的**`原子性`**，要求属性具有原子性，不可再分解；

> 表：字段1、 字段2(字段2.1、字段2.2)、字段3 ......

如学生（学号，姓名，性别，出生年月日），如果认为最后一列还可以再分成（出生年，出生月，出生日），它就不是一范式了，否则就是；

### 二、第二范式

2NF是对记录的**`唯一性`**，要求记录有唯一标识，即实体的唯一性，即**不存在部分依赖**；

> 表：学号、课程号、姓名、学分;

这个表明显说明了两个事务:学生信息, 课程信息;由于非主键字段必须依赖主键，这里**学分依赖课程号**，**姓名依赖与学号**，所以不符合二范式。

**可能会存在问题：**

- `数据冗余:`，每条记录都含有相同信息；
- `删除异常：`删除所有学生成绩，就把课程信息全删除了；
- `插入异常：`学生未选课，无法记录进数据库；
- `更新异常：`调整课程学分，所有行都调整。

**正确做法:**
学生：`Student`(学号, 姓名)；
课程：`Course`(课程号, 学分)；
选课关系：`StudentCourse`(学号, 课程号, 成绩)。

### 三、第三范式

3NF是对字段的**`冗余性`**，要求任何字段不能由其他字段派生出来，它要求字段没有冗余，即**不存在传递依赖**；

> 表: 学号, 姓名, 年龄, 学院名称, 学院电话

因为存在**依赖传递**: (学号) → (学生)→(所在学院) → (学院电话) 。

**可能会存在问题：**

- `数据冗余:`有重复值；
- `更新异常：`有重复的冗余信息，修改时需要同时修改多条记录，否则会出现**数据不一致的情况** 。

**正确做法：**

学生：(学号, 姓名, 年龄, 所在学院)；

学院：(学院, 电话)。



## 索引

### 什么是索引？　

索引是对数据库表的一列或者多列的值进行[排序]()一种结构，使用索引可以快速访问数据表中的特定信息。

索引存储在磁盘中，内存断电重新生成数据太耗时

### 索引的优缺点？

优点：

- 大大加快数据检索的速度。 
- 将随机I/O变成顺序I/O(因为B+树的叶子节点是连接在一起的) 
- 加速表与表之间的连接 

缺点：

- 从空间角度考虑，建立索引需要占用物理空间 
- 从时间角度考虑，创建和维护索引都需要花费时间，例如对数据进行增删改的时候都需要维护索引。

### 索引的数据结构？

索引的数据结构主要有B+树和[哈希表]()，对应的索引分别为B+树索引和哈希索引。InnoDB引擎的索引类型有B+树索引和哈希索引，默认的索引类型为B+树索引。



#### B+树索引、B树、红黑树

**B+对比B树**

存储结构：

B 树的所有节点既存放键(key) 也存放数据(data)，而 B+树只有叶子节点存放 key 和 data，其他内节点只存放 key，一页可以存放的索引更多，减少了I/O次数。（InnoDB采取的方式是，将数据划分为若干个页，以页作为磁盘和内存之间交互的基本单位。InnoDB中页的大小一般为16KB。）
B 树的叶子节点都是独立的;B+树的叶子节点有一条引用链指向与它相邻的叶子节点。

搜索过程：

B 树的检索的过程从根节点开始，可能还没有到达叶子节点，检索就结束了。而 B+树的检索效率就很稳定了，任何查找都是从根节点到叶子节点的过程。

由于叶子结点指针相连，因此B+树更适合进行区间查找，B树进行区间查找需要中序遍历。

**对比红黑树**

红黑树等平衡树也可以用来实现索引（主要应用于内存，如HashMap，内存中不使用b+,数据量小，多路平衡查找退化为链表查找），但是文件系统及数据库系统普遍采用 B+ Tree 作为索引结构，主要有以下两个原因:

1.更少的查找次数，B+树查找操作的时间复杂度等于树高 h，而树高大致为 O(h)=O(logdN)，其中 d 为每个节点的出度。

2.为了减少磁盘 I/O，磁盘往往不是严格按需读取，而是每次都会预读。**b+树叶子结点相连接的结构使得相邻的节点也能够被预先载入**



#### 哈希索引(mysql自适应哈希索引)

InnoDB 存储引擎有一个特殊的功能叫“自适应哈希索引”，当某个**索引值被使用的非常频繁时**，会在 B+Tree 索引之上再创建一个哈希索引，这样就让 B+Tree 索引具有哈希索引的一些优点，比如快速的哈希查找。

哈希索引是基于[哈希表]()实现的，对于每一行数据，存储引擎会对索引列通过哈希[算法]()进行哈希计算得到哈希码，并且哈希[算法]()要尽量保证不同的列值计算出的哈希码值是不同的，将哈希码的值作为[哈希表]()的key值，将指向数据行的指针作为[哈希表]()的value值。这样查找一个数据的时间复杂度就是O(1)，一般多用于精确查找。

### Hash索引和B+树的区别？

因为两者数据结构上的差异导致它们的使用场景也不同，哈希索引一般多用于**精确的等值**查找，B+索引则多用于除了精确的等值查找外的其他查找。在大多数情况下，会选择使用B+树索引。

- 哈希索引不支持[排序]()，因为[哈希表]()是无序的。 
- 哈希索引不支持范围查找。 
- 哈希索引不支持模糊查询及多列索引的**最左前缀匹配**。 
  - **最左前缀匹配原则：**在MySQL建立联合索引时会遵守最左前缀匹配原则，即最左优先，在检索数据时从联合索引的最左边开始匹配。
- 因为[哈希表]()中会存在哈希冲突，所以哈希索引的性能是不稳定的，而B+树索引的性能是相对稳定的，每次查询都是从根节点到叶子节点 

### 索引的类型有哪些?

**从物理存储角度:** 

​	聚簇索引和非聚簇索引

**从数据结构角度:** 

​	B+树索引、hash索引、FULLTEXT(全文)索引、R-Tree索引

**从逻辑角度:**

- 主键索引：主键索引是一种特殊的唯一索引，不允许有空值
- 普通索引:  基本的索引类型，可以为NULL
- 组合索引：复合索引指多个字段上创建的索引，只有在查询条件中使用了创建索引时的第一个字段，索引才会被使用。使用复合索引时遵循最左前缀集合
- 唯一索引
- 空间索引：空间索引是对空间数据类型的字段建立的索引，MYSQL中的空间数据类型有4种，分别是GEOMETRY、POINT、LINESTRING、POLYGON。

#### 什么是聚簇索引，什么是非聚簇索引？

聚簇索引和非聚簇索引最主要的区别是**数据和索引是否分开存储**。

- 聚簇索引：将数据和索引放到一起存储，索引结构的**叶子节点保留了数据行**。 
- 非聚簇索引：将数据进和索引分开存储，索引叶子节点存储的是**主键**。 

在InnoDB存储引擎中，默认的索引为B+树索引，利用主键创建的索引为主索引，也是聚簇索引，在主索引之上创建的索引为辅助索引，也是非聚簇索引。为什么说辅助索引是在主索引之上创建的呢，因为**辅助索引中的叶子节点存储的是主键**。

在MyISAM存储引擎中，默认的索引也是B+树索引，但**主索引和辅助索引都是非聚簇索引**，也就是说索引结构的叶子节点存储的都是一个**指向数据行的地址**。并且使用辅助索引检索无需访问主键的索引。

可以从非常经典的两张图看看它们的区别：

![图片说明](https://uploadfiles.nowcoder.com/images/20210908/975641190_1631057902326/E8B5126C6C720C4643F2543CB83781CB) 

![图片说明](https://uploadfiles.nowcoder.com/images/20210908/975641190_1631057962523/70BCE6DF348B8AAF62246C6079F56E09) 

### 非聚簇索引一定会进行回表查询吗？

上面是说了非聚簇索引的叶子节点存储的是**主键**，也就是说要先通过非聚簇索引找到主键，再通过聚簇索引找到主键所对应的数据，后面这个再通过聚簇索引找到主键对应的数据的过程就是回表查询，那么非聚簇索引就一定会进行回表查询吗？

答案是不一定的，这里涉及到一个索引覆盖的问题，如果查询的数据再辅助索引上完全能获取到便不需要回表查询。例如有一张表存储着个人信息包括id、name、age等字段。假设聚簇索引是以ID为键值构建的索引，非聚簇索引是以name为键值构建的索引，`select id,name from user where name = 'zhangsan';`这个查询便不需要进行回表查询因为，通过非聚簇索引已经能全部检索出数据，这就是索引覆盖的情况。如果查询语句是这样，`select id,name,age from user where name = 'zhangsan';`则需要进行回表查询，因为通过非聚簇索引不能检索出age的值。那应该如何解决那呢？只需要将索引覆盖即可，建立age和name的联合索引再使用`select id,name,age from user where name = 'zhangsan';`进行查询即可。

所以通过索引覆盖能解决非聚簇索引回表查询的问题。


### 什么是前缀索引？

前缀索引是指对文本或者字符串的前几个字符建立索引，这样索引的长度更短，查询速度更快。

使用场景：前缀的区分度比较高的情况下。

建立前缀索引的方式

```
ALTER TABLE table_name ADD KEY(column_name(prefix_length));
```

这里面有个prefix_length参数很难确定，这个参数就是前缀长度的意思。通常可以使用以下方法进行确定，先计算全列的区分度

```
SELECT COUNT(DISTINCT column_name) / COUNT(*) FROM table_name;
```

然后在计算前缀长度为多少时和全列的区分度最相似。

```
SELECT COUNT(DISTINCT LEFT(column_name, prefix_length)) / COUNT(*) FROM table_name;
```

不断地调整prefix_length的值，直到和全列计算出区分度相近。

### 什么是最左匹配原则？

最左匹配原则：从最左边为起点开始连续匹配，遇到范围查询（<、>、between、like）会停止匹配。

例如建立索引(a,b,c)，大家可以猜测以下几种情况是否用到了索引。

- 第一种

```
select * from table_name where a = 1 and b = 2 and c = 3 
select * from table_name where b = 2 and a = 1 and c = 3
```

上面两次查询过程中所有值都用到了索引，where后面字段调换不会影响查询结果，因为MySQL中的优化器会自动优化查询顺序。

- 第二种

```
select * from table_name where a = 1
select * from table_name where a = 1 and b = 2  
select * from table_name where a = 1 and b = 2 and c = 3
```

- 第三种

```
select * from table_name where  b = 1 
select * from table_name where  b = 1 and c = 2 
```

答案是这两个查询语句都没有用到索引，因为不是从最左边开始匹配的

- 第四种

```
select * from table_name where a = 1 and c = 2 
```

这个查询语句只有a列用到了索引，c列没有用到索引，因为中间跳过了b列，不是从最左开始连续匹配的。

- 第五种

```
select * from table_name where  a = 1 and b < 3 and c < 1
```

这个查询中只有a列和b列使用到了索引，而c列没有使用索引，因为根据最左匹配查询原则，遇到范围查询会停止。

- 第六种

```
select * from table_name where a like 'ab%'; 
select * from table_name where  a like '%ab'
select * from table_name where  a like '%ab%'
```

- 只有1用到了索引，对于列为字符串的情况，只有**前缀匹配可以使用索引，中缀匹配和后缀匹配只能进行全表扫描**，以%开头会导致索引失效。

### 索引在什么情况下会失效？

在上面介绍了几种不符合最左匹配原则的情况会导致索引失效，除此之外，以下这几种情况也会导致索引失效。

- 条件中有or，例如`select * from table_name where a = 1 or b = 3` 
- 在索引上进行计算会导致索引失效，例如`select * from table_name where a + 1 = 2` 
- 在索引的类型上进行数据类型的隐形转换，会导致索引失效，例如字符串一定要加引号，假设 `select * from table_name where a = '1'`会使用到索引，如果写成`select * from table_name where a = 1`则会导致索引失效。 
- 在索引中使用函数会导致索引失效，例如`select * from table_name where abs(a) = 1` 
- 在使用like查询时以%开头会导致索引失效 
- 索引上使用！、=、<>进行判断时会导致索引失效，例如`select * from table_name where a != 1` 
- 索引字段上使用 is null/is not null判断时会导致索引失效，例如`select * from table_name where a is null`


## 事务

### 什么是数据库的事务？

数据库事务( transaction)是访问并可能操作各种数据项的一个数据库操作序列，这些操作要么全部执行,要么全部不执行，是一个不可分割的工作单位。

### 事务的四大特性是什么？

- 原子性：原子性是指包含事务的操作要么全部执行成功，要么全部失败回滚。 
- 一致性：一致性指事务在执行前后状态是一致的。 
- 隔离性：一个事务所进行的修改在最终提交之前，对其他事务是不可见的。 
- 持久性：数据一旦提交，其所作的修改将永久地保存到数据库中。

### ACID是靠什么保证的？

原子性:由undolog日志来保证，它记录了需要回滚的日志信息，事务回滚时撤销已经执行成功的sql

一致性:是由其他三大特性保证，程序代码要保证业务上的一致性

隔离性:是由MVCC来保证

持久性:由redolog来保证，mysql修改数据的时候会在redolog中记录一份日志数据，就算数据没有保存成功，只要日志保存成功了，数据仍然不会丢失

### 数据库的并发一致性问题

当多个事务并发执行时，可能会出现以下问题：

- 脏读：事务A更新了数据，但还没有提交，这时事务B读取到事务A更新后的数据，然后事务A回滚了，事务B读取到的数据就成为脏数据了。 
- 不可重复读：事务A对数据进行多次读取，事务B在事务A多次读取的过程中执行了更新操作并提交了，导致事务A多次读取到的数据并不一致。 
- 幻读：事务A在读取数据后，事务B向事务A读取的数据中插入了几条数据，事务A再次读取数据时发现多了几条数据，和之前读取的数据不一致。 
- 丢失修改：事务A和事务B都对同一个数据进行修改，事务A先修改，事务B随后修改，事务B的修改覆盖了事务A的修改。 

不可重复度和幻读看起来比较像，它们主要的区别是：在不可重复读中，发现数据不一致主要是数据被更新了。在幻读中，发现数据不一致主要是数据增多或者减少了。

### 数据库的隔离级别有哪些？

- 读未提交(READ-UNCOMMITTED)：一个事务在提交前，它的修改对其他事务也是可见的。事务A可以读取到事务B未提交的数据。
- 读已提交(READ-COMMITTED)：一个事务提交之后，它的修改才能被其他事务看到。  事务A只能读取到事务B提交之后的数据。
- 可重复读(REPEATABLE-READ)：在同一个事务中多次读取到的数据是一致的。 事务A开启之后，不管是多久，每一次在事务A中读取到的数据都是一致的。即使事务B将数据已经修改，并且提交了，事务A读取到的数据还是没有发生改变，这就是可重复读。
- 串行化(SERIALIZABLE)：需要加锁实现，会强制事务串行执行。 

数据库的隔离级别分别可以解决数据库的脏读、不可重复读、幻读等问题。

| 隔离级别 | 脏读   | 不可重复读 | 幻读   |
| :------- | ------ | ---------- | ------ |
| 读未提交 | 允许   | 允许       | 允许   |
| 读已提交 | 不允许 | 允许       | 允许   |
| 可重复读 | 不允许 | 不允许     | 允许   |
| 串行化   | 不允许 | 不允许     | 不允许 |

**MySQL的默认隔离级别是可重复读。**

### 隔离级别是如何实现的？

事务的隔离机制主要是依靠锁机制和MVCC(多版本并发控制)实现的，提交和可重复读可以通过MVCC实现，串行化可以通过锁机制实现。

InnoDB使用Next-Key Lock(间隙锁) + MVCC实现了在可重复读下的幻读问题.

## MVCC

### MVCC解决的问题是什么？

​		数据库并发场景有三种，分别为：

​		1、读读：不存在任何问题，也不需要并发控制

​		2、读写：有线程安全问题，可能会造成事务隔离性问题，可能遇到脏读、幻读、不可重复读

​		3、写写：有线程安全问题，可能存在更新丢失问题

​		MVCC是一种用来**解决读写冲突的无锁并发控制**，也就是为事务分配单项增长的时间戳，为每个修改保存一个版本，版本与事务时间戳关联，读操作只读该事务开始前的数据库的快照，所以MVCC可以为数据库解决一下问题：

​		1、在并发读写数据库时，可以做到在读操作时不用阻塞写操作，写操作也不用阻塞读操作，提高了数据库并发读写的性能

​		2、解决脏读、幻读、不可重复读等事务隔离问题，但是不能解决更新丢失问题

### MVCC整体操作流程

1.  首先获取事务自己的版本号，也就是事务 ID；
2.  获取 ReadView；
3.  查询得到的数据，然后与 ReadView 中的事务版本号进行比较；
4.  如果不符合 ReadView 规则，就需要从 Undo Log 中获取历史快照；
5.  最后返回符合规则的数据。

* 在隔离级别为读已提交（Read Committed）时，一个事务中的每一次 SELECT 查询都会重新获取一次 Read View。此时同样的查询语句都会重新获取一次 Read View，这时如果 Read View 不同，就可能产生 不可重复读或者幻读的情况。
* 当隔离级别为可重复读的时候，就避免了不可重复读，这是因为一个事务只在第一次 SELECT 的时候会 获取一次 Read View，而后面所有的 SELECT 都会复用这个 Read View，如下表所示：

### MVCC实现原理

​		MVCC，全称Multi-Version Concurrency Control，多版本并发控制

​		mvcc的实现原理主要依赖于记录中的三个**隐藏字段，undolog，read view**来实现的。

#### **隐藏字段**

​		每行记录除了我们自定义的字段外，还有数据库隐式定义的DB_TRX_ID,DB_ROLL_PTR,DB_ROW_ID等字段

​		DB_TRX_ID

​		6字节，最近修改事务id，记录创建这条记录或者最后一次修改该记录的事务id

​		DB_ROLL_PTR

​		7字节，回滚指针，指向这条记录的上一个版本,用于配合undolog，指向上一个旧版本

​		DB_ROW_JD

​		6字节，隐藏的主键，如果数据表没有主键，那么innodb会自动生成一个6字节的row_id

​		记录如图所示：

​		在上图中，DB_ROW_ID是数据库默认为该行记录生成的唯一隐式主键，DB_TRX_ID是当前操作该记录的事务ID，DB_ROLL_PTR是一个回滚指针，用于配合undo日志，指向上一个旧版本

#### **undo log**

​		undolog被称之为回滚日志，表示在进行insert，delete，update操作的时候产生的方便回滚的日志

​		当进行insert操作的时候，产生的undolog只在事务回滚的时候需要，并且在事务提交之后可以被立刻丢弃

​		当进行update和delete操作的时候，产生的undolog不仅仅在事务回滚的时候需要，在快照读的时候也需要，所以不能随便删除，只有在快照读或事务回滚不涉及该日志时，对应的日志才会被purge线程统一清除（当数据发生更新和删除操作的时候都只是设置一下老记录的deleted_bit，并不是真正的将过时的记录删除，因为为了节省磁盘空间，innodb有专门的purge线程来清除deleted_bit为true的记录，如果某个记录的deleted_id为true，并且DB_TRX_ID相对于purge线程的read view 可见，那么这条记录一定时可以被清除的）

​		**下面我们来看一下undolog生成的记录链**

​		1、假设有一个事务编号为1的事务向表中插入一条记录，那么此时行数据的状态为：

![image-20210225235444975](C:\Users\50131\Desktop\春招笔记\images\1.png)

​		2、假设有第二个事务编号为2对该记录的name做出修改，改为lisi

​		在事务2修改该行记录数据时，数据库会对该行加排他锁

​		然后把该行数据拷贝到undolog中，作为 旧记录，即在undolog中有当前行的拷贝副本

​		拷贝完毕后，修改该行name为lisi，并且修改隐藏字段的事务id为当前事务2的id，回滚指针指向拷贝到undolog的副本记录中

​		事务提交后，释放锁

![image-20210313220450629](C:\Users\50131\Desktop\春招笔记\images\2.png)

​		3、假设有第三个事务编号为3对该记录的age做了修改，改为32

​		在事务3修改该行数据的时，数据库会对该行加排他锁

​		然后把该行数据拷贝到undolog中，作为旧纪录，发现该行记录已经有undolog了，那么最新的旧数据作为链表的表头，插在该行记录的undolog最前面

​		修改该行age为32岁，并且修改隐藏字段的事务id为当前事务3的id，回滚指针指向刚刚拷贝的undolog的副本记录

​		事务提交，释放锁

![image-20210313220337624](C:\Users\50131\Desktop\春招笔记\images\3.png)

​		从上述的一系列图中，大家可以发现，不同事务或者相同事务的对同一记录的修改，会导致该记录的undolog生成一条记录版本线性表，即链表，undolog的链首就是最新的旧记录，链尾就是最早的旧记录。

#### **Read View**

​		上面的流程如果看明白了，那么大家需要再深入理解下read view的概念了。

​		Read View是事务进行快照读操作的时候生产的读视图，在该事务执行快照读的那一刻，会生成一个数据系统当前的快照，记录并维护系统当前活跃事务的id，事务的id值是递增的。

​		其实Read View的最大作用**是用来做可见性判断的**，也就是说当某个事务在执行快照读的时候，对该记录创建一个Read View的视图，把它当作条件去判断当前事务能够看到哪个版本的数据，有可能读取到的是最新的数据，也有可能读取的是当前行记录的undolog中某个版本的数据

​		Read View遵循的可见性算法**主要是将要被修改的数据的最新记录中的DB_TRX_ID取出来，与系统当前其他活跃事务的id去对比，如果DB_TRX_ID跟Read View的属性做了比较，不符合可见性，那么就通过DB_ROLL_PTR回滚指针去取出undolog中的DB_TRX_ID做比较，即遍历链表中的DB_TRX_ID，直到找到满足条件的DB_TRX_ID,这个DB_TRX_ID所在的旧记录就是当前事务能看到的最新老版本数据**。

​		Read View的可见性规则如下所示：

​		首先要知道Read View中的三个全局属性：

​		trx_list:一个数值列表，用来维护Read View生成时刻系统正活跃的事务ID（1,2,3）

​		up_limit_id:记录trx_list列表中事务ID最小的ID（1）

​		low_limit_id:Read View生成时刻系统尚未分配的下一个事务ID，（4）

​		具体的比较规则如下：

​		1、首先比较DB_TRX_ID < up_limit_id,如果小于，则当前事务能看到DB_TRX_ID所在的记录，如果大于等于进入下一个判断

​		2、接下来判断DB_TRX_ID >= low_limit_id,如果大于等于则代表DB_TRX_ID所在的记录在Read View生成后才出现的，那么对于当前事务肯定不可见，如果小于，则进入下一步判断

​		3、判断DB_TRX_ID是否在活跃事务中，如果在，则代表在Read View生成时刻，这个事务还是活跃状态，还没有commit，修改的数据，当前事务也是看不到，如果不在，则说明这个事务在Read View生成之前就已经开始commit，那么修改的结果是能够看见的。

#### RC、RR级别下的InnoDB快照读有什么不同

​		因为Read View生成时机的不同，从而造成RC、RR级别下快照读的结果的不同

​		1、在RR级别下的某个事务的对某条记录的第一次快照读会创建一个快照即Read View,将当前系统活跃的其他事务记录起来，此后在调用快照读的时候，还是使用的是同一个Read View,所以只要当前事务在其他事务提交更新之前使用过快照读，那么之后的快照读使用的都是同一个Read View,所以对之后的修改不可见

​		2、在RR级别下，快照读生成Read View时，Read View会记录此时所有其他活动和事务的快照，这些事务的修改对于当前事务都是不可见的，而早于Read View创建的事务所做的修改均是可见

​		3、在RC级别下，事务中，每次**快照读都会新生成一个快照和Read View**,这就是我们在RC级别下的事务中可以看到别的事务提交的更新的原因。

​		**总结：在RC隔离级别下，是每个快照读都会生成并获取最新的Read View,而在RR隔离级别下，则是同一个事务中的第一个快照读才会创建Read View，之后的快照读获取的都是同一个Read View.**。MVCC是一种并发控制的方法，一般在数据库管理系统中，实现对数据库的并发访问，在编程语言中实现事务内存。

#### 快照读、当前读

- **当前读**

像select lock in share mode(共享锁), select for update ; update, insert ,delete(排他锁)这些操作都是一种当前读，为什么叫当前读？就是它**读取的是记录的最新版本**，读取时还要**保证其他并发事务不能修改当前记录**，会对读取的记录进行加锁

- **快照读**

像**不加锁的select操作**就是快照读，即不加锁的非阻塞读；快照读的前提是隔离级别不是串行级别，串行级别下的快照读会退化成当前读；之所以出现快照读的情况，是基于提高并发性能的考虑，快照读的实现是基于多版本并发控制，即MVCC,可以认为MVCC是行锁的一个变种，但它在很多情况下，避免了加锁操作，降低了开销；既然是基于多版本，即快照读可能读到的并不一定是数据的最新版本，而有可能是之前的历史版本







## 数据库的锁

### 什么是数据库的锁？

当数据库有并发事务的时候，保证数据访问顺序的机制称为锁机制。

### 数据库的锁与隔离级别的关系？

| 隔离级别 | 实现方式                                 |
| -------- | ---------------------------------------- |
| 未提交读 | 总是读取最新的数据，无需加锁             |
| 提交读   | 读取数据时加共享锁，读取数据后释放共享锁 |
| 可重复读 | 读取数据时加共享锁，事务结束后释放共享锁 |
| 串行化   | 锁定整个范围的键，一直持有锁直到事务结束 |

#### 数据库锁的类型有哪些？

按照锁的粒度可以将MySQL锁分为三种：

| MySQL锁类别 | 资源开销 | 加锁速度 | 是否会出现死锁 | 锁的粒度 | 并发度 |
| ----------- | -------- | -------- | -------------- | -------- | ------ |
| 行级锁      | 大       | 慢       | 会             | 小       | 高     |
| 表级锁      | 小       | 快       | 不会           | 大       | 低     |
| 页面锁      | 一般     | 一般     | 不会           | 一般     | 一般   |

### 从锁的类别上区别可以分为共享锁和排他锁

- 共享锁：共享锁又称读锁，简写为S锁，一个事务对一个数据对象加了S锁，可以对这个数据对象进行读取操作，但不能进行更新操作。并且在加锁期间其他事务只能对这个数据对象加S锁，不能加X锁。 
- 排他锁：排他锁又称为写锁，简写为X锁，一个事务对一个数据对象加了X锁，可以对这个对象进行读取和更新操作，加锁期间，其他事务不能对该数据对象进行加X锁或S锁。 

### MySQL中InnoDB引擎的行锁模式及其是如何实现的？

**行锁模式**

在存在行锁和表锁的情况下，**一个事务想对某个表加X锁时**，需要先检查是否有**其他事务对这个表加了锁或对这个表的某一行加了锁**，对表的每一行都进行检测一次这是非常低效率的，为了解决这种问题，实现多粒度锁机制，InnoDB还有两种内部使用的意向锁，两种意向锁都是表锁。

- 意向共享锁：简称IS锁，一个事务打算给数据行加共享锁前必须先获得该表的IS锁。 
- 意向排他锁：简称IX锁，一个事务打算给数据行加排他锁前必须先获得该表的IX锁。 

有了意向锁，一个事务想对某个表加X锁，只需要检查是否有其他事务对这个表加了X/IX/S/IS锁即可。

锁的兼容性如下：

![图片说明](https://uploadfiles.nowcoder.com/images/20210908/975641190_1631058161616/83AD9A58DAAD28E8781CC364F1785EE2) 

行锁实现方式：INnoDB的行锁是通过给索引上的索引项加锁实现的，如果没有索引，InnoDB将通过隐藏的聚簇索引来对记录进行加锁。

InnoDB行锁主要分三种情况：

- Record lock：对索引项加锁 
- Gap lock：对索引之间的“间隙”、第一条记录前的“间隙”或最后一条后的间隙加锁。 
- Next-key lock：前两种放入组合，对记录及前面的间隙加锁。 

InnoDB行锁的特性：如果不通过索引条件检索数据，那么InnoDB将对表中所有记录加锁，实际产生的效果和表锁是一样的。

MVCC不能解决幻读问题，在可重复读隔离级别下，使用MVCC+Next-Key Locks可以解决幻读问题。**在快照读时使用MVCC,在当前读时使用next_key锁** ,[避免幻读 : next-key锁与MVCC](https://blog.csdn.net/weixin_43705457/article/details/104849943?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-1.pc_relevant_default&spm=1001.2101.3001.4242.2&utm_relevant_index=4)

相关知识点：

1. innodb对于行的查询使用next-key lock
2. Next-locking keying为了解决Phantom Problem幻读问题
3. 当查询的索引含有唯⼀属性时，将next-key lock降级为record key
4. Gap锁设计的目的是为了阻止多个事务将记录插⼊到同⼀范围内，而这会导致幻读问题的产⽣
5. 有两种方式显式关闭gap锁：（除了外键约束和唯⼀性检查外，其余情况仅使⽤recordlock）
   A. 将事务隔离级别设置为RC
   B. 将参数innodb_locks_unsafe_for_binlog设置为1  

### 间隙锁（Next-Key锁）
在**RR级别**下，当我们用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁；对于键值在条件范围内但并不存在的记录，叫做“间隙（GAP)”，InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁（Next-Key锁）。

#### 间隙锁锁定的区域：



对记录加锁时，**加锁的基本单位是 next-key lock**，它是由记录锁和间隙锁组合而成的，根据检索条件向左寻找最靠近检索条件的记录值A，作为左区间，向右寻找最靠近检索条件的记录值B作为右区间,**next-key lock** 是**前开后闭**区间，而**间隙锁**是**前开后开**区间**。但是，next-key lock 在一些场景下会退化成记录锁或间隙锁。

> ##### 唯一索引等值查询

当我们用唯一索引进行等值查询的时候，查询的记录存不存在，加锁的规则也会不同：

- **当查询的记录是存在的，在用「唯一索引进行等值查询」时，next-key lock 会退化成「记录锁」**。
- **当查询的记录是不存在的，在用「唯一索引进行等值查询」时，next-key lock 会退化成「间隙锁」**。

> ##### 唯一索引范围查询

- **当查询的记录是存在的，在用「唯一索引进行等值查询」时，next-key lock 会退化成「记录锁」**。
- **当查询的记录是不存在的，在用「唯一索引进行等值查询」时，next-key lock 会退化成「间隙锁」**。

> ##### 非唯一索引等值查询

当我们用非唯一索引进行等值查询的时候，查询的记录存不存在，加锁的规则也会不同：

- **当查询的记录存在时，除了会加 next-key lock 外，还额外加间隙锁，也就是会加两把锁**。
- **当查询的记录不存在时，只会加 next-key lock，然后会退化为间隙锁，也就是只会加一把锁。**

> ##### 非唯一索引范围查询

非唯一索引和主键索引的范围查询的加锁也有所不同，不同之处在于**普通索引范围查询，next-key lock 不会退化为间隙锁和记录锁**。

例子：

<img src="https://img-blog.csdnimg.cn/img_convert/a2c6f094cacce02e51ed439b2ba5c13e.png" alt="图片说明" style="zoom:100%;" /> 

```sql
select * from news where number=4 for update ;
```
检索条件number=4,向左取得最靠近的值2作为左区间，向右取得最靠近的5作为右区间，因此，session 1的间隙锁的范围(2，4)，(4，5)
间隙锁锁定的区间为(2，4)，(4，5)，即记录(id=1,number=2)和记录(id=3,number=4)之间间隙会被锁定，记录(id=3,number=4)和记录(id=6,number=5)之间间隙被锁定。

#### innodb自动使用间隙锁的条件：

(1)必须在RR级别下
(2)检索条件必须有索引(没有索引的话，mysql会全表扫描，那样会锁定整张表所有的记录，包括不存在的记录，此时其他事务不能修改不能删除不能添加)


### 什么是数据库的乐观锁和悲观锁，如何实现？

乐观锁：系统假设数据的更新在大多数时候是不会产生冲突的，所以数据库只在**更新操作提交的时候对数据检测冲突**，如果存在冲突，则数据更新失败。

乐观锁实现方式：一般通过**版本号和CAS算法**实现。

悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。通俗讲就是每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁。

悲观锁的实现方式：通过数据库的锁机制实现，对查询语句添加for updata。



### 什么是死锁？如何避免？

死锁是指两个或者两个以上进程在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象。在MySQL中，MyISAM是一次获得所需的全部锁，要么全部满足，要么等待，所以不会出现死锁。在InnoDB存储引擎中，除了单个SQL组成的事务外，锁都是逐步获得的，所以存在死锁问题。

如何避免MySQL发生死锁或锁冲突：

- 如果不同的程序并发存取多个表，尽量以相同的**顺序访问表**。
- 在程序以批量方式处理数据的时候，如果已经对数据[排序]()，尽量保证每个线程按照固定的顺序来处理记录。
- 在事务中，如果需要更新记录，应直接申请足够级别的排他锁，而不应该先申请共享锁，更新时在申请排他锁，因为在当前用户申请排他锁时，其他事务可能已经获得了相同记录的共享锁，从而造成锁冲突或者死锁。
- 尽量使用较低的隔离级别
- 尽量使用索引访问数据，使加锁更加准确，从而减少锁冲突的机会
- 合理选择事务的大小，小事务发生锁冲突的概率更低
- 尽量用相等的条件访问数据，可以避免Next-Key锁对并发插入的影响。
- 不要申请超过实际需要的锁级别，查询时尽量不要显示加锁
- 对于一些特定的事务，可以表锁来提高处理速度或减少死锁的概率。

## SQL语句基础知识

#### SQL语句主要分为哪几类　＊

- 数据据定义语言DDL（Data Definition Language）：主要有CREATE，DROP，ALTER等对逻辑结构有操作的，包括表结构、视图和索引。 
- 数据库查询语言DQL（Data Query Language）：主要以SELECT为主 
- 数据操纵语言DML（Data Manipulation Language）：主要包括INSERT，UPDATE，DELETE 
- 数据控制功能DCL（Data Control Language）：主要是权限控制能操作，包括GRANT，REVOKE，COMMIT，ROLLBACK等。 

#### SQL约束有哪些？　＊＊

- 主键约束：主键为在表中存在一列或者多列的组合，能唯一标识表中的每一行。一个表只有一个主键，并且主键约束的列不能为空。 
- 外键约束：外键约束是指用于在两个表之间建立关系，需要指定引用主表的哪一列。只有主表的主键可以被从表用作外键，被约束的从表的列可以不是主键，所以创建外键约束需要先定义主表的主键，然后定义从表的外键。 
- 唯一约束：确保表中的一列数据没有相同的值，一个表可以定义多个唯一约束。 
- 默认约束：在插入新数据时，如果该行没有指定数据，系统将默认值赋给该行，如果没有设置没默认值，则为NULL。 
- Check约束：Check会通过逻辑表达式来判断数据的有效性，用来限制输入一列或者多列的值的范围。在列更新数据时，输入的内容必须满足Check约束的条件。 

#### 什么是子查询？　＊＊

子查询：把一个查询的结果在另一个查询中使用

子查询可以分为以下几类：

- 标量子查询：指子查询返回的是一个值，可以使用 =,>,<,>=,<=,<>等操作符对子查询标量结果进行比较，一般子查询会放在比较式的右侧。

```
SELECT * FROM user WHERE age = (SELECT max(age) from user)  //查询年纪最大的人
```

- 列子查询：指子查询的结果是n行一列，一般应用于对表的某个字段进行查询返回。可以使用IN、ANY、SOME和ALL等操作符，不能直接使用

```
SELECT num1 FROM table1 WHERE num1 > ANY (SELECT num2 FROM table2)
```

- 行子查询：指子查询返回的结果一行n列

```
SELECT * FROM user WHERE (age,sex) = (SELECT age,sex FROM user WHERE name="zhangsan")
```

#### 了解MySQL的几种连接查询吗？　＊＊＊

MySQl的连接查询主要可以分为外连接，内连接，交叉连接

- 外连接

  外连接主要分为左外连接(LEFT JOIN)、右外连接(RIGHT JOIN)、全外连接。

  左外连接：显示左表中所有的数据及右表中符合条件的数据，右表中不符合条件的数据为null。

  ![图片说明](https://uploadfiles.nowcoder.com/images/20210908/975641190_1631058190699/31A7605403B42A797C4D2838845A4C0E) 

  右外连接：显示右表中所有的数据及左表中符合条件的数据，左表中不符合条件的数据为null。

  ![图片说明](https://uploadfiles.nowcoder.com/images/20210908/975641190_1631058219654/F266ACF7331F5927C24175458BB90AB5) 

  MySQL中不支持全外连接。

- 内连接：只显示符合条件的数据

  ![图片说明](https://uploadfiles.nowcoder.com/images/20210908/975641190_1631058233940/C0A6AC82B167BDBAAE5E7BB1589B5B64) 

- 交叉连接：使用笛卡尔积的一种连接。

  笛卡尔积，[百度]()百科的解释：两个集合*X*和*Y*的笛卡尔积表示为*X* × *Y*，第一个对象是*X*的成员而第二个对象是*Y*的所有可能有序对的其中一个成员 。例如：A={a,b}，B={0,1,2}，A × B = {(a,0)，(a,1)，(a,2)，(b,0)，(b,1)，(b,2)}

举例如下：有两张表分为L表和R表。

L表

| A    | B    |
| ---- | ---- |
| a1   | b1   |
| a2   | b2   |
| a3   | b3   |

R表

| B    | C    |
| ---- | ---- |
| b1   | c1   |
| b2   | c2   |
| b4   | c3   |

- 左外连接 ：`select L.`*`,R.`*` from L left join R on L.b=R.b`

  | A    | B    | B    | C    |
  | ---- | ---- | ---- | ---- |
  | a1   | b1   | b1   | c1   |
  | a2   | b2   | b2   | c2   |
  | a3   | b3   | null | null |

- 右外连接：`select L.`*`,R.`*` from L right join R on L.b=R.b`

  | B    | C    | A    | B    |
  | ---- | ---- | ---- | ---- |
  | b1   | c1   | a1   | b1   |
  | b2   | c2   | a2   | b2   |
  | b4   | c3   | null | null |

- 内连接：`select L.`*`,R.`*` from L inner join R on L.b=R.b`

  | A    | B    | B    | C    |
  | ---- | ---- | ---- | ---- |
  | a1   | b1   | b1   | c1   |
  | a2   | b2   | b2   | c2   |

- 交叉连接：`select L.`*`,R.`*` from L,R`

  | A    | B    | B    | C    |
  | ---- | ---- | ---- | ---- |
  | a1   | b1   | b1   | c1   |
  | a1   | b1   | b2   | c2   |
  | a1   | b1   | b4   | c3   |
  | a2   | b2   | b1   | c1   |
  | a2   | b2   | b2   | c2   |
  | a2   | b2   | b4   | c3   |
  | a3   | b3   | b1   | c1   |
  | a3   | b3   | b2   | c2   |
  | a3   | b3   | b4   | c3   |



#### mysql中in和exists的区别？　＊＊

in和exists一般用于子查询。

- 使用**exists时会先进行外表查询**，将查询到的每行数据带入到内表查询中看是否满足条件；使用**in一般会先进行内表查询**获取结果集，然后对外表查询匹配结果集，返回数据。 
- in在内表查询或者外表查询过程中**都会用到索引**。 
- exists仅在**内表查询**时会用到索引 
- 一般来说，当子查询的结果集比较大，外表较小使用exist效率更高；当子查询寻得结果集较小，外表较大时，使用in效率更高。 
- 对于not in和not exists，not exists效率比not in的效率高，与子查询的结果集无关，因为not in对于内外表都进行了全表扫描，没有使用到索引。not exists的子查询中可以用到表上的索引。 

#### where和having区别
 where:
where**是在结果返回之前起约束作用的**;
where中**不能使用聚合函数**(sum、count、max、avg）。

 having:
having是一个过滤声明;
在查询返回结果集以后，**对查询结果进行的过滤操作**;
在having中可以使用聚合函数（需配合group by使用)。 


#### varchar和char的区别？　＊＊＊

- varchar表示变长，char表示长度固定。当所插入的字符超过他们的长度时，在严格模式下，会拒绝插入并提示错误信息，在一般模式下，会截取后插入。如char(5)，无论插入的字符长度是多少，长度都是5，插入字符长度小于5，则用空格补充。对于varchar(5)，如果插入的字符长度小于5，则存储的字符长度就是插入字符的长度，不会填充。 
- 存储容量不同，对于char来说，最多能存放的字符个数为255。对于varchar，最多能存放的字符个数是65532。 
- 存储速度不同，char长度固定，存储速度会比varchar快一些，但在空间上会占用额外的空间，属于一种空间换时间的策略。而varchar空间利用率会高些，但存储速度慢，属于一种时间换空间的策略。 



#### MySQL中int(10)和char(10)和varchar(10)的区别？　＊＊＊

int(10)中的10表示的是显示数据的长度，而char(10)和varchar(10)表示的是存储数据的大小。



#### drop、delete和truncate的区别？　＊＊

|          | drop                               | delete                               | truncate                     |
| -------- | ---------------------------------- | ------------------------------------ | ---------------------------- |
| 速度     | 快                                 | 逐行删除，慢                         | 较快                         |
| 类型     | DDL                                | DML                                  | DDL                          |
| 回滚     | 不可回滚                           | 可回滚                               | 不可回滚                     |
| 删除内容 | 删除整个表，数据行、索引都会被删除 | 表结构还在，删除表的一部分或全部数据 | 表结构还在，删除表的全部数据 |

一般来讲，删除整个表，使用drop，删除表的部分数据使用delete，保留表结构删除表的全部数据使用truncate。



#### UNION和UNION ALL的区别？　＊＊

union和union all的作用都是将两个结果集合并到一起。

- union会对结果去重并[排序]()，union all直接直接返回合并后的结果，不去重也不进行[排序]()。 
- union all的性能比union性能好。 



#### 了解慢日志查询吗？统计过慢查询吗？对慢查询如何优化？　＊＊＊

慢查询一般用于记录执行时间超过某个临界值的SQL语句的日志。

相关参数：

- slow_query_log：是否开启慢日志查询，1表示开启，0表示关闭。 
- slow_query_log_file：MySQL数据库慢查询日志存储路径。 
- long_query_time：慢查询阈值，当SQL语句查询时间大于阈值，会被记录在日志上。 
- log_queries_not_using_indexes：未使用索引的查询会被记录到慢查询日志中。 
- log_output：日志存储方式。“FILE”表示将日志存入文件。“TABLE”表示将日志存入数据库。 

如何对慢查询进行优化？

- 分析语句的执行计划，查看SQL语句的索引是否命中 
- 优化数据库的结构，将字段很多的表分解成多个表，或者考虑建立中间表。 
- 优化LIMIT分页。 



#### 为什么要设置主键？　＊＊

主键是唯一区分表中每一行的唯一标识，如果没有主键，更新或者删除表中特定的行会很困难，因为不能唯一准确地标识某一行。



#### 主键一般用自增ID还是UUID？　＊＊

使用自增ID的好处：

- 字段长度较uuid会小很多。 
- 数据库自动编号，按顺序存放，利于检索 
- 无需担心主键重复问题 

使用自增ID的缺点：

- 因为是自增，在某些业务场景下，容易被其他人查到业务量。 
- 发生数据迁移时，或者表合并时会非常麻烦 
- 在高并发的场景下，竞争自增锁会降低数据库的吞吐能力 

UUID：通用唯一标识码，UUID是基于当前时间、计数器和硬件标识等数据计算生成的。

使用UUID的优点：

- 唯一标识，不会考虑重复问题，在数据拆分、合并时也能达到全局的唯一性。 
- 可以在应用层生成，提高数据库的吞吐能力。 
- 无需担心业务量泄露的问题。 

使用UUID的缺点：

- 因为UUID是随机生成的，所以会发生随机IO，影响插入速度，并且会造成硬盘的使用率较低。 
- UUID占用空间较大，建立的索引越多，造成的影响越大。 
- UUID之间比较大小较自增ID慢不少，影响查询速度。 

最后说下结论，一般情况MySQL**推荐使用自增ID**。因为在MySQL的InnoDB存储引擎中，主键索引是一种**聚簇索引**，主键索引的B+树的叶子节点按照顺序存储了主键值及数据，如果主键索引是自增ID，只需要按顺序往后排列即可，如果是UUID，ID是随机生成的，在数据插入时会造成大量的数据移动，产生大量的内存碎片，造成插入性能的下降。



#### 字段为什么要设置成not null?　＊＊

首先说一点，NULL和空值是不一样的，空值是不占用空间的，而NULL是占用空间的，所以字段设为NOT NULL后仍然可以插入空值。

字段设置成not null主要有以下几点原因：

- NULL值会影响一些函数的统计，如count，遇到NULL值，这条记录不会统计在内。

- B树不存储NULL，所以索引用不到NULL，会造成第一点中说的统计不到的问题。

- NOT IN子查询在有NULL值的情况下返回的结果都是空值。

  例如user表如下

  | id   | username |
  | ---- | -------- |
  | 0    | zhangsan |
  | 1    | lisi     |
  | 2    | null     |

  `select * from `user` where username NOT IN (select username from `user` where id != 0)`，这条查询语句应该查到zhangsan这条数据，但是结果显示为null。

- MySQL在进行比较的时候，NULL会参与字段的比较，因为NULL是一种比较特殊的数据类型，数据库在处理时需要进行特数处理，增加了数据库处理记录的复杂性。

#### 如何优化查询过程中的数据访问？　＊＊＊

从减少数据访问方面考虑：

- 正确使用索引，尽量做到索引覆盖 
- 优化SQL执行计划 

从返回更少的数据方面考虑：

- 数据分页处理 
- 只返回需要的字段 

从减少服务器CPU开销方面考虑：

- 合理使用[排序]() 
- 减少比较的操作 
- 复杂运算在客户端处理 

从增加资源方面考虑：

- 客户端多进程并行访问 
- 数据库并行处理 

#### 如何优化长难的查询语句？　＊＊

- 将一个大的查询分解为多个小的查询 
- 分解关联查询，使缓存的效率更高 

#### 如何优化LIMIT分页？　＊＊

- 在LIMIT偏移量较大的时候，查询效率会变低，可以记录每次取出的最大ID，下次查询时可以利用ID进行查询
- 建立复合索引

#### 如何优化UNION查询　＊＊

如果不需要对结果集进行去重或者[排序]()建议使用UNION ALL，会好一些。

#### 如何优化WHERE子句　＊＊＊

- 不要在where子句中使用!=和<>进行不等于判断，这样会导致放弃索引进行全表扫描。 
- 不要在where子句中使用null或空值判断，尽量设置字段为not null。 
- 尽量使用union all代替or 
- 在where和order by涉及的列建立索引 
- 尽量减少使用in或者not in，会进行全表扫描 
- 在where子句中使用参数会导致全表扫描 
- 避免在where子句中对字段及进行表达式或者函数操作会导致存储引擎放弃索引进而全表扫描 

#### SQL语句执行的很慢原因是什么？　＊＊＊

- 如果SQL语句只是偶尔执行很慢，可能是执行的时候遇到了锁，**也可能是redo log日志写满了**，要将redo log中的数据同步到磁盘中去。 
- 如果SQL语句一直都很慢，可能是字段上没有索引或者字段有索引但是没用上索引。 



## 数据库优化
### sql语句优化
1.优化insert语句: 多条插入语句写成一条  
2.避免可能导致索引失效的查询 
3.使用limit的sql语句当偏移量较大时使用子查询  
```sql
原:select * from employees limit 200000,10;  
优化:select * from employees where emp_no >= (select emp_no from employees limit 200000,1) limit 10;
优化思路：子查询在主键索引上进行查找，避免了回表。然后再从这里根据主键值选相应的记录。
```
### 索引优化
#### 适合创建索引的字段
1.经常作**查询选择**的字段  
2.经常作**表连接**的字段  
3.经常出现在 order by, group by, distinct 后面的字段  
4.有**唯一性限制**

#### 不适合创建索引的字段
1.数据量小  
2.有大量重复数据的列上(查询效率)  
3.经常更新的字段

### 表结构优化
选择合适数据类型  
表的范式的优化  
第一范式（1NF）：数据库表中的字段都是单一属性的，不可再分。
| 学号 | 姓名 | 系名 | 系主任 | 课程号 | 成绩 |
| ------| ------ | ------ | ------ | ------ | ------ |

第二范式（2NF）：数据库表中不存在非主键字段对主键的部分函数依赖  

| 学号 | 姓名 | 系名 | 系主任 |
| ------| ------ | ------ | ------ |

| 课程号 | 成绩 |
| ------ | ------ |

第三范式（3NF）：在第二范式的基础上，数据表中如果不存在非主属性对任一主属性的传递函数依赖则符合第三范式。  

| 学号 | 姓名 | 
| ------| ------ |

|系名 | 系主任 |
| ------| ------ |

| 课程号 | 成绩 |
| ------ | ------ |




### 实战:一条sql语句慢（优化）？
1.原因:索引失效/没有索引/sql语句  
2.优化过程  
a.开启慢查询——my.ini定义慢查询时间或者在mysql中使用show_query_log=on开启慢查询  
b.explain分析慢查询  

**没有索引：为合适字段创建索引**

**索引失效：检查sql语句语法，分析哪里造成了索引失效(or、函数计算、最左前缀、模糊查询)。**

**其他情况：优化sql语句、表结构。**

type：表示MySQL在表中找到所需行的方式，或者叫访问类型  
type=ALL，全表扫描，MySQL遍历全表来找到匹配行  
type=index，索引全扫描  
type=range，索引范围扫描  
type=eq_ref，唯一索引  
type=NULL，MySQL不用访问表或者索引，直接就能够得到结果（性能最好）

possible_keys: 表示查询可能使用的索引

key: 实际使用的索引

key_len: 使用索引字段的长度

rows: 扫描行的数量

Extra：  
using index：覆盖索引，不回表  
using where：回表查询  
using filesort：需要额外的排序，不能通过索引得到排序结果

### 大表如何优化？　＊＊＊

- 限定数据的范围：避免不带任何限制数据范围条件的查询语句。 
- 读写分离：主库负责写，从库负责读。 
- 垂直分表：将一个表按照字段分成多个表，每个表存储其中一部分字段。 
- 水平分表：在同一个数据库内，把一个表的数据按照一定规则拆分到多个表中。 
- 对单表进行优化：对表中的字段、索引、查询SQL进行优化。 
- 添加缓存 

#### 什么是垂直分表、垂直分库、水平分表、水平分库？　＊＊＊

垂直分表：将一个表按照字段分成多个表，每个表存储其中一部分字段。一般**会将常用的字段放到一个表中**，**将不常用的字段放到另一个表中。**

垂直分表的优势：

- **避免IO竞争减少锁表的概率**。因为大的字段效率更低，第一数据量大，需要的读取时间长。第二，大字段占用的空间更大，单页内存储的行数变少，会使得IO操作增多。
- 可以更好地提升热门数据的查询效率。

垂直分库：按照业务对表进行分类，部署到不同的数据库上面，不同的数据库可以放到不同的服务器上面。

垂直分库的优势：

- 降低业务中的耦合，方便对不同的业务进行分级管理。 
- 可以提升IO、数据库连接数、解决单机硬件资源的瓶颈问题。 

垂直拆分（分库、分表）的缺点：

- 主键出现冗余，需要管理冗余列 
- 事务的处理变得复杂 
- 仍然存在单表数据量过大的问题 

水平分表：在同一个数据库内，把同一个表的数据按照一定规则拆分到多个表中。

水平分表的优势：

- 解决了单表数据量过大的问题 
- 避免IO竞争并减少锁表的概率 

水平分库：把同一个表的数据按照一定规则拆分到不同的数据库中，不同的数据库可以放到不同的服务器上。

水平分库的优势：

- 解决了单库大数据量的瓶颈问题 
- IO冲突减少，锁的竞争减少，某个数据库出现问题不影响其他数据库（可用性），提高了系统的稳定性和可用性 

水平拆分（分表、分库）的缺点：

- 分片事务一致性难以解决 
- 跨节点JOIN性能差，逻辑会变得复杂 
- 数据扩展难度大，不易维护 

在系统设计时应根据业务耦合来确定垂直分库和垂直分表的方案，在数据访问压力不是特别大时应考虑缓存、读写分离等方法，若数据量很大，或持续增长可考虑水平分库分表，水平拆分所涉及的逻辑比较复杂，常见的方案有客户端架构和恶代理架构。

### 分库分表后，ID键如何处理？　＊＊＊

分库分表后不能每个表的ID都是从1开始，所以需要一个全局ID，设置全局ID主要有以下几种方法：

- UUID：优点：本地生成ID，不需要远程调用；全局唯一不重复。缺点：占用空间大，不适合作为索引。

- 数据库自增ID：在分库分表表后使用数据库自增ID，需要一个专门用于生成主键的库，每次服务接收到请求，先向这个库中插入一条没有意义的数据，获取一个数据库自增的ID，利用这个ID去分库分表中写数据。优点：简单易实现。缺点：在高并发下存在瓶颈。系统结构如下图（图片来源于网络）

  ![图片说明](https://uploadfiles.nowcoder.com/images/20210908/975641190_1631058342197/6586A5077D34DE1267461D166FE412F9) 

- Redis生成ID：优点：不依赖数据库，性能比较好。缺点：引入新的组件会使得系统复杂度增加

- Twitter的snowflake[算法]()：是一个64位的long型的ID，其中有1bit是不用的，41bit作为毫秒数，10bit作为工作机器ID，12bit作为序列号。

  1bit：第一个bit默认为0，因为二进制中第一个bit为1的话为负数，但是ID不能为负数.

  41bit：表示的是时间戳，单位是毫秒。

  10bit：记录工作机器ID，其中5个bit表示机房ID，5个bit表示机器ID。

  12bit：用来记录同一毫秒内产生的不同ID。

- [美团]()的Leaf分布式ID生成系统，[美团点评分布式ID生成系统](https://tech.meituan.com/2017/04/21/mt-leaf.html)

### MySQL的复制原理及流程？如何实现主从复制？　＊＊＊

MySQL复制：为**保证主服务器和从服务器的数据一致性**，在向主服务器插入数据后，**从服务器会自动将主服务器中修改的数据同步**过来。

主从复制的作用：**高可用和故障转移、负载均衡、数据备份、升级测试** 

主从复制的原理：

主从复制主要有三个线程：binlog线程，I/O线程，SQL线程。

- **binlog线程：负责将主服务器上的数据更改写入到二进制日志（Binary log）中。** 
- **I/O线程：负责从主服务器上读取二进制日志（Binary log），并写入从服务器的中继日志（Relay log）中。** 
- **SQL线程：负责读取中继日志，解析出主服务器中已经执行的数据更改并在从服务器中重放** 

复制过程如下（图片来源于网络）：

![图片说明](https://uploadfiles.nowcoder.com/images/20210908/975641190_1631058361359/E8FDAF1ABF59E596BD35B262F933C4D1) 

1. Master在每个事务更新数据完成之前，将操作记录写入到binlog中。 
2. Slave从库连接Master主库，并且Master有多少个Slave就会创建多少个binlog dump线程。当Master节点的binlog发生变化时，binlog dump会通知所有的Slave，并将相应的binlog发送给Slave。 
3. I/O线程接收到binlog内容后，将其写入到中继日志（Relay log）中。 
4. SQL线程读取中继日志，并在从服务器中重放。 

这里补充一个通俗易懂的图。

![图片说明](https://uploadfiles.nowcoder.com/images/20210908/975641190_1631058400656/464F14CF483B16A8E405700C72FC1A44) 



### 了解读写分离吗？　＊＊＊

读写分离主要依赖于**主从复制**，主从复制为读写分离服务。

读写分离的优势：

- **主服务器负责写**，**从服务器负责读**，缓解了锁的竞争 
- 从服务器可以使用MyISAM，提升查询性能及节约系统开销 
- 增加冗余，提高可用性

## 三大日志
MySQL InnoDB 引擎使用 redo log(重做日志) 保证事务的持久性，让Mysql有了崩溃恢复的能力，使用 undo log(回滚日志) 来保证事务的原子性。

MySQL数据库的数据备份、主备、主主、主从都离不开binlog(归档日志)，需要依靠binlog来同步数据，保证数据一致性。
### redo log
redo log（重做日志）是InnoDB存储引擎独有的，它让MySQL拥有了**崩溃恢复**能力。**记录内容是在某个数据页上做了什么修改**。  



#### 查询过程（mysql缓存）  

MySQL 中**数据是以页为单位**，你查询一条记录，会从硬盘把一页的数据加载出来，加载出来的数据叫数据页，会放入到 Buffer Pool 中。

后续的查询都是先从 Buffer Pool 中找，没有命中再去硬盘加载，减少硬盘 IO 开销，提升性能。

更新表数据的时候，也是如此，发现 Buffer Pool 里存在要更新的数据，就直接在 Buffer Pool 里更新。

**然后会把“在某个数据页上做了什么修改”记录到重做日志缓存（redo log buffer）里，接着刷盘到 redo log 文件里。**



#### redo log刷盘策略

**日志先写到文件缓存（page cache）里，接着刷盘到 redo log 文件里。**

InnoDB提供参数innodb_flush_log_at_trx_commit参数控制page cache
同步到redo log file。默认为1，每次提交事务都同步，设置为2时redo log buffer刷盘至page cache（文件系统缓存）并不是直接刷到磁盘中，只是刷到文件系统缓存(page cache)中，操作系统决定什么时候将page cache刷到磁盘中。设置为0时每次事务提交不进行刷盘，隔一段时间刷盘。

##### mysql使用querycache(query cache-一级缓存，区别于buffer pool-二级缓存)优缺点

query cache只能读，buffer pool可读可写。

优点：减少了大量的磁盘I/O，导致效率非常高。

 缺点：

mysql缓存生效必须是同样的语句，**条件苛刻**。

缓存的内存分配问题，不可避免地产生一些**内存碎片**。

当表内容发生变化或者表**结构发生变化**，对应的查询**缓存内容都会失效**。

### binlog

`binlog` 是**逻辑日志，记录内容是语句的原始逻辑**,一般用于主从备份，主主备份。

#### 记录格式

`binlog` 日志有三种格式，可以通过`binlog_format`参数指定。

- **statement**
- **row**
- **mixed**

指定`statement`，**记录的内容是`SQL`语句原文**，比如执行一条`update T set update_time=now() where id=1`，记录的内容如下。

![img](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/02-20220305234738688.png)

同步数据时，会执行记录的`SQL`语句，但是有个问题，`update_time=now()`这里会获取当前系统时间，直接执行会**导致与原库的数据不一致。**



为了解决这种问题，我们需要指定为`row`，记录的内容不再是简单的`SQL`语句了，还包含操作的具体数据，记录内容如下。

![img](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/03-20220305234742460.png)

**`row`格式记录的内容看不到详细信息，要通过`mysqlbinlog`工具解析出来。**

`update_time=now()`变成了具体的时间`update_time=1627112756247`，条件后面的@1、@2、@3 都是该行数据第 1 个~3 个字段的原始值（**假设这张表只有 3 个字段**）。

这样就能保证同步数据的一致性，通常情况下都是指定为`row`，这样**可以为数据库的恢复与同步带来更好的可靠性**。

但是这种格式，**需要更大的容量**来记录，比较占用空间，恢复与同步时会更消耗`IO`资源，影响执行速度。



所以就有了一种折中的方案，指定为**`mixed`**，记录的内容是前两者的混合。

`MySQL`会判断这条`SQL`语句是否可能引起数据不一致，如果是，就用`row`格式，否则就用`statement`格式。

#### 写入机制
binlog的写入时机也非常简单，事务执行过程中，先把日志写到binlog cache，事务提交的时候，再把binlog cache写到binlog文件中。  
刷盘时机由sync_binlog控制，默认是0。系统自行判断什么时候刷盘。为1时，事务提交都会执行刷盘。设置为N时，累计N个事务提交执行刷盘。

#### 两阶段提交
解决由于两份日志写入时机不同导致redo log和binlog之间不一致的问题（`redo log`在事务执行过程中可以不断写入，而`binlog`只有在提交事务时才写入，可能事务提交后redo log已经写完，但`binlog`没写完就异常，这时候`binlog`里面没有对应的修改记录），InnoDB存储引擎使用两阶段提交方案。  
将redo log的写入拆成了两个步骤prepare和commit，这就是两阶段提交。
![avatar](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/04-20220305234956774.png)

使用两阶段提交后，写入binlog时发生异常也不会有影响，因为MySQL根据redo log日志恢复数据时，发现redo log还处于prepare阶段，并且没有对应binlog日志，就会回滚该事务。
![avatar](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/05-20220305234937243.png)

redo log设置commit阶段发生异常,并不会回滚事务，它会执行上图框住的逻辑，虽然redo log是处于prepare阶段，但是能通过事务id找到对应的binlog日志，所以MySQL认为是完整的，就会提交事务恢复数据。
![avatar](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/06-20220305234907651.png)

### undo log
保证事务的原子性，在异常发生时，对已经执行的操作进行回滚、以及实现MVCC

undo log是一个逻辑日志，记录变化过程，例如执行一个delete，undo log会记录一个相反的insert。数据库事务开始之前，将要修改的记录存放到Undo log里，当事务回滚或数据库崩溃时利用Undo log对未提交的事务进行回滚操作。

**在 `InnoDB` 存储引擎中 `undo log` 分为两种： `insert undo log` 和 `update undo log`：**

1. **`insert undo log`** ：指在 `insert` 操作中产生的 `undo log`。因为 `insert` 操作的记录，不加锁情况下会对别的事务可见，造成幻读，加锁情况下只对事务本身可见，对其他事务不可见，故该 `undo log` 可以在事务提交后直接删除。不需要进行 `purge` 操作
2. **`update undo log`** ：`update` 或 `delete` 操作中产生的 `undo log`。该 `undo log`可能需要提供 `MVCC` 机制，因此不能在事务提交时就进行删除。提交时放入 `undo log` 链表，等待 `purge线程` 进行最后的删除。

**delete产生的undo log为什么不直接删除？**

了解了一下为什么事务执行过程中不会出现别的事务将记录删除造成当前事务查询不到记录的问题出现，是因为**delete操作**产生的undo log会分为两个阶段，阶段一：仅仅将delete_mask标识位设置为1，这个阶段称为**delete mask**，此时记录并未删除；阶段二：当该删除语句所在的事务提交之后，会有专门的线程来真正把记录删除掉，其实就是把该记录从正常记录链表中移除，并加入到垃圾链表中，然后还要调整一些页面其他信息。这个阶段被称为**purge**。







# redis

[Redis面试题](https://www.nowcoder.com/discuss/837063?type=all&order=recall&pos=&page=1&ncTraceId=&channel=-1&source_id=search_all_nctrack&gio_id=458B6C726D5F0BB6AD6B9AA68D870748-1647274644169)

## 简单介绍一下 Redis 

* Redis是基于C语言编写的，而且是内存中的数据库，读写速度很快。
* 除了来做缓存外、Redis 也经常用来做分布式锁，甚至是消息队列
* 同时还支持**事务 、持久化、Lua 脚本、多种集群方案。**

### 追问：与 Memcached 的区别是什么？**

现在公司一般都是用 Redis 来实现缓存，而且 Redis 自身也越来越强大了！不过，了解 Redis 和 Memcached 的区别和共同点，有助于我们在做相应的技术选型的时候，能够做到有理有据！

**共同点** ：

1. 都是基于内存的数据库，一般都用来当做缓存使用。 
2. 都有过期策略。 
3. 两者的性能都非常高。 

**区别** ：

1. Redis 支持**更丰富的数据类型**（支持更复杂的应用场景）。Redis 不仅仅支持简单的 k/v 类型的数据，同时还提供 list，set，zset，hash 等数据结构的存储。Memcached 只支持最简单的 k/v 数据类型。 
2. Redis **支持数据的持久化**，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用,而 Memecache 把数据全部存在内存之中。 
3. Redis **有灾难恢复机制**。因为可以把缓存中的数据持久化到磁盘上。 
4. Redis 在**服务器内存使用完之后，可以将不用的数据放到磁盘上**。但是，Memcached 在服务器内存使用完之后，就会直接报异常。 
5. Memcached 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据；但是 Redis 目前是**原生支持 cluster 模式**的。 
6. Memcached 是多线程，非阻塞 IO 复用的网络模型；Redis 使用**单线程的多路 IO 复用**模型。（Redis 6.0 引入了多线程 IO ） 
7. Redis 支持发布订阅模型、Lua 脚本、事务等功能，而 Memcached 不支持。并且，Redis 支持更多的编程语言。 
8. Memcached过期数据的删除策略只用了**惰性删除**，而 Redis 同时使用了**惰性删除**与**定期删除**。 

## Redis有哪些数据类型？

## Redis优缺点

**优点**：

1. **基于内存操作**，内存读写速度快。
2. Redis是**单线程**的，避免线程切换开销及多线程的竞争问题。单线程是指网络请求使用一个线程来处理，即一个线程处理所有网络请求，Redis 运行时不止有一个线程，比如数据持久化的过程会另起线程。
3. **支持多种数据类型**，包括String、Hash、List、Set、ZSet等。
4. **支持持久化**。Redis支持RDB和AOF两种持久化机制，持久化功能可以有效地避免数据丢失问题。
5. **支持事务**。Redis的所有操作都是原子性的，同时Redis还支持对几个操作合并后的原子性执行。
6. **支持主从复制**。主节点会自动将数据同步到从节点，可以进行读写分离。

**缺点：**

1. 对结构化查询的支持比较差。
2. 数据库容量受到物理内存的限制，不适合用作[海量数据](https://www.nowcoder.com/jump/super-jump/word?word=海量数据)的高性能读写，因此Redis适合的场景主要局限在较小数据量的操作。
3. Redis 较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。



## Redis为什么这么快？

- **基于内存**：Redis是使用内存存储，没有磁盘IO上的开销。数据存在内存中，读写速度快。

- **单线程实现**（ Redis 6.0以前）：Redis使用单个线程处理请求，避免了多个线程之间线程切换和锁资源争用的开销。
- **IO多路复用模型**：Redis 采用 IO 多路复用技术。Redis 使用单线程来轮询描述符，将数据库的操作都转换成了事件，不在网络I/O上浪费过多的时间。
- **高效的数据结构**：Redis 每种数据类型底层都做了优化，目的就是为了追求更快的速度。

## Redis为何选择单线程

- 避免过多的**上下文切换开销**。程序始终运行在进程中单个线程内，没有多线程切换的场景。
- **避免同步机制的开销**：如果 Redis选择多线程模型，需要考虑数据同步的问题，则必然会引入某些同步机制，会导致在操作数据过程中带来更多的开销，增加程序复杂度的同时还会降低性能。
- **实现简单，方便维护**：如果 Redis使用多线程模式，那么所有的底层数据结构的设计都必须考虑线程安全问题，那么 Redis 的实现将会变得更加复杂。

## Redis6.0为何引入多线程？

Redis支持多线程主要有两个原因：

- 可以充分利用服务器 CPU 资源，单线程模型的主线程只能利用一个cpu；
- 多线程任务可以分摊 Redis 同步 IO 读写的负荷。



## **常见的有五种基本数据类型及其数据结构**

**（详细底层数据结构见下一部分）**

#### 1 **String**

Redis 构建了⼀种简单动态字符串（simple dynamic string，SDS）,Redis 的 SDS可以保存文本和二进制数据，并且获取字符串长度的复杂度为 O(1)（C 字符串为 O(N)）,除此之外,Redis 的 SDS API 是安全的，不会造成缓冲区溢出。

*应用场景*：⼀般常用在需要计数的场景,用户的访问次数、热点文章的点赞转发数量。

底层实现：SDS

#### **2 list**

list 是双向链表。易于数据元素的插⼊和删除 并且可以灵活调整链表长度，但是链表的随机访问困难。 C 语言没有实现链表，所以 Redis 实现了自己的链表数据结构。Redis 的 list 的实现为⼀个 双向链表，即可以支持反向查找和遍历，更方便操作，不过带来了部分额外的内存开销。

*应用场景*：实现消息队列、慢查询

底层实现：quickList（双向列表+压缩列表）

#### **3 hash**

类似于 JDK1.8 前的 HashMap，使用数组 + 链表的数据结构。不过， Redis 的 hash 做了更多优化。另外，hash 是⼀个 string 类型的 field 和 value 的映射表， 特别适合用于存储对象，可以直接仅仅修改这个对象中的某个字段的值。 比如我们可以 hash 数据结构来存储用户信息，商品信息等等。

*应用场景*：系统中对象数据的存储

底层实现：哈希表



#### **4 set**

类似于 Java 中的 HashSet 。Redis 中的 set 类型是⼀种无序集合。当你需要存储⼀个列表数据，又不希望出现重复数据时，set 是⼀个很好的选择，并且 set 提供了判断某个成员是否在⼀个 set 集合内的重要接口，这个也是 list 所不能提供的。可以基于 set 轻易实现交集、并集、差集的操作。比如：你可以将⼀个用户所有的关注⼈存在⼀个集合中，将其所有粉丝存在⼀个集合。Redis 可以非常方便的实现如共同关注、共同粉丝、共同喜好等功能。这个过程也就是求交集的过程。

*应用场景*：需要存放的数据不能重复以及需要获取多个数据源交集和并集等场景

底层实现：哈希表/intset

#### **5 zset**

和 set 相比，zset 增加了⼀个权重参数 score，使得集合中的元素能够按 score 进行有序排列，还可以通过 score 的范围来获取元素的列表。有点像是 Java 中 HashMap 和 TreeSet 的结合体。

*应用场景*：需要对数据根据某个权重进行排序的场景。比如在直播系统中，实时排行信息包含直播间在线用户列表，各种礼物排行榜，弹幕消息（可以理解为按消息维度的消息排行榜）等信息。

底层实现：两种编码ziplist+skiplist，ziplist在一定条件会转为skiplist，skiplist使用跳表+哈希表作为底层数据结构(不过后续版本有更新)

#### redis数据结构

##### 数据结构--SDS

a) 针对C语言中字符串两个问题优化

- 字符串的结尾是以 “\0” 字符标识，字符串里面不能包含有 “\0” 字符，因此不能保存二进制数据；
- **字符串操作函数**不高效且不安全，比如**有缓冲区溢出的风险**，有可能会造成程序运行终止；

b) 结构设计

- **len，记录了字符串长度**。这样获取字符串长度的时候，只需要返回这个成员变量值就行，时间复杂度只需要 O（1）。
- **alloc，分配给字符数组的空间长度**。这样在修改字符串的时候，可以通过 `alloc - len` 计算出剩余的空间大小，可以用来判断空间是否满足修改需求，如果不满足的话，就会自动将 SDS  的空间扩展至执行修改所需的大小，然后才执行实际的修改操作，所以使用 SDS 既不需要手动修改 SDS 的空间大小，也不会出现前面所说的缓冲区溢出的问题。
- **flags，用来表示不同类型的 SDS**。一共设计了 5 种类型，分别是 sdshdr5、sdshdr8、sdshdr16、sdshdr32 和 sdshdr64，**为了能灵活保存不同大小的字符串，从而有效节省内存空间**。
- **buf[]，字符数组，用来保存实际数据**。不仅可以保存字符串，也可以保存二进制数据。



##### 数据结构--intset

```c
typedef struct intset {
    uint32_t encoding;
    uint32_t length;
    int8_t contents[];
} intset;
    
```

`encoding` 表示编码方式，的取值有三个：INTSET_ENC_INT16, INTSET_ENC_INT32, INTSET_ENC_INT64

`length` 代表其中存储的整数的个数

`contents` 指向实际存储数值的连续内存区域, 就是一个数组；整数集合的每个元素都是 contents 数组的一个数组项（item），各个项在数组中按值得大小**从小到大有序排序**，且数组中不包含任何重复项。（虽然 intset 结构将 contents 属性声明为 int8_t 类型的数组，但实际上 contents 数组并不保存任何 int8_t 类型的值，contents 数组的真正类型取决于 encoding 属性的值）

##### 数据结构--哈希表

1） 类似于jdk中结构，链表法解决哈希冲突但不涉及红黑树结构

存放两个哈希表(rehash操作)，每个哈希表存放一个dictEntry数组，每个dictEntry对象保存指向下一个dictEntry对象的指针

2）rehash简介：

随着数据逐步增多，触发了 rehash 操作，这个过程分为三步：

- 给「哈希表 2」 分配空间，一般会比「哈希表 1」 大 2 倍；
- 将「哈希表 1 」的数据迁移到「哈希表 2」 中；
- 迁移完成后，「哈希表 1 」的空间会被释放，并把「哈希表 2」 设置为「哈希表 1」，然后在「哈希表 2」 新创建一个空白的哈希表，为下次 rehash 做准备。

 Redis 采用了**渐进式 rehash**，也就是将数据的迁移的工作不再是一次性迁移完成，而是分多次迁移(查询元素先表1后表2，新增元素仅表2)。

![图片](https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZc0xXMUofrwAVS3mR0DxcWCfIdllCPYjTQo6Dicibw41samsSSa36ibGpEoaTFbb5WhvVgoe2ZavO5Vw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



##### 数据结构--quicklist

「双向链表 + 压缩列表」组合，因为一个 quicklist 就是一个链表，而链表中的每个元素又是一个压缩列表(改进的压缩列表)。

**通过控制每个链表节点中的压缩列表的大小或者元素个数，来减小连锁更新带来的影响，从而提供了更好的访问性能。**

##### 数据结构--跳表

a) 结构

跳表是一个**带有层级关系的链表**，包含跳表头尾结点，跳表长度，跳表最大层数，每一层级节点通过指针连接起来，实现这一特性就是靠**跳表节点结构体**中的**zskiplistLevel 结构体类型的 level 数组**。

level 数组中的每一个元素代表跳表的一层，也就是由 zskiplistLevel 结构体表示，比如 leve[0] 就表示第一层，leve[1] 就表示第二层。zskiplistLevel 结构体里定义了**「指向下一个跳表节点的指针」和「跨度」**，跨度时用来记录两个节点之间的距离。

b) 查询过程

查找一个跳表节点的过程时，跳表会从头节点的最高层开始，逐一遍历每一层。在遍历某一层的跳表节点时，会用跳表节点中的 SDS 类型的元素和元素的权重来进行判断，共有两个判断条件：

- 如果当前节点的权重「小于」要查找的权重时，跳表就会访问该层上的下一个节点。
- 如果当前节点的权重「等于」要查找的权重时，并且当前节点的 SDS 类型数据「小于」要查找的数据时，跳表就会访问该层上的下一个节点。

c) 层数设置

**跳表在创建节点时候，会生成范围为[0-1]的一个随机数，如果这个随机数小于 0.25（相当于概率 25%），那么层数就增加 1 层，然后继续生成下一个随机数，直到随机数的结果大于 0.25 结束，最终确定该节点的层数**。作用：每一层结点分布均匀，避免集中在一层查询效率没有提升

##### 数据结构--listpack(整合压缩列表)

前身--压缩列表：**由连续内存块组成的顺序型数据结构**，有点类似于数组。表头记录了尾部节点，节点数量，结束点，列表占用内存字节数。压缩列表节点结构包含三个部分，前一个节点长度（**前一个节点的长度小于 254 字节**，那么 prevlen 属性需要用 **1 字节的空间**来保存这个长度值，**前一个节点的长度大于等于 254 字节**，那么 prevlen 属性需要用 **5 字节的空间**来保存这个长度值，当新增元素内存过大，可能会导致之后节点的prevlen都发生变化**导致连锁更新**），当前节点数据类型及其长度，当前节点的数据。

listpack在压缩列表结点的基础上，取消了prevlen，而是只记录当前节点的长度。

## 事务

Redis 通过 MULTI（开启） 、 DISCARD(取消) 、 EXEC（执行） 和 WATCH（监视） 四个命令来实现事务功能。

redis事务不支持原子性和持久性（**虽然支持持久化**），支持隔离性、一致性。

- redis对单个指令支持原子性((incr,decr)，对多个指令不支持(可借助lua脚本、分布式锁setnx、结合watch和事务也可以解决，每个客户端可以通过**watch来监控自己即将要更新的key**，这样在事务更新的时候，如果发现自己监控的key被修改了，那么拒绝执行，事务执行失败，这样就不会存在更新覆盖的问题。)。



![img](https://pic3.zhimg.com/80/v2-69d17e81c6a90e67d7aea5bd068eae7a_720w.jpg)



1. A先获取value=100
2. B先获取value=100
3. A设置value=101
4. B设置value=101

整个流程下来，发现库存丢了一个，这正是因为整个操作不是原子性的，导致A影响了B，或者B影响了A。

- 因为redis存储于内存，当数据持久化到磁盘时，必定会有一定几率出现故障(无论使用何种持久化方式)，所以并不支持持久性。
- redis是单线程的，所以天生支持事务隔离性。
- 一致性情况比较复杂，可以分为入队错误、执行错误、Redis 进程被终结三个部分。
  – 入队错误： **事务中有错误命令**，会将事务状态设置为 REDIS_DIRTY_EXEC，表明此事务为脏执行数据，当客户端执行 EXEC 命令时， Redis 会**拒绝执行**状态为 REDIS_DIRTY_EXEC 的事务， 并返回失败信息。
  – 执行错误：命令在**事务执行的过程中发生错误**，比如对key进行了错误操作，那么**此条指令会执行错误**，但并**不影响之前和之后的指令执行。**
  – 进程被终结：rdb模式会在事务执行完成后再进行快照存储，所以rdb不会影响一致性。aof模式存储了一半事务，还原数据库时程序会检测到 AOF 文件并不完整，Redis 会退出，并报告错误。需要使用 redis-check-aof 工具将部分成功的事务命令移除之后，才能再次启动服务器。

WATCH 命令的实现：代表数据库的redisDB结构中保存了一个 watched_keys 字典， 它的键是redis数据库被监视的键， 值则是一个链表， 链表中保存了所有监视这个键的客户端。

## Lua

redis使用lua脚本的好处

- 减少网络开销。可以将多个请求通过脚本的形式一次发送，减少网络时延。
- 原子操作。Redis会将整个脚本作为一个整体执行，中间不会被其他请求插入。因此在脚本运行过程中无需担心会出现竞态条件，无需使用事务。
- 复用。客户端发送的脚本会永久存在redis中，这样其他客户端可以复用这一脚本，而不需要使用代码完成相同的逻辑。

## Redis运行机制

### Redis数据结构

```C
typedef struct redisDb {
// 保存着数据库以整数表示的号码
int id;

// 保存着数据库中的所有键值对数据
// 这个属性也被称为键空间（key space）
dict *dict;

// 保存着键的过期信息
dict *expires;

// 实现列表阻塞原语，如 BLPOP
// 在列表类型一章有详细的讨论
dict *blocking_keys;
dict *ready_keys;

// 用于实现 WATCH 命令
// 在事务章节有详细的讨论
dict *watched_keys;
} redisDb
```

### expires过期时间

expires 字典的**键是一个指向 dict 字典（实际用户空间）里某个键的指针**， 值则是键所指向的dict字典键的到期时间， 这个值以 long long 类型表示。
expires 的键和dict的键是同一个字符串对象，不会浪费任何空间。
通过 EXPIRE 、EXPIREAT、PEXPIRE 和 PEXPIREAT 四个命令， 客户端可以给某个存在的键设置过期时间， 当键的过期时间到达时， 键就不再可用。

### Redis删除策略

定时删除、惰性删除、定期删除(这里删除的都是具有expires属性的键)。

- 定时删除是键过期立刻删除。
- 惰性删除是使用到一个键时对其进行判断，如果过期了就删除并返回空。
- 定期删除每隔一段时间执行一次删除操作，并通过限制删除操作执行的时长和频率，籍此来减少删除操作对 CPU 时间的影响。
  **redis使用的是惰性删除和定期删除共用的策略**。

但是，仅仅通过给 key 设置过期时间还是有问题的。因为还是可能存在定期删除和惰性删除漏掉了很多过期 key 的情况。这样就导致大量过期 key 堆积在内存里，然后就Out of memory了。此时需要**Redis内存淘汰机制**解决该问题

### Redis的内存淘汰策略

Redis 提供 6 种数据淘汰策略：

1. **volatile-lru（least recently used）**：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最久未使用的数据淘汰 

2. **volatile-ttl**：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 

3. **volatile-random**：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 

4. **allkeys-lru（least recently used）**：当内存不足以容纳新写入数据时，在键空间中，移除最近最久未使用的 key（这个是最常用的） 

5. **allkeys-random**：从数据集（server.db[i].dict）中任意选择数据淘汰 

6. **no-eviction**：禁止驱逐数据，也就是说当内存不足以容纳新写入数据时，新写入操作会报错。

   **4.0 版本后增加以下两种：**

7. **volatile-lfu（least frequently used）**：从已设置过期时间的数据集(server.db[i].expires)中 挑选最不经常使⽤的数据淘汰 

8. **allkeys-lfu（least frequently used）**：当内存不⾜以容纳新写⼊数据时，在键空间中，移 除最不经常使⽤的 key

### Redis持久化

Redis 提供了两种持久化方式:RDB（默认）快照方式和AOF追加方式。

#### RDB

**RDB(Redis DataBase）**:通过**创建快照来获取存储在内存里面的数据在某个时间点上的副本**。在创建快照之后，用户可以**对快照进行备份**，可以将快照复制到其他服务器从而创建相同数据的服务器副本。会根据指定的规则定时将内存中的数据保存到硬盘中.是**redis默认的持久化方式**

RDB持久化有时间间隔，可以使用SAVE 和 BGSAVE 两个命令手动持久化，都会调用 rdbSave 函数，但它们调用的方式各有不同：

- SAVE 直接调用 rdbSave ，阻塞 Redis 主进程，直到保存完成为止。在主进程阻塞期间，服务器不能处理客户端的任何请求。
- BGSAVE 则 fork 出一个子进程，子进程负责调用 rdbSave ，并在保存完成之后向主进程发送信号，通知保存已完成。因为 rdbSave 在子进程被调用，所以 Redis 服务器在 BGSAVE 执行期间仍然可以继续处理客户端的请求。

#### AOF

**AOF(Append Only File)：**将被执行的**写命令**写到AOF文件的末尾)，将每次执行的命令及时同步到硬盘中(同步策略如下)，恢复时执行日志文件的命令。

优点：实时性更好，丢失的数据更少.

```
ppendfsync always #每次有数据修改发⽣时都会写⼊AOF⽂件,这样会严重降低Redis的速度
appendfsync everysec #每秒钟同步⼀次，显示地将多个写命令同步到硬盘
appendfsync no #让操作系统决定何时进⾏同步
```

为了兼顾数据和**写入性能**，用户可以考虑 **appendfsync everysec 选项** ，让 Redis 每秒同步⼀次 AOF ⽂件，Redis 性能几乎没受到任何影响。而且这样即使出现系统崩溃，用户最多只会丢失⼀ 秒之内产⽣的数据。

AOF重写模式：**重写机制是放在子进程**中的，这样可以使服务器进程（父进程）可以继续处理其他命令。同时他也产生了一个新问题，数据不一致。为了解决这个问题， Redis 增加了一个 **AOF 重写缓存**， 这个缓存在 fork 出子进程之后开始启用， Redis 主进程在接到新的写命令之后， 除了会将这个写命令的协议内容追加到现有的 AOF 文件之外， 还会追加到这个缓存中。

## 使用过Redis分布式锁么，它是怎么实现的？

先拿setnx（SET if Not eXists）来争抢锁，抢到之后，再用expire给锁加一个过期时间防止锁忘记了释放。



## 什么是缓存穿透？如何避免？什么是缓存击穿，如何避免？什么是缓存雪崩？何如避免？

**缓存穿透**

查询为null值的key，redis却没有储存null值， 导致大量请求直接落到数据库

一些恶意的请求会故意查询不存在的key,请求量很大，就会对后端系统造成很大的压力。这就叫做缓存穿透。

**如何避免？**
1：对查询结果为空的情况也进行缓存，缓存时间设置短一点，或者该key对应的数据insert了之后清理缓存。
2：对一定不存在的key进行过滤。可以把所有的可能存在的key放到一个大的Bitmap中，查询时通过该bitmap过滤。

**缓存击穿**：对于设置了过期时间的 key，缓存在某个时间点过期的时候，恰好这时间点对这个 Key 有大量的并发请求过来，这些请求发现缓存过期一般都会从后端 DB 加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把 DB 压垮。

**如何避免？**

1.使用互斥锁：当缓存失效时，不立即去查询数据库，先使用如 Redis 的 setnx 去设置一个互斥锁，第1个进入的线程，获取锁并从数据库去取数据，并将数据写入缓存。

2.设置热点数据永不过期：物理不过期，但逻辑过期（后台异步线程去刷新）。

**缓存雪崩**
当缓存服务器重启或者大量缓存集中在某一个时间段失效，这样在失效的时候，会给后端系统带来很大压力。导致系统崩溃。

**如何避免？**
1：在缓存失效后，通过加锁或者队列来控制读数据库写缓存的线程数量。比如对某个key只允许一个线程查询数据和写缓存，其他线程等待。
2：**做二级缓存**，A1为原始缓存，A2为拷贝缓存，A1失效时，可以访问A2，A1缓存失效时间设置为短期，A2设置为长期
3：**不同的key，设置不同的过期时间，让缓存失效的时间点尽量均匀**

## 如果解决缓存数据不一致问题？

### 先删缓存，再更新数据库(不推荐)

先删除缓存，数据库还没有更新成功，此时如果读取缓存，缓存不存在，**去数据库中读取到的是旧值**，缓存不一致发生。

![图片](https://uploadfiles.nowcoder.com/images/20211201/3639882_1638323426158/6E6A9D5E1F4ED8159F9D0A307CE747CB) 

### 解决方案

#### 延时双删

延时双删的方案的思路是，为了避免更新数据库的时候，其他线程从缓存中读取不到数据，就在更新完数据库之后，再sleep一段时间，然后再次删除缓存。

sleep的时间要对业务读写缓存的时间做出评估，sleep时间大于读写缓存的时间即可。

流程如下：

1. 线程1删除缓存，然后去更新数据库 
2. 线程2来读缓存，发现缓存已经被删除，所以直接从数据库中读取，这时候由于线程1还没有更新完成，所以读到的是旧值，然后把旧值写入缓存 
3. 线程1，根据估算的时间，sleep，由于sleep的时间大于线程2读数据+写缓存的时间，所以缓存被再次删除 
4. 如果还有其他线程来读取缓存的话，就会再次从数据库中读取到最新值 

<img src="https://uploadfiles.nowcoder.com/images/20211201/3639882_1638323505468/907E5A330211493F14DDC0F0991F7F5D" alt="图片说明"  /> 

### 先更新数据库，再删除缓存

如果反过来操作，先更新数据库，再删除缓存呢？

更新数据库成功，**如果删除缓存失败或者还没有来得及删除**，那么，其他线程从缓存中读取到的就是旧值，还是会发生不一致。

![图片](https://uploadfiles.nowcoder.com/images/20211201/3639882_1638323528451/943B9FA66A5ECE6B2772F10F966347A7) 

### 解决方案(保证更新数据库后一定能删除缓存)

#### 1 设置过期时间

如果对于实时性要求不是很高的情况，可以采用这种方案。

因为假设删除缓存失败，读取到的仍是旧数据。

#### 2 消息队列

先更新数据库，**成功后往消息队列发消息，消费到消息后再删除缓存，借助消息队列的重试机制来实现**，达到最终一致性的效果。

![图片](https://uploadfiles.nowcoder.com/images/20211201/3639882_1638323538857/44290FF2DF71C8F4070B39F17A953AFF) 

这个解决方案其实问题更多。

1. 引入消息中间件之后，问题更复杂了，怎么保证消息不丢失更麻烦 
2. 就算更新数据库和删除缓存都没有发生问题，消息的延迟也会带来短暂的不一致性，不过这个延迟相对来说还是可以接受的 

#### 进阶版消息队列

为了解决缓存一致性的问题单独引入一个消息队列，太复杂了。

其实，一般大公司本身都会有监听binlog消息的消息队列存在，主要是为了做一些核对的工作。

这样，我们可以借助监听binlog的消息队列来做删除缓存的操作。这样做的好处是，不用你自己引入，侵入到你的业务代码中，中间件帮你做了解耦，同时，中间件的这个东西本身就保证了高可用。

当然，这样消息延迟的问题依然存在，但是相比单纯引入消息队列的做法更好一点。

而且，如果并发不是特别高的话，这种做法的实时性和一致性都还算可以接受的。
![图片说明](https://uploadfiles.nowcoder.com/images/20211201/3639882_1638323350568/2CF7DFB308BCB8F3B9E6D7600F87EBF1) 

### 为什么是删除，而不是更新缓存？

我们以**先更新数据库，再删除缓存**来举例。

如果是更新的话，那就是**先更新数据库，再更新缓存**。

举个例子：如果数据库1小时内更新了1000次，那么缓存也要更新1000次，但是这个缓存可能在1小时内只被读取了1次，那么这1000次的更新有必要吗？

反过来，如果是删除的话，就算数据库更新了1000次，那么也只是做了1次缓存删除，只有当缓存真正被读取的时候才去数据库加载。

### 总结

首先，我们要明确一点，缓存不是更新，而应该是删除。

删除缓存有两种方式：

1. 先删除缓存，再更新数据库。解决方案是使用延迟双删。 
2. 先更新数据库，再删除缓存。解决方案是消息队列或者其他binlog同步，引入消息队列会带来更多的问题，并不推荐直接使用。 

针对缓存一致性要求不是很高的场景，那么只通过设置超时时间就可以了。

其实，如果不是很高的并发，无论你选择先删缓存还是后删缓存的方式，都几乎很少能产生这种问题，但是在高并发下，你应该知道怎么解决问题。

## Redis 单线程模型

**Redis 基于 Reactor 模式来设计开发了自己的⼀套高效的事件处理模型**

既然是单线程，那怎么监听⼤量的客户端连接呢？ Redis 通过IO 多路复⽤程序 来监听来自客户端的大量连接（或者说是监听多个 socket），它会 将感兴趣的事件及类型(读、写）注册到内核中并监听每个事件是否发生

I/O 多路复⽤技术的使⽤让 Redis 不需要额外创建多余的线程来监听客户 端的⼤量连接，降低了资源的消耗

![](https://uploadfiles.nowcoder.com/images/20220312/265505588_1647066692071/E3817969C91D0635E9DB451B4227BDEC)



# RabbitMQ

简介：我们通常谈到消息队列，就会联想到这其中的三者：**生产者、消费者和消息队列**，生产者将消息发送到消息队列，消费者从消息队列中获取消息进行处理。对于RabbitMQ，它在此基础上做了一层抽象，**引入了交换器exchange的概念**，**交换器是作用于生产者和消息队列之间的中间桥梁，它起了一种消息路由的作用**，也就是说生产者并不和消息队列直接关联，而是先发送给交换器，再由交换器路由到对应的队列。

## 使用场景

### **1 解耦**

用户下单后，订单系统需要通知库存系统。传统的做法是，订单系统调用库存系统的接口。这时候如果系统宕机，会造成订单丢失，引入MQ，将订单消息发入mq，库存系统再去mq消费，就能解决这一问题。

### 2 **异步提升效率**

传统的模式：用户下单—>邮件发送—>短信提醒，三个步骤全部完成，才能返回用户消费成功，因为后面两个步骤完全没有必须是当前时间完成，可以用户下单成功后，直接发送给mq，返回给用户消费成功，可以其他时间段来消费mq的消息，发送邮件发送和短信提醒给用户。

### 3 流量削峰

例子：高峰期每秒请求有5000个请求左右，但系统A每秒最多只能处理2000请求，将这5000请求写入MQ里面，系统A从MQ中慢慢拉取请求。

## 调度策略

**生产者**将消息发送给Exchange时，一般会**指定一个Routing Key**，来指定这个消息的路由规则，而这个**Routing Key需要与Exchange Type及交换机和队列之间的Binding Key**联合使用才能最终生效。

### ExchangeType

#### 1 Fanout（订阅模式|广播模式）

Fanout交换器会把所有发送到该交换器的消息路由到所有与该交换器绑定的消息队列中。

#### 2 Direct（路由模式）

Direct交换器需要消息的Routing Key与 Exchange和Queue 之间的Binding Key完全匹配，如果匹配成功，将消息分发到该Queue。

#### 3 Topic（通配符模式）

Topic交换器按照正则表达式模糊匹配：用消息的Routing Key与 Exchange和Queue 之间的Binding Key进行模糊匹配，如果匹配成功，将消息分发到该Queue。

#### 4 Headers（几乎不使用）

## RPC(远程调用)

1，RabbiteMq发送请求（消息）时，在消息的属性（MessageProperties，在AMQP协议中定义了14个属性，这些属性会随着消息一起发送）中**设置两个属性值replyTo**（一个Queue名称，用于告诉消费者处理完成后将通知我的消息发送到这个Queue中）和**correlationId**（此次请求的标识号，消费者处理完成后需要将此属性返还，生产者将根据这个id了解哪条请求被成功执行了或执行失败）。
2，消费者收到消息并处理。
3，消费者处理完消息后，将生成一条应答消息到replyTo指定的Queue，同时带上correlationId属性。
4，**生产者之前已订阅replyTo指定的Queue，从中收到服务器的应答消息后，根据其中的correlationId属性分析哪条请求被执行**了，根据执行结果进行后续业务处理。

## 消息确认机制

### 消费者确认机制

在实际应用中，可能会发生消费者收到Queue中的消息，但没有处理完成就宕机（或出现其他意外）的情况，这种情况下就可能会导致消息丢失。

要求消费者在消费完消息后发送一个回执给RabbitMQ，RabbitMQ收到消息回执（Message acknowledgment）后才将该消息从Queue中移除；如果RabbitMQ没有收到回执并检测到消费者的RabbitMQ连接断开，则RabbitMQ会将该消息发送给其他消费者（如果存在多个消费者）进行处理。

### 生产者确认机制（Confirm）

#### 同步确认

简介：

消息发送成功后才会执行后面的代码

*操作流程*

channel.confirmSelect()开启确认机制;

waitForConfirms**确认一条消息**，waitForConfirmsOrDie**批量确认多条消息**。

#### 异步确认

简介：

利用channel对象提供的ConfirmListener，监听包含deliveryTag回调方法（handleAck），channel对象维护一个unconfirm的消息序号集合，每发送一条信息，unconfirm集合元素加1，每回调一次handleAck方法(发送成功)，unConfirm集合删掉一条或多条记录。

*操作流程*：

- 在生产者的channel上开启确认机制： `channel.confirmSelect();`
- 在channel上添加Confirm监听事件： `channel.addConfirmListener(new ConfirmListener() `

- waitForConfirm()确认是否发送成功;

## 消息持久化机制

如果我们希望即使在RabbitMQ服务重启的情况下，也不会丢失消息，我们可以将Queue与Message都设置为可持久化的（durable）

## RabbitMQ事务（很少用降低性能）

简介：RabbitMQ事务要保证生产者发送消息全部发送成功，防止丢失消息(事务/Confirm机制均可保证，常用Confirm机制即生产者消息确认机制)

对事务的支持是AMQP协议的一个重要特性。假设当生产者将一个持久化消息发送给服务器时，因为consume命令本身没有任何Response返回，所以即使服务器崩溃，没有持久化该消息，生产者也无法获知该消息已经丢失。如果此时使用事务，即**通过txSelect()开启一个事务**，然后发送消息给服务器，然后通过**txCommit()提交该事务**，即可以保证，如果txCommit()提交了，则该消息一定会持久化，如果txCommit()还未提交即服务器崩溃，则该消息不会服务器接收。当然Rabbit MQ也提供了txRollback()命令用于回滚某一个事务。

## 消息分发机制

我们在应用程序使用消息系统时，一个队列有多个消费者同时消费数据。工作队列有两种分发数据的方式：轮询分发（Round-robin）和 公平分发（Fair dispatch）。

***轮询分发***：队列给每一个消费者发送数量一样的数据，并不会因为两个消费者处理数据速度不一样使得两个消费者取得不一样数量的数据。但是这种分发方式存在着一些隐患，消费者虽然得到了消息，但是如果消费者没能成功处理业务逻辑，在RabbitMQ中也不存在这条消息。就会出现消息丢失并且业务逻辑没能成功处理的情况。

***公平分发***：消费者设置每次从队列中取一条数据，并且消费完后手动应答，继续从队列取下一个数据。数据是一条条随着消费者消费完从而减少的，并不是一下子全部分发完了。采用公平分发方式就不会出现消息丢失并且业务逻辑没能成功处理的情况。

## 进程

### 1.进程是什么？

进程是计算机程序的一次运行活动，是系统进行资源分配的基本单位。 一个进程可以产生多个线程，各进程间是独立的,利于维护。

### 2.进程的状态有哪些？

首先要知道进程是如何创建和终止的：

1. 创建：提供物理和逻辑资源（CPU 时间、内存、文件、I/O 设备等），传入初始化数据
   - 系统初始化
   - 系统调用创建
   - 用户请求创建
   - 批处理创建
2. 终止：释放占有的资源
   - 正常退出：自愿
   - 错误退出：自愿
   - 严重错误：非自愿
   - 被其他进程杀死：非自愿

其次是进程状态：

1. 运行：可运行，占用CPU时间片运行
2. 就绪：可运行，但没有获得时间片
3. 阻塞：不可运行，等待外部事件发生

### 3.进程调度的方法有哪些？

首先要理解进程调度的时机：

1. 主动：主动终止、异常、阻塞；
2. 被动：CPU时间用完、中断、更高优先级进程进入队列
3. 不能调度的情况：处理中断过程中，进程在内核程序临界区中，原子操作中

其次要知道进程调度是有代价的，需要进行进程的保存和恢复



### 4.进程与线程的区别有哪些？

1. 基本概念：进程是程序的一次执行过程，线程是一个进程中的执行任务。进程之间相互独立，线程之间不独立。
2. 资源占用：进程资源是系统分配的，线程间共享进程资源。
3. 切换：进程内的线程切换不会影响进程，进程间的线程切换会导致进程切换。
4. 开销：进程的创建、销毁开销均大于线程。
5. 通信：进程间通信需要通过IPC，线程间通信可以通过共享内存。

#### 4.1 操作系统如何实现线程？

1. 内核实现线程
2. 用户实现线程
3. 混合实现

#### 4.2为什么要区分用户态和内核态？他们直接是如何转换的？

为了系统调用的安全性考虑，内核态的操作具有最高的访问控制级别，直接由用户操作可能影响硬件资源安全性。

用户态与内核态转换的方式：系统调用、异常和中断。

- 系统调用？

  用户态:用户态运行的进程可以直接读取用户程序的数据。
  系统态:系统态运行的进程或程序几乎可以访问计算机的任何资源。

  大多数我们运行的程序在用户态，当需要调用系统态级别的功能时，需要系统调用。

按功能可分为:设备管理，文件管理，内存管理，进程通信，进程控制

#### 4.3 Java如何实现线程？

Java使用操作系统的原生线程，每创建一个Thread对象就将其映射到操作系统的轻量级进程。

### 5.进程间如何通信？IPC是什么？

IPC指进程间通信。通信是进程间传递信息的方式，同步是定义进程执行的先后顺序

1. 管道：需要先建立连接，半双工通信，FIFO，存在于内存中，适用于父子进程(fork--redis持久化)。
2. 命名管道：需要先建立连接，半双工通信，FIFO，通过命名可以访问非亲缘进程，存在于磁盘或文件系统。
3. 信号：内核与用户进程间的消息通信机制，可以任何时候发送给进程。
4. 消息队列：存放在内核中的共享消息链表，信息需要占用CPU时间进行复制。
5. 共享内存：无需复制，信息量大，但是需要与信号量配合使用以实现同步。
6. 套接字：使用socket接口进行的跨主机通信。

### 6.进程的同步方式有哪些？

1. 临界区：对临界资源进行访问的代码，需要达到互斥效果。
2. 信号量：在同一时间访问共享资源的最大进程数
3. 互斥量：值为1的信号量
4. 管程：独立的访问控制代码，同一时间只有一个进行执行管程。



## 死锁

### 1.死锁是什么？

多个进程相互申请对方所持有的资源，形成一条循环等待链，陷入阻塞，形成死锁。

### 2.死锁的原因是什么？

四大必要条件：

1. 互斥条件：一个资源每次只能被一个进程使用；
2. 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放；
3. 不剥夺条件：进程已获得的资源，在没使用完之前，不能强行剥夺；
4. 循环等待条件：多个进程之间形成一种互相循环等待资源的关系。

### 3.解决死锁

#### 死锁预防

破坏四个必要条件，如静态分配(破坏请求保持)，进程要么占有所有的资源然后开始执行，要么不占有资源，如层次分配(破坏循环等待)，一个进程得到某一层的一个资源后，它只能再申请较高一层的资源；当一个进程要释放某层的一个资源时，必须先释放所占用的较高层的资源。

#### 死锁避免

银行家算法通过先 **试探** 分配给该进程资源，然后通过 **安全性算法** 判断分配后系统是否处于安全状态，若不安全则试探分配作废，让该进程继续等待，若能够进入到安全的状态，则就 **真的分配资源给该进程**。

#### 死锁检测

死锁预防和避免不利于各进程对系统资源的**充分共享**

死锁检测:

1. 如果进程-资源分配图中无环路，则此时系统没有发生死锁
2. 如果进程-资源分配图中有环路，且每个资源类仅有一个资源，则系统中已经发生了死锁。
3. 如果进程-资源分配图中有环路，且涉及到的资源类有多个资源，此时系统未必会发生死锁。如果能在进程-资源分配图中找出一个 **既不阻塞又非独立的进程** ，该进程能够在有限的时间内归还占有的资源，也就是把边给消除掉了，重复此过程，直到能在有限的时间内 **消除所有的边** ，则不会发生死锁，否则会发生死锁。(消除边的过程类似于 **拓扑排序**)

死锁解除：

1. **逐个撤销涉及死锁的进程，回收其资源直至死锁解除。**
2. **抢占资源** ：从涉及死锁的一个或几个进程中抢占资源，把夺得的资源再分配给涉及死锁的进程直至死锁解除。

## 内存

### 内存管理介绍

负责内存的分配与回收，逻辑地址转化为物理地址。

### 内存管理机制

- 连续分配管理:为一个用户程序分配一个连续的内存空间。  
**块式管理**:将内存分为固定大小的几个块，每个块只包含一个进程。很容易造成浪费。  
- 非连续分配管理:允许为用户程序分配离散的内存空间  
**页式管理**:主存分为大小相等且固定的一页一页的形式，页较小，提高内存利用率。页式管理通过页表对应逻辑地址和物理地址。  
**段式管理**:段式管理把主存分为一段段的，段是有实际意义的，每个段定义了一组逻辑信息，例如,有主程序段 MAIN、子程序段 X、数据段 D 及栈段 S 等。 段式管理通过段表对应逻辑地址和物理地址。  
**段页式管理**:段页式管理机制结合了段式管理和页式管理的优点。简单来说段页式管理机制就是把主存先分成若干段，每个段又分成若干页。

### 快表与多级页表

快表利用了缓存思想。相当于页表的缓存  
使用了快表之后，地址转换过程如下:  
根据虚拟地址中页号首先查快表，如果命中，直接获取相应页的物理地址。如果不在快表中，访问内存中的页表，同时将该记录添加至快表中，当快表满时，按照一定策略淘汰页表中的记录。

多级页表避免把全部页表一直放在内存中占用过多空间，特别是那些根本就不需要的页表就不需要保留在内存中。多级页表属于时间换空间的典型场景(访问两次内存，第一次一级页表，第二次二级页表到物理地址)。

### 分页与分段区别

共同点：均是非连续分配方式，提高了内存利用率。
不同点：页的大小是固定的，段的大小不固定由运行的程序决定。页是物理单位，段是逻辑单位。分段能更好满足用户需求。

### cpu寻址与虚拟内存空间

- cpu寻址(地址映射)  
CPU 需要将虚拟地址翻译成物理地址，这样才能访问到真实的物理内存。 实际上完成虚拟地址转换为物理地址转换的硬件是 CPU 中含有一个被称为 内存管理单元（Memory Management Unit, MMU） 的硬件，MMU中存放的是一级页表的基地址(页表中可以存放磁盘上的地址值——虚拟内存技术)。虚拟地址分为页面号和偏移量。
- 虚拟内存空间  
如果直接把物理地址暴露出来的话会带来严重问题，比如可能对操作系统造成伤害以及给同时运行多个程序造成困难。  
程序可以使用一系列相邻的虚拟地址来访问物理内存中不相邻的内存缓冲区。  
不同进程使用的**虚拟地址彼此隔离**。一个进程中的代码无法更改正在由另一进程或操作系统使用的物理内存。

### 什么是虚拟内存

虚拟内存是一种内存管理技术，可以让程序可以拥有超过系统物理内存大小的可用内存空间。另外，虚拟内存为每个进程提供了一个一致的、私有的地址空间，它让每个进程产生了一种自己在独享主存的错觉

### 局部性原理

时间局部性:如果程序中的某条指令一旦执行，不久以后该指令可能再次执行；如果某数据被访问过，不久以后该数据可能再次被访问。  
空间局部性:一旦程序访问了某个存储单元，在不久之后，其附近的存储单元也将被访问。  

### 虚拟内存技术实现

请求分页:建立在分页管理之上，为了支持虚拟存储器功能而增加了请求调页功能和页面置换功能。在作业开始运行之前，仅装入当前要执行的部分页即可运行。假如在作业运行的过程中发现要访问的页面不在内存，则由处理器通知操作系统按照对应的页面置换算法将相应的页面调入到主存  
请求分段:建立在分段存储管理之上，增加了请求调段功能、分段置换功能。请求分段储存管理方式就如同请求分页储存管理方式一样，在作业开始运行之前，仅装入当前要执行的部分段即可运行；在执行过程中，可使用请求调入中断动态装入要访问但又不在内存的程序段。  
请求段页式

### 页面置换算法

OPT 页面置换算法（最佳页面置换算法）：最佳(Optimal, OPT)置换算法所选择的被淘汰页面将是以后永不使用的，或者是在最长时间内不再被访问的页面,这样可以保证获得最低的缺页率。(理想算法)  
FIFO（First In First Out） 页面置换算法（先进先出页面置换算法） : 总是淘汰最先进入内存的页面，即选择在内存中驻留时间最久的页面进行淘汰。  
LRU （Least Recently Used）页面置换算法（最近最久未使用页面置换算法） ：LRU算法赋予每个页面一个访问字段，用来记录一个页面自上次被访问以来所经历的时间 T，当须淘汰一个页面时，选择现有页面中其 T 值最大的，即最近最久未使用的页面予以淘汰。  
LFU （Least Frequently Used）页面置换算法（最少使用页面置换算法）: 该置换算法选择在之前时期使用最少的页面作为淘汰页。

## 磁盘

### 1.磁盘调度方法有哪些？

1. FCFS
2. 最短寻道时间
3. 电梯算法

# 多线程

## 什么是进程？是什么线程？

进程是程序的一次执行过程，是系统运行资源分配和调度的基本单位。线程是一个进程中的执行任务，是比进程更小的执行单位，一个进程可以包含多个线程。

在 Java 中，当我们启动 main 函数时其实就是启动了⼀个 JVM 的进程，而 main 函数所在的线 程就是这个进程中的⼀个线程，也称主线程。与进程不同的是同类的多个线程共享进程的**堆**和**方法区**资源，但每个线程有自己的**程序计数器**、**虚拟机栈**和**本地方法栈**，所以系统在产生⼀个线程，或是在各个线程之间作切换工作 时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。

## 进程和线程的关系？

#### [可能是把Java内存区域讲的最清楚的一篇文章]([JavaGuide/可能是把Java内存区域讲的最清楚的一篇文章.md at 3965c02cc0f294b0bd3580df4868d5e396959e2e · Snailclimb/JavaGuide · GitHub](https://github.com/Snailclimb/JavaGuide/blob/3965c02cc0f294b0bd3580df4868d5e396959e2e/Java相关/可能是把Java内存区域讲的最清楚的一篇文章.md))

![图片说明](https://uploadfiles.nowcoder.com/images/20210917/975641190_1631836413645/E1DEA9F5DD0E334CF05ECC8AB4ACB77F)

**从Java虚拟机的角度来理解：**

Java虚拟机的运行时数据区包含堆、方法区、虚拟机栈、本地方法栈、程序计数器

一个进程可以有多个线程， 多个线程共享进程的堆和方法区，但每个线程有自己的程序计数器、虚拟机栈和本地方法栈

每个进程具备各自的数据空间，进程之间的切换会有较大的开销。属于同一进程的线程会共享堆和方法区，同时具备私有的虚拟机栈、本地方法栈、程序计数器，线程之间的切换资源开销较小。

## 多线程的优缺点（为什么使用多线程、多线程会引发什么问题）

优点：当一个线程进入等待状态或者阻塞时，CPU可以先去执行其他线程，提高CPU的利用率。

缺点：

- **上下文切换**：频繁的上下文切换会影响多线程的执行速度。 
- **死锁** 
- **资源限制**：在进行并发编程时，程序的执行速度受限于计算机的硬件或软件资源。在并发编程中，程序执行变快的原因是将程序中串行执行的部分变成并发执行，如果因为资源限制，并发执行的部分仍在串行执行，程序执行将会变得更慢，因为程序并发需要上下文切换和资源调度。 
- **内存泄漏**

先从总体上来说：

-  线程可以⽐作是轻量级的进程，是程序执行的最小单位,线程间的切换和调度的成本远远小于进程。另外，**多核 CPU 时代意味着多个线程可以同时运行**，这减少了线程**上下文切换的开销**。
-  现在的系统动不动就要求百万级甚⾄千万级的并发量，而多线程并发编程正是开发⾼并发系统的基础，**利用好多线程机制可以大大提⾼系统整体的并发能⼒以及性能。**

再深⼊到计算机底层来探讨：

- 单核时代： 在单核时代多线程主要是为了提高 CPU 和 IO 设备的综合利用率。举个例子：当只有⼀个线程的时候会导致 CPU 计算时， IO 设备空闲；进行 IO 操作时， CPU 空闲。
- 多核时代: 多核时代多线程主要是为了**提高CPU 利用率**。举个例子：假如我们要计算⼀个复杂的任务，我们只用⼀个线程的话， CPU 只会⼀个 CPU 核心被利用到。 

## 线程的上下文切换

多线程中一般线程个数都大于CPU核心数，而一个CPU核心在任意时刻只能被一个线程使用，为了让这些线程都能得到有效执行，CPU采取为每个线程**分配时间片并轮转**的形式。

CPU会通过时间片分配[算法]()来循环执行任务，当前任务执行完一个时间片后会切换到下一个任务，但切换前会保存上一个任务的状态，因为下次切换回这个任务时还要加载这个任务的状态继续执行，从任务保存到再加载的过程就是一次上下文切换。



## 线程的生命周期和状态

线程状态的划分并不唯一，但是都大同小异，这里参考《Java并发编程的艺术》，主要有以下几种状态：

| 状态         |                                                              |
| ------------ | ------------------------------------------------------------ |
| NEW          | 初始状态，**注意此时还未调用`start()`方法**                  |
| RUNNABLE     | 运行状态，包含就绪和运行中两种状态                           |
| BLOCKED      | 阻塞状态                                                     |
| WAITING      | 等待状态                                                     |
| TIME_WAITING | 超时等待状态，和等待状态不同的是，它可以在制定的时间自行返回 |
| TERMINATED   | 终止状态，线程运行结束                                       |

线程转化过程如下：

<img src="C:/Users/50131/Desktop/春招笔记/images/线程生命周期.jpg" />

<img src="C:/Users/50131/Desktop/春招笔记/images/线程状态.png" />

分别是

- 新建

- - 当一个线程对象被创建，但还未调用 start方法时处于新建状态
  - 此时未与操作系统底层线程关联

- 可运行

- - 调用了 start 方法，就会由新建进入可运行
  - 此时与底层线程关联，由操作系统调度执行

- 终结

- - 线程内代码已经执行完毕，由可运行进入终结
  - 此时会取消与底层线程关联

- 阻塞

- - 当获取锁失败后，由可运行进入 Monitor的**阻塞队列**阻塞，此时不占用 cpu 时间
  - 当持锁线程释放锁时，会按照一定规则唤醒阻塞队列中的阻塞线程，唤醒后的线程进入可运行状态

- 等待

- - 当获取锁成功后，但由于条件不满足，调用了 wait()方法，此时从可运行状态释放锁进入 Monitor **等待集合**等待，同样不占用 cpu 时间
  - 当其它持锁线程调用 notify() 或 notifyAll() 方法，会按照一定规则唤醒等待集合中的等待线程，恢复为可运行状态

- 有时限等待

- - 当获取锁成功后，但由于条件不满足，调用了 wait(long)      方法，此时从**可运行状态释放锁**进入 Monitor 等待集合进行有时限等待，同样不占用 cpu 时间
  - 当其它持锁线程调用 notify() 或 notifyAll() 方法，会按照一定规则唤醒等待集合中的有时限等待线程，恢复为可运行状态，并重新去竞争锁
  - 如果等待超时，也会从有时限等待状态恢复为可运行状态，并重新去竞争锁
  - 还有一种情况是调用 sleep(long)      方法也会从可运行状态进入有时限等待状态，但与 Monitor 无关，不需要主动唤醒，超时时间到自然恢复为可运行状态

其它情况（只需了解）

- 可以用 interrupt() 方法打断等待、有时限等待的线程，让它们恢复为可运行状态
- park，unpark 等方法也可以让线程等待和唤醒

## 创建线程一共有哪几种方法？

- 继承`Thread`类创建线程 
- 实现`Runnable`接口创建线程 
- 使用`Callable`和`Future`创建线程 
- 使用线程池例如用`Executor`框架 

**继承Thread类创建线程**，首先继承Thread类，重写`run()`方法，在`main()`函数中调用子类实实例的`start()`方法。

```
public class ThreadDemo extends Thread {
    @Override
    public void run() {
        System.out.println(Thread.currentThread().getName() + " run()方法正在执行");
    }
}
```

```
public class TheadTest {
    public static void main(String[] args) {
        ThreadDemo threadDemo = new ThreadDemo();     
        threadDemo.start();
        System.out.println(Thread.currentThread().getName() + " main()方法执行结束");
    }
}
```

输出结果：

```
main main()方法执行结束
Thread-0 run()方法正在执行
```

**实现Runnable接口创建线程：**首先创建实现`Runnable`接口的类`RunnableDemo`，重写`run()`方法；创建类`RunnableDemo`的实例对象`runnableDemo`，以`runnableDemo`作为参数创建`Thread`对象，调用`Thread`对象的`start()`方法。

```
public class RunnableDemo implements Runnable {
    @Override
    public void run() {
        System.out.println(Thread.currentThread().getName() + " run()方法执行中");
    }
}
```

```
public class RunnableTest {
    public static void main(String[] args) {
        RunnableDemo  runnableDemo = new RunnableDemo ();
        Thread thread = new Thread(runnableDemo);
        thread.start();
        System.out.println(Thread.currentThread().getName() + " main()方法执行完成");
}
```

输出结果：

```
main main()方法执行完成
Thread-0 run()方法执行中
```

**使用Callable和Future创建线程：** 1. 创建Callable接口的实现类`CallableDemo`，重写`call()`方法。2. 以类`CallableDemo`的实例化对象作为参数创建`FutureTask`对象。3. 以`FutureTask`对象作为参数创建`Thread`对象。4. 调用`Thread`对象的`start()`方法。

```java
class NumThread implements Callable{
    //2.实现call方法，将此线程需要执行的操作声明在call()中
    @Override
    public Object call() throws Exception {
        int sum = 0;
        for (int i = 1; i <= 100; i++) {
            if(i % 2 == 0){
                System.out.println(i);
                sum += i; 
            }
        }
        return sum;
    }
}

public class ThreadNew {
    public static void main(String[] args) {
        //3.创建Callable接口实现类的对象
        NumThread numThread = new NumThread();
        //4.将此Callable接口实现类的对象作为传递到FutureTask构造器中，创建FutureTask的对象
        FutureTask futureTask = new FutureTask(numThread);
        //5.将FutureTask的对象作为参数传递到Thread类的构造器中，创建Thread对象，并调用start()
        new Thread(futureTask).start();
        try {
            //6.获取Callable中call方法的返回值
            //get()返回值即为FutureTask构造器参数Callable实现类重写的call()的返回值。
            Object sum = futureTask.get();
            System.out.println("总和为：" + sum);
        } catch (InterruptedException e) {
            e.printStackTrace();
        } catch (ExecutionException e) {
            e.printStackTrace();
        }
    }
}
```

**使用线程池例如用Executor框架：** `Executors`可提供四种线程池，分别为：

- `newCachedThreadPool`创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 
- `newFixedThreadPool` 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 
- `newScheduledThreadPool` 创建一个定长线程池，支持定时及周期性任务执行。 
- `newSingleThreadExecutor` 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序执行。

## runnable 和 callable 有什么区别？

**相同点**：

- 两者都是接口 
- 两者都需要调用`Thread.start()`启动线程 

**不同点**：

- callable的核心是`call()`方法，**允许返回值**，`runnable`的核心是`run()`方法，**没有返回值** 
- `call()`方法**可以抛出异常**，但是`run()`方法不行 
- `callable`和`runnable`都可以应用于`executors`，**`thread`类只支持`runnable`** 

## 线程的run()和start()有什么区别？

- 线程是通过`Thread`对象所对应的方法`run()`来完成其操作的，而线程的启动是通过`start()`方法执行的。 
- `run()`方法可以重复调用，`start()`方法只能调用一次 
- 调用start方法方可启动线程，而run方法只是thread类中的一个**普通方法**调用，还是在主线程里执行。



## 为什么调用start()方法时会执行run()方法，而不直接执行run()方法？

`start()`方法来启动线程，真正实现了多线程运行，这时无需等待`run()`方法体代码执行完毕而直接继续执行下面的代码。通过调用Thread类的 `start()`方法来启动一个线程，这时此线程处于就绪（可运行）状态，一旦得到cpu时间片，就开始执行`run()`方法，这里方法`run()`称为线程体，它包含了要执行的这个线程的内容，`run()`方法运行结束，此线程随即终止。

`run()`方法只是类的一个普通方法而已，如果直接调用`run`方法，程序中依然只有主线程这一个线程，其程序执行路径还是只有一条，还是要顺序执行，还是要等待`run()`方法体执行完毕后才可继续执行下面的代码，这样就没有达到写线程的目的。

调用`start()`方法可以开启一个线程，而`run()`方法只是thread类中的一个普通方法，直接调用`run()`方法还是在主线程中执行的。

调⽤ start() 方法方可启动线程并使线程进⼊就绪状态，直接执行 run() 方法的话不会 以多线程的方式执行。

## 线程同步和线程调度相关的方法问题

### 线程同步以及线程调度相关的方法有哪些？

- `wait()`：使一个线程处于等待（阻塞）状态，并且**释放所持有的对象的锁**；
- `sleep()`：使当前线程进入指定毫秒数的休眠，暂停执行，**需要处理`InterruptedException`**。
- `notify()`：唤醒一个处于等待状态的线程，当然在调用此方法的时候，并不能确切的唤醒某一个等待状态的线程，而是由 JVM 确定唤醒哪个线程，而且与优先级无关。
- `notifyAll()`：唤醒所有处于等待状态的线程，该方法并不是将对象的锁给所有线程，而是让它们竞争，只有获得锁的线程才能进入就绪状态。
- `join()`：与`sleep()`方法一样，是一个可中断的方法，在一个线程中调用另一个线程的`join()`方法**使得当前的线程挂起，直到执行`join()`方法的线程结束**。例如在B线程中调用A线程的`join()`方法，B线程进入阻塞状态，直到A线程结束或者到达指定的时间。
- `yield()`：提醒调度器愿意放弃当前的CPU资源，使得当前线程从`RUNNING`状态切换到`RUNABLE`状态。

### 线程的sleep()方法和yield()方法有什么不同？

- sleep()方法给其他线程运行机会时不考虑线程的[优先级](https://so.csdn.net/so/search?q=优先级&spm=1001.2101.3001.7020)，因此会给低优先级的线程以运行的机会；yield()方法只会给相同优先级或更高优先级的线程以运行的机会。 
- `sleep()`使得线程进入到**阻塞状态**，`yield()`只是对CPU进行提示，如果CPU没有忽略这个提示，会使得线程上下文的切换，进入到就绪状态，就绪态指进程所有的执行条件都满足，只等着cpu来调度，阻塞态指进程不具备执行条件，比如需等待I/O。 
- `sleep()`需要抛出`InterruptedException`，而`yield()`方法无需抛出异常。

### sleep()方法和wait()方法的区别？

相同点：

- `wait()`方法和`sleep()`方法都可以使得线程进入到**阻塞状态**。 
- `wait()`和`sleep()`方法都是可中断方法，被中断后都会收到**中断异常**。 

不同点：

* `sleep()` 方法没有释放锁，而 `wait()`方法释放了锁 

- `wait()`是Object的方法，`sleep()`是Thread的方法。 

- `wait()`必须在同步方法(即有synchronized关键字修饰的方法)中进行，`sleep()`方法不需要。 

- `sleep()`方法在短暂的休眠之后会主动退出阻塞，而`wait()`方法在没有指定wait时间的情况下需要被其他线程中断才可以退出阻塞。 

- `wait() `通常被用于线程间交互/通信， `sleep() `通常被用于暂停执行。

### wait()方法一般在循环块中使用还是if块中使用？

wait() 方法应该在循环调用，因为当线程获取到CPU 开始执行的时候，**其他条件可能还没有满足**，所以在处理前，循环检测条件是否满足会更好。if可能出现虚假唤醒。

### 线程通信的方法有哪些？ 

- 锁与同步 
- `wait()`/`notify()`或`notifyAll()` 
- 信号量 
- 管道 

### 为什么wait()、notify()、notifyAll()被定义在Object类中而不是在Thread类中？

**加锁解锁都是对对象进行操作**。

因为这些方法在操作同步线程时，都必须要标识他们操作线程的锁，**只有同一个锁上的被等待线程，可以被同一个锁上的`notify()`或`notifyAll()`唤醒**，不可以对不同锁中的线程进行唤醒，也就是说等待和唤醒必须是同一锁。而**锁可以是任意对象**，所以可以被任意对象调用的方法是定义在`Object`类中。

如果把`wait()`、`notify()`、`notifyAll()`定义在Thread类中，则会出现一些难以解决的问题，例如如何让一个线程可以持有多把锁？如何确定线程等待的是哪把锁？既然是当前线程去等待某个对象的锁，则应通过操作对象来实现而不是操作线程，而Object类是所有对象的父类，所以将这三种方法定义在Object类中最合适。

> JAVA提供的锁是对象级的而不是线程级的，每个对象都有个锁，而线程是可以获得这个对象的。因此线程需要等待某些锁，那么只要调用对象中的wait()方法便可以了。而wait()方法如果定义在Thread类中的话，那么线程正在等待的是哪个锁就不明确了。这也就是说wait，notify和notifyAll都是锁级别的操作，所以把他们定义在Object类中是因为锁是属于对象的原因。

### 为什么wait()，notify()和notifyAll()必须在同步方法或者同步块中被调用？

因为`wait()`暂停的是持有锁的对象，`notify()`或`notifyAll()`唤醒的是等待锁的对象。所以`wait()`、`notify()`、`notifyAll()`都需要线程持有锁的对象，进而需要在同步方法或者同步块中被调用。

### 为什么Thread类的sleep()和yield()方法是静态的？ 

因为该方法的作用是让当前正在运行的线程休眠

该代码只有在某个A线程执行时会被执行，这种情况下通知某个B线程yield是无意义的（因为B线程本来就没在执行）。因此只有当前线程执行yield才是有意义的。通过使该方法为static，你将不会浪费时间尝试yield 其他线程。

`sleep()`和`yield()`都是需要正在执行的线程调用的，**那些本来就阻塞或者等待的线程调用这个方法是无意义的**，所以这两个方法是静态的。

### 如何停止一个正在运行的线程？ 

- 中断：`Interrupt`方法中断线程 
- 使用`volatile boolean`标志位停止线程：在线程中设置一个`boolean`标志位，同时用`volatile`修饰保证可见性，在线程里不断地读取这个值，其他地方可以修改这个`boolean`值。 
- 使用`stop()`方法停止线程，但该方法已经被废弃。因为这样线程不能在停止前保存数据，会出现数据完整性问题。 

### 如何唤醒一个阻塞的线程？ 

如果线程是由于`wait()`、`sleep()`、`join()`、`yield()`等方法进入阻塞状态的，是可以进行唤醒的。如果线程是IO阻塞是无法进行唤醒的，因为IO是操作系统层面的，Java代码无法直接接触操作系统。

- `wait()`：可用`notify()`或`notifyAll()`方法唤醒。 
- `sleep()`：调用该方法使得线程在指定时间内进入阻塞状态，等到指定时间过去，线程再次获取到CPU时间片进而被唤醒。 
- `join()`：当前线程A调用另一个线程B的`join()`方法，当前线程转A入阻塞状态，直到线程B运行结束，线程A才由阻塞状态转为可执行状态。 
- `yield()`：使得当前线程放弃CPU时间片，但随时可能再次得到CPU时间片进而激活。 

### Java如何实现两个线程之间的通信和协作？

- `syncrhoized`加锁的线程的`Object`类的`wait()`/`notify()`/`notifyAll()` 
- `ReentrantLock`类加锁的线程的`Condition`类的`await()`/`signal()`/`signalAll()` 
- 通过管道进行线程间通信：1）字节流；2）字符流 ，就是一个线程发送数据到输出管道，另一个线程从输入管道读数据。

### 同步方法和同步代码块哪个效果更好？

同步块更好些，因为它锁定的范围更灵活些，只在需要锁住的代码块锁住相应的对象，而同步方法锁住整个对象。

### 什么是线程同步？什么是线程互斥？他们是如何实现的？

- 线程互斥：某一个资源只能被一个访问者访问，具有唯一性和排他性。但访问者对资源访问的顺序是乱序的。 
- 线程同步：在互斥的基础上使得访问者对资源进行有序访问。 

线程同步的实现方法：

- 同步方法 
- 同步代码块 
- `wait()`和`notify()` 
- 使用volatile实现线程同步 
- 使用重入锁实现线程同步 
- 使用局部变量实现线程同步 
- 使用阻塞队列实现线程同步 



### 在Java程序中如何保证线程的运行安全？ 

线程安全问题 主要体现在原子性、可见性和有序性。

- 原子性：一个或者多个操作在 CPU 执行的过程中不被中断的特性。线程切换带来的原子性问题。
- 可见性：一个线程对共享变量的修改，另外一个线程能够立刻看到。缓存导致的可见性问题。
- 有序性：程序执行的顺序按照代码的先后顺序执行。编译优化带来的有序性问题。

解决方法：

- 原子性问题：可用JDK `Atomic`开头的原子类、`synchronized`、`LOCK`来解决 
- 可见性问题：可用`synchronized`、`volatile`、`LOCK`来解决 
- 有序性问题：可用`Happens-Before` 规则来解决 



### 线程类的构造方法、静态块是被哪个线程调用的？ 

线程类的构造方法、静态块是被`new`这个线程类所在的线程所调用的，而`run()`方法里面的代码才是被线程自身所调用的。

一个很经典的例子：

假设`main()`函数中`new`了一个线程Thread1，那么Thread1的构造方法、静态块都是`main`线程调用的，Thread1中的`run()`方法是自己调用的。

假设在Thread1中`new`了一个线程Thread2，那么Thread2的构造方法、静态块都是Thread1线程调用的，Thread2中的`run()`方法是自己调用的。







### 线程数量过多会造成什么异常？ 

- 消耗更多的内存和CPU 
- 频繁进行上下文切换 



## 三个线程T1、T2、T3，如何让他们按顺序执行？ 

```
public class TestTwo {
        static TestTwo t=new TestTwo();
        class T1 extends Thread{
            @Override
            public void run() {
                try {
                    Thread.sleep(100);
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
                //T1线程中要处理的东西
                System.out.println("T1线程执行")
            }
        }

        class T2 extends Thread{
            @Override
            public void run() {
                try {
                    Thread.sleep(200);
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
                //T2线程中要处理的东西
                System.out.println("T2线程执行");
                t.new T1().start();
            }
        }

        class T3 extends Thread{
            @Override
            public void run() {
                try {
                    Thread.sleep(300);
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
                //T3线程中要处理的东西
                System.out.println("T3线程执行");
                t.new T2().start();
            }
        }
        
        public static void main(String[] args) {
            t.new T3().start();
　　　　　　　//打印结果如下：
             //T3线程执行
　　　　　　　　//T2线程执行
              //T1线程执行

        }
        
}
```

这是一道面试中常考的并发编程的代码题，与它相似的问题有：

- 使用两个线程打印 1-100。线程1, 线程2 交替打印

```
class Number implements Runnable{
    private int number = 1;
    private Object obj = new Object();
    @Override
    public void run() {
        while(true){
            synchronized (obj) {
                obj.notify();
                if(number <= 100){
                    try {
                        Thread.sleep(10);
                    } catch (InterruptedException e) {
                        e.printStackTrace();
                    }
                    System.out.println(Thread.currentThread().getName() + ":" + number);
                    number++;
                    try {
                        //使得调用如下wait()方法的线程进入阻塞状态
                        obj.wait();
                    } catch (InterruptedException e) {
                        e.printStackTrace();
                    }
                }else{
                    break;
                }
            }
        }
    }
}

public class CommunicationTest {
    public static void main(String[] args) {
        Number number = new Number();
        Thread t1 = new Thread(number);
        Thread t2 = new Thread(number);
        t1.setName("线程1");
        t2.setName("线程2");
        t1.start();
        t2.start();
    }
}
```

- 两个线程交替打印1-100的奇偶数 

```java
class  Wait_Notify_Odd_Even{
    private Object monitor = new Object();
    private volatile int count;

    Wait_Notify_Odd_Even(int initCount) {
        this.count = initCount;
    }

    private void printOddEven() {
        synchronized (monitor) {
            while (count < 10) {
                try {
                    System.out.print( Thread.currentThread().getName() + "：");
                    System.out.println(++count);
                    monitor.notifyAll();
                    monitor.wait();
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
            }
            //防止count=10后，while()循环不再执行，有子线程被阻塞未被唤醒，导致主线程不能退出
            monitor.notifyAll();
        }
    }
    
    public static void main(String[] args) throws InterruptedException {
        Wait_Notify_Odd_Even waitNotifyOddEven = new Wait_Notify_Odd_Even(0);
        new Thread(waitNotifyOddEven::printOddEven, "odd").start();
        Thread.sleep(10); //为了保证线程odd先拿到锁
        new Thread(waitNotifyOddEven::printOddEven, "even").start();
    }
}
```

- 三个线程T1、T2、T3轮流打印ABC，打印n次，如ABCABCABCABC....... 

```java
class Wait_Notify_ACB {

    private int num;
    private static final Object LOCK = new Object();

    private void printABC(int targetNum) {
        for (int i = 0; i < 10; i++) {
            synchronized (LOCK) {
                while (num % 3 != targetNum) { //想想这里为什么不能用if代替，想不起来可以看公众号上一篇文章
                    try {
                        LOCK.wait();
                    } catch (InterruptedException e) {
                        e.printStackTrace();
                    }
                }
                num++;
                System.out.print(Thread.currentThread().getName());
                LOCK.notifyAll();
            }
        }
    }
    public static void main(String[] args) {
        Wait_Notify_ACB  wait_notify_acb = new Wait_Notify_ACB ();
        new Thread(() -> {
            wait_notify_acb.printABC(0);
        }, "A").start();
        new Thread(() -> {
            wait_notify_acb.printABC(1);
        }, "B").start();
        new Thread(() -> {
            wait_notify_acb.printABC(2);
        }, "C").start();
    }    
}
```

- N个线程循环打印1-100 

```java
class Wait_Notify_100 {

    private int num;
    private static final Object LOCK = new Object();
    private int maxnum = 10;

    private void printABC(int targetNum) {
        while (true) {
            synchronized (LOCK) {
                while (num % 3 != targetNum) { //想想这里为什么不能用if代替，想不起来可以看公众号上一篇文章
                    if(num >= maxnum){
                        break;
                    }
                    try {
                        LOCK.wait();
                    } catch (InterruptedException e) {
                        e.printStackTrace();
                    }
                }
                if(num >= maxnum){
                    break;
                }
                num++;
                System.out.println(Thread.currentThread().getName() + ": " + num);
                LOCK.notifyAll();
            }
        }

    }
    
    public static void main(String[] args) {
        Wait_Notify_100  wait_notify_100 = new Wait_Notify_100 ();
        new Thread(() -> {
            wait_notify_100.printABC(0);
        }, "thread1").start();
        new Thread(() -> {
            wait_notify_100.printABC(1);
        }, "thread2").start();
        new Thread(() -> {
            wait_notify_100.printABC(2);
        }, "thread3").start();
    }    
}
```

其实这类问题本质上都是线程通信问题，思路基本上都是一个线程执行完毕，阻塞该线程，唤醒其他线程，按顺序执行下一个线程。下面先来看最简单的，如何按顺序执行三个线程。

- synchronized+wait/notify 

基本思路就是线程A、线程B、线程C三个线程同时启动，因为变量`num`的初始值为`0`，所以线程B或线程C拿到锁后，进入`while()`循环，然后执行`wait()`方法，线程线程阻塞，释放锁。只有线程A拿到锁后，不进入`while()`循环，执行`num++`，打印字符`A`，最后唤醒线程B和线程C。此时`num`值为`1`，只有线程B拿到锁后，不被阻塞，执行`num++`，打印字符`B`，最后唤醒线程A和线程C，后面以此类推。

下面看第二个问题，两个线程交替打印1-100的奇偶数，为了减少输入所占篇幅，这里将100 改成了10。基本思路上面类似，线程odd先拿到锁——打印数字——唤醒线程even——阻塞线程odd，以此循环。

[面试官：请用五种方法实现多线程交替打印问题 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/370130458)

## 实现生产者消费者模型

 **wait()和notify()方法的实现**

这也是最简单最基础的实现，缓冲区满和为空时都调用wait()方法等待，当生产者生产了一个产品或者消费者消费了一个产品之后会唤醒所有线程。

```java
/**
 * 生产者和消费者，wait()和notify()的实现
 * @author ZGJ
 * @date 2017年6月22日
 */
public class Test1 {
    private static Integer count = 0;
    private static final Integer FULL = 10;
    private static String LOCK = "lock";
    
    public static void main(String[] args) {
        Test1 test1 = new Test1();
        new Thread(test1.new Producer()).start();
        new Thread(test1.new Consumer()).start();
        new Thread(test1.new Producer()).start();
        new Thread(test1.new Consumer()).start();
        new Thread(test1.new Producer()).start();
        new Thread(test1.new Consumer()).start();
        new Thread(test1.new Producer()).start();
        new Thread(test1.new Consumer()).start();
    }
    class Producer implements Runnable {
        @Override
        public void run() {
            for (int i = 0; i < 10; i++) {
                try {
                    Thread.sleep(3000);
                } catch (Exception e) {
                    e.printStackTrace();
                }
                synchronized (LOCK) {
                    while (count == FULL) {
                        try {
                            LOCK.wait();
                        } catch (Exception e) {
                            e.printStackTrace();
                        }
                    }
                    count++;
                    System.out.println(Thread.currentThread().getName() + "生产者生产，目前总共有" + count);
                    LOCK.notifyAll();
                }
            }
        }
    }
    class Consumer implements Runnable {
        @Override
        public void run() {
            for (int i = 0; i < 10; i++) {
                try {
                    Thread.sleep(3000);
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
                synchronized (LOCK) {
                    while (count == 0) {
                        try {
                            LOCK.wait();
                        } catch (Exception e) {
                        }
                    }
                    count--;
                    System.out.println(Thread.currentThread().getName() + "消费者消费，目前总共有" + count);
                    LOCK.notifyAll();
                }
            }
        }
    }
}
```

**结果:**

```text
Thread-0生产者生产，目前总共有1
Thread-4生产者生产，目前总共有2
Thread-3消费者消费，目前总共有1
Thread-1消费者消费，目前总共有0
Thread-2生产者生产，目前总共有1
Thread-6生产者生产，目前总共有2
Thread-7消费者消费，目前总共有1
Thread-5消费者消费，目前总共有0
Thread-0生产者生产，目前总共有1
Thread-4生产者生产，目前总共有2
Thread-3消费者消费，目前总共有1
Thread-6生产者生产，目前总共有2
Thread-1消费者消费，目前总共有1
Thread-7消费者消费，目前总共有0
Thread-2生产者生产，目前总共有1
Thread-5消费者消费，目前总共有0
Thread-0生产者生产，目前总共有1
Thread-4生产者生产，目前总共有2
Thread-3消费者消费，目前总共有1
Thread-7消费者消费，目前总共有0
Thread-6生产者生产，目前总共有1
Thread-2生产者生产，目前总共有2
Thread-1消费者消费，目前总共有1
Thread-5消费者消费，目前总共有0
Thread-0生产者生产，目前总共有1
Thread-4生产者生产，目前总共有2
Thread-3消费者消费，目前总共有1
Thread-1消费者消费，目前总共有0
Thread-6生产者生产，目前总共有1
Thread-7消费者消费，目前总共有0
Thread-2生产者生产，目前总共有1
```

**可重入锁ReentrantLock的实现**

java.util.concurrent.lock 中的 Lock 框架是锁定的一个抽象，通过对lock的lock()方法和unlock()方法实现了对锁的显示控制，而synchronize()则是对锁的隐性控制。
可重入锁，也叫做递归锁，指的是同一线程 外层函数获得锁之后 ，内层递归函数仍然有获取该锁的代码，但不受影响，简单来说，该锁维护这一个与获取锁相关的计数器，如果拥有锁的某个线程再次得到锁，那么获取计数器就加1，函数调用结束计数器就减1，然后锁需要被释放两次才能获得真正释放。已经获取锁的线程进入其他需要相同锁的同步代码块不会被阻塞。

```java
import java.util.concurrent.locks.Condition;
import java.util.concurrent.locks.Lock;
import java.util.concurrent.locks.ReentrantLock;
/**
 * 生产者和消费者，ReentrantLock的实现
 * 
 * @author ZGJ
 * @date 2017年6月22日
 */
public class Test2 {
    private static Integer count = 0;
    private static final Integer FULL = 10;
    //创建一个锁对象
    private Lock lock = new ReentrantLock();
    //创建两个条件变量，一个为缓冲区非满，一个为缓冲区非空
    private final Condition notFull = lock.newCondition();
    private final Condition notEmpty = lock.newCondition();
    public static void main(String[] args) {
        Test2 test2 = new Test2();
        new Thread(test2.new Producer()).start();
        new Thread(test2.new Consumer()).start();
        new Thread(test2.new Producer()).start();
        new Thread(test2.new Consumer()).start();
        new Thread(test2.new Producer()).start();
        new Thread(test2.new Consumer()).start();
        new Thread(test2.new Producer()).start();
        new Thread(test2.new Consumer()).start();
    }
    class Producer implements Runnable {
        @Override
        public void run() {
            for (int i = 0; i < 10; i++) {
                try {
                    Thread.sleep(3000);
                } catch (Exception e) {
                    e.printStackTrace();
                }
                //获取锁
                lock.lock();
                try {
                    while (count == FULL) {
                        try {
                            notFull.await();
                        } catch (InterruptedException e) {
                            e.printStackTrace();
                        }
                    }
                    count++;
                    System.out.println(Thread.currentThread().getName()
                            + "生产者生产，目前总共有" + count);
                    //唤醒消费者
                    notEmpty.signal();
                } finally {
                    //释放锁
                    lock.unlock();
                }
            }
        }
    }
    class Consumer implements Runnable {
        @Override
        public void run() {
            for (int i = 0; i < 10; i++) {
                try {
                    Thread.sleep(3000);
                } catch (InterruptedException e1) {
                    e1.printStackTrace();
                }
                lock.lock();
                try {
                    while (count == 0) {
                        try {
                            notEmpty.await();
                        } catch (Exception e) {
                            e.printStackTrace();
                        }
                    }
                    count--;
                    System.out.println(Thread.currentThread().getName()
                            + "消费者消费，目前总共有" + count);
                    notFull.signal();
                } finally {
                    lock.unlock();
                }
            }
        }
    }
}
```

**信号量Semaphore的实现**
Semaphore（信号量）是用来控制同时访问特定资源的线程数量，它通过协调各个线程，以保证合理的使用公共资源，在操作系统中是一个非常重要的问题，可以用来解决哲学家就餐问题。Java中的Semaphore维护了一个许可集，一开始先设定这个许可集的数量，可以使用acquire()方法获得一个许可，当许可不足时会被阻塞，release()添加一个许可。在下列代码中，还加入了另外一个mutex信号量，维护生产者消费者之间的同步关系，保证生产者和消费者之间的交替进行

```java
import java.util.concurrent.Semaphore;
/**
 * 使用semaphore信号量实现
 * @author ZGJ
 * @date 2017年6月29日
 */
public class Test4 {
    private static Integer count = 0;
    //创建三个信号量
    final Semaphore notFull = new Semaphore(10);
    final Semaphore notEmpty = new Semaphore(0);
    final Semaphore mutex = new Semaphore(1);
    public static void main(String[] args) {
        Test4 test4 = new Test4();
        new Thread(test4.new Producer()).start();
        new Thread(test4.new Consumer()).start();
        new Thread(test4.new Producer()).start();
        new Thread(test4.new Consumer()).start();
        new Thread(test4.new Producer()).start();
        new Thread(test4.new Consumer()).start();
        new Thread(test4.new Producer()).start();
        new Thread(test4.new Consumer()).start();
    }
    class Producer implements Runnable {
        @Override
        public void run() {
            for (int i = 0; i < 10; i++) {
                try {
                    Thread.sleep(3000);
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
                try {
                    notFull.acquire();
                    mutex.acquire();
                    count++;
                    System.out.println(Thread.currentThread().getName()
                            + "生产者生产，目前总共有" + count);
                } catch (InterruptedException e) {
                    e.printStackTrace();
                } finally {
                    mutex.release();
                    notEmpty.release();
                }
            }
        }
    }
    class Consumer implements Runnable {
        @Override
        public void run() {
            for (int i = 0; i < 10; i++) {
                try {
                    Thread.sleep(3000);
                } catch (InterruptedException e1) {
                    e1.printStackTrace();
                }
                try {
                    notEmpty.acquire();
                    mutex.acquire();
                    count--;
                    System.out.println(Thread.currentThread().getName()
                            + "消费者消费，目前总共有" + count);
                } catch (InterruptedException e) {
                    e.printStackTrace();
                } finally {
                    mutex.release();
                    notFull.release();
                }
            }
        }
    }
}
```

## synchronized关键字

### 说说对于 synchronized 关键字的了解

synchronized 关键字解决的是多个线程之间访问资源的**同步性**， synchronized 关键字可以保证被它修饰的方法或者代码块在任意时刻只能有⼀个线程执行。
另外，在 Java 早期版本中， synchronized 属于**重量级锁**，效率低下。
因为监视器锁（monitor）是依赖于底层的操作系统的 Mutex Lock 来实现的， Java 的线程是映射到操作系统的原⽣线程之上的。如果要挂起或者唤醒⼀个线程，都需要操作系统帮忙完成，而操作系统实现线程之间的切换时需要从用户态转换到内核态，这个状态之间的转换需要相对比较长的时间，时间成本相对较⾼。
庆幸的是在 Java 6 之后 Java 官方对从 JVM 层⾯对 synchronized 较⼤优化，所以现在的synchronized 锁效率也优化得很不错了。 JDK1.6 对锁的实现引⼊了⼤量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。
所以，你会发现目前的话，不论是各种开源框架还是 JDK 源码都大量使用了 synchronized 关键字。  

### Java内存的可见性问题

在了解synchronized关键字的底层原理前，需要先简单了解下Java的内存模型，看看synchronized关键字是如何起作用的。

![图片说明](https://uploadfiles.nowcoder.com/images/20210917/975641190_1631836513446/46883DED9E9CB826A77E838A4B30A03D) 

这里的本地内存并不是真实存在的，只是Java内存模型的一个抽象概念，它包含了控制器、运算器、缓存等。同时Java内存模型规定，线程对共享变量的操作必须在自己的本地内存中进行，不能直接在主内存中操作共享变量。这种内存模型会出现什么问题呢？，

1. 线程A获取到共享变量X的值，此时本地内存A中没有X的值，所以加载主内存中的X值并缓存到本地内存A中，线程A修改X的值为1，并将X的值刷新到主内存中，这时主内存及本地内存中的X的值都为1。 
2. 线程B需要获取共享变量X的值，此时本地内存B中没有X的值，加载主内存中的X值并缓存到本地内存B中，此时X的值为1。线程B修改X的值为2，并刷新到主内存中，此时主内存及本地内存B中的X值为2，本地内存A中的X值为1。 
3. 线程A再次获取共享变量X的值，此时本地内存中存在X的值，所以直接从本地内存中A获取到了X为1的值，但此时主内存中X的值为2，到此出现了所谓内存不可见的问题。 

该问题Java内存模型是通过synchronized关键字和volatile关键字就可以解决，那么synchronized关键字是如何解决的呢，**其实进入synchronized块就是把在synchronized块内使用到的变量从线程的本地内存中擦除**，这样在synchronized块中再次使用到该变量就不能从本地内存中获取了，需要从主内存中获取，解决了内存不可见问题。

### synchronized关键字三大特性是什么？

> 面试时经常拿synchronized关键字和volatile关键字的特性进行对比，synchronized关键字可以保证并发编程的三大特性：原子性、可见性、有序性，而volatile关键字只能保证可见性和有序性，不能保证原子性，也称为是轻量级的synchronized。

- 原子性：一个或多个操作要么全部执行成功，要么全部执行失败。synchronized关键字可以保证只有一个线程拿到锁，访问共享资源。 
- 可见性：当一个线程对共享变量进行修改后，其他线程可以立刻看到。执行synchronized时，会对应执行 lock 、unlock原子操作，保证可见性。 
- 有序性：程序的执行顺序会按照代码的先后顺序执行。 

### synchronized关键字可以实现什么类型的锁？

- 悲观锁：synchronized关键字实现的是悲观锁，每次访问共享资源时都会上锁。 
- 非公平锁：synchronized关键字实现的是非公平锁，即线程获取锁的顺序并不一定是按照线程阻塞的顺序。 
- 可重入锁：synchronized关键字实现的是可重入锁，即已经获取锁的线程可以再次获取锁。 
- 独占锁/排他锁：synchronized关键字实现的是独占锁，即该锁只能被一个线程所持有，其他线程均被阻塞。 

### synchronized关键字的使用方式

**修饰实例方法**: 作用于当前对象实例加锁，进入同步代码前要获得当前对象实例的锁

```java
synchronized void method() {
	//业务代码
}
```

**修饰静态方法**: 也就是给当前类加锁，会作用于类的所有对象实例，因为静态成员不属于任何一个实例对象，是类成员（ static 表明这是该类的一个静态资源，不管new了多少个对象，只有一份）。所以如果一个线程A调用一个实例对象的非静态 synchronized 方法，而线程B需要调用这个实例对象所属类的静态 synchronized 方法，是允许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁是当前类的锁，而访问非静态 synchronized 方法占用的锁是当前实例对象锁。

```java
synchronized void staic method() {
	//业务代码
}
```

**修饰代码块**: 指定加锁对象，对给定对象加锁，进入同步代码块前要获得给定对象的锁。

```
synchronized(this) {
	//业务代码
}
```

总结：

-  synchronized 关键字加到 static 静态方法和 synchronized(class)代码块上都是是给 Class 类上锁。
-  synchronized 关键字加到实例方法上是给对象实例上锁。
-  尽量不要使用 synchronized(String a) 因为JVM中，字符串常量池具有缓存功能！

### synchronized关键字的底层原理

> 这个问题也是面试比较高频的一个问题，也是比较难理解的，理解synchronized需要一定的Java虚拟机的知识。

在jdk1.6之前，synchronized被称为重量锁，在jdk1.6中，为了减少获得锁和释放锁带来的性能开销，引入了偏向锁和轻量级锁。下面先介绍jdk1.6之前的synchronized原理。

- 对象头 

在HotSpot虚拟机中(JVM实现方式)，Java对象在内存中的布局大致可以分为三部分：**对象头**、**实例数据**和**填充对齐**。因为synchronized用的锁是存在对象头里的，这里我们需要重点了解对象头。如果对象头是数组类型，则对象头由**Mark Word**、**Class MetadataAddress**和**Array length**组成，如果对象头非数组类型，对象头则由**Mark Word**和**Class MetadataAddress**组成。在32位虚拟机中，数组类型的Java对象头的组成如下表：

| 内容                  | 说明                                   | 长度  |
| --------------------- | -------------------------------------- | ----- |
| Mark Word             | 存储对象的hashCode、分代年龄和锁标记位 | 32bit |
| Class MetadataAddress | 存储到对象类型数据的指针               | 32bit |
| Array length          | 数组的长度                             | 32bit |

这里我们需要重点掌握的是**Mark Word**。

- Mark Word 

在运行期间，Mark Word中存储的数据会随着锁标志位的变化而变化，在32位虚拟机中，不同状态下的组成如下：

![图片说明](https://uploadfiles.nowcoder.com/compress/mw1000/images/20210917/975641190_1631836544651/A01DBDB24E4837EAF125865D879783BF) 

其中线程ID表示持有偏向锁线程的ID，Epoch表示偏向锁的时间戳，偏向锁和轻量级锁是在jdk1.6中引入的。

- 重量级锁的底部实现原理：Monitor 

正所谓Object Monitor机制的名称一样，它就是以Java对象为基础的，在多线程环境下对特定对象的操作权限的一种控制方式。在这种控制方式中有三个象限：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20181210223759413.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lpbndlbmppZQ==,size_20,color_FF00FF,t_70)

第一个象限为待进入**监控区部分（Entry Set）**，停留在这个区域的线程由于还没有获得对象操作权限的原因，依然停留在synchronized同步块以外，具体来说就是synchronized(Object)这句代码的位置。处于“Entry Set”区域的线程，其线程状态被标识为BLOCKED。

第二个象限为**对象操作权持有区（Owner）**，对于一个特定对象的Object Monitor控制来说，**一个时间点最多有一个线程处于这个区域**。也就是说一个时间点只可能有一个线程能拥有这个对象的操作权限。而当前持有对象操作权限的线程互斥量将被记录在这个对象的对象头中。

另外请明确，本专题之前已经介绍过操作权和抢占权之间的关系：某一个线程通过**wait等相关方法释放了对象的操作权限**，但是只要这个线程没有退出synchronized同步块，就不会释放这个对象的抢占权。这是因为没有退出synchronized同步块，且**暂时没有对象操作权限**的线程都会被**放置到待授权区域**（Wait Set）。

但是并不是处于待授权区（Wait Set）的线程都可以重新参与对象操作权的抢占，而是只有通过notify()或者相似方法被通知转移的线程能够参与。

### 为什么`wait()`、`notify()`等方法要在同步方法或同步代码块中来执行呢

这里就能找到原因，是因为`wait()`、`notify()`方法需要借助ObjectMonitor对象内部方法来完成。

答：Java虚拟机是通过进入和退出Monitor对象来实现代码块同步和方法同步的，代码块同步使用的是monitorenter 和 monitorexit 指令实现的，而方法同步是通过Access flags后面的标识来确定该方法是否为同步方法。



### 当线程进入到 synchronized 处尝试获取该锁时， synchronized 锁升级流程如下

![](C:/Users/50131/Desktop/春招笔记/images/dnm68znn6a.png)

如上图所示， synchronized 锁升级的顺序为：偏向锁->轻量级锁->重量级锁，每一步触发锁升级的情况如下：

- **偏向锁**  

介绍:如果在运行过程中，同步锁只有一个线程访问，不存在多线程争用的情况，则线程是不需要触发同步的，这种情况下，就会给线程加一个偏向锁。如果在运行过程中，遇到了其他线程抢占锁，则持有偏向锁的线程会被挂起，JVM会消除它身上的偏向锁，将锁升级到标准的轻量级锁。  
使用场景:仅一个线程执行同步块  
缺点:遇到竞争，撤销偏向锁并升级为轻量级锁的过程会导致性能下降。

- **轻量级锁**:  

介绍:如果成功使用CAS将对象头中的Mark Word中轻量级锁**指向栈中锁记录的指针**替换为指向当前线程锁记录(Lock Record)的指针，则获得锁，失败则当前线程尝试使用自旋(循环等待)来获取锁。

当竞争线程的自旋次数 达到界限值（threshold），轻量级锁将会膨胀为重量级锁。为了防止继续自旋，一旦升级，将无法降级。

缺点:如果始终得不到锁竞争的线程，使用自旋会消耗cpu

- **重量级锁**

锁被占有时，其他线程试图获取锁时，都会被阻塞，只有持有锁的线程释放锁之后才会唤醒这些线程，进行竞争。

缺点:线程阻塞，响应时间缓慢。


- **锁升级策略**

锁主要存在四种状态，依次是：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态，他们会随着竞争的激烈而逐渐升级。注意锁可以升级不可降级，这种策略是为了提高获得锁和释放锁的效率。

- **可重入**

synchronized 拥有强制原子性的内部锁机制，是一把可重入锁。因此，在一个线程使用 synchronized 方法时调用该对象另一个 synchronized 方法，即一个线程得到一个对象锁后再次请求该对象锁，是永远可以拿到锁的。在 Java 中线程获得对象锁的操作是以线程为单位的，而不是以调用为单位的。 synchronized 锁的对象头的 markwork 中会记录该锁的线程持有者和计数器，当一个线程请求成功后， JVM 会记下持有锁的线程，并将计数器计为1。此时其他线程请求该锁，则必须等待。而该持有锁的线程如果再次请求这个锁，就可以再次拿到这个锁，同时计数器会递增。当线程退出一个  synchronized 方法/块时，计数器会递减，如果计数器为 0 则释放该锁锁。

**悲观锁(互斥锁、排他锁)**

 synchronized 是一把悲观锁(独占锁)，当前线程如果获取到锁，会导致其它所有需要锁该的线程等待，一直等待持有锁的线程释放锁才继续进行锁的争抢。



### [浅谈偏向锁, 轻量级锁, 重量级锁](https://www.jianshu.com/p/36eedeb3f912)

<img src="https://upload-images.jianshu.io/upload_images/4491294-e3bcefb2bacea224.png?imageMogr2/auto-orient/strip"  />



## Lock

### synchronized 和 Lock 锁之间的区别

从性能上来讲，当并发量高、竞争激烈的场景下，Lock 锁会较 synchronized 性能上表现的 更稳定些。反之，当并发量不高的情况下，synchronized 有分级锁的优势，因此两者性能差不多，synchronized 相对来说使用上更加简单，不用考虑手动释放锁。

```text
Synchronized是java关键字,Lock是一个类  
Synchronized锁基于JVM实现，Lock基于JDK  
Synchronized不能判断是否获取到锁，Lock可以判断  
Synchronized不需要显示加锁解锁，Lock需要
Synchronized是可重入、非公平锁，Lock可重入，可实现公平/非公平  
并发量大选Lock，并发程度不高选Synchronized
```

[聊聊 Java 的几把 JVM 级锁 - 云+社区 - 腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/1587489)

[让人头大的各种锁，从这里让你思绪清晰 - 云+社区 - 腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/1516281)

#### ReentrantLock 对比 synchronized 

同:两者都是可重入锁,都可以保证并发编程的原子性和可见性。  
**"可重入锁"**:当前线程获取对象锁后，此时对象锁还未释放，当前线程想再次获取该对象锁还是可以获取，如果锁无法重入，会造成死锁。  

ReentrantLock 比 synchronized 增加了一些高级功能  

- 等待可中断 : ReentrantLock提供了一种能够中断等待锁的线程的机制，通过 lock.lockInterruptibly() 来实现这个机制。也就是说正在等待的线程可以选择放弃等待，改为处理其他事情。
- 可实现公平锁 : ReentrantLock可以指定是公平锁还是非公平锁。而synchronized只能是非公平锁。所谓的公平锁就是先等待的线程先获得锁，非公平锁不严格按照队列顺序让线程获得锁。ReentrantLock默认情况是非公平的，可以通过 ReentrantLock类的ReentrantLock(boolean fair)构造方法来制定是否是公平的。
- 可实现选择性通知（锁可以绑定多个条件）: synchronized关键字与wait()和notify()/notifyAll()方法相结合可以实现等待/通知机制。ReentrantLock类当然也可以实现，但是需要借助于Condition接口与newCondition()方法。

#### ReentrantReadWriteLock

ReentrantReadWriteLock 借鉴了读写锁的一个思想，内部维护了两个锁，一个用于读操作，一个用于写操作。

对于读多写少的场景使用ReentrantReadWriteLock 性能会比ReentrantLock高出不少。在多线程读时互不影响，不像ReentrantLock即使是多线程读也需要每个线程获取锁。不过任何一个线程在写的时候就和ReentrantLock类似，其他线程无论读还是写都必须获取锁。


## volatile关键字 * * *

### volatile的作用是什么？

`volatile`是一个轻量级的`synchronized`，一般作用于**变量**，在多处理器开发的过程中保证了内存的可见性。相比于`synchronized`关键字，`volatile`关键字的执行成本更低，效率更高。



### volatile的特性有哪些？

> 并发编程的三大特性为可见性、有序性和原子性。通常来讲`volatile`可以保证可见性和有序性。

- 可见性：`volatile`可以保证不同线程对共享变量进行操作时的可见性。即当一个线程修改了共享变量时，另一个线程可以读取到共享变量被修改后的值。 
- 有序性：`volatile`会通过禁止指令重排序进而保证有序性。 
- 原子性：对于单个的`volatile`修饰的变量的读写是可以保证原子性的，但对于`i++`这种复合操作并不能保证原子性。这句话的意思基本上就是说`volatile`不具备原子性了。 



### Java内存的可见性问题

Java的内存模型如下图所示。

![图片说明](https://uploadfiles.nowcoder.com/images/20210917/975641190_1631836621934/46883DED9E9CB826A77E838A4B30A03D) 

这里的本地内存并不是真实存在的，只是Java内存模型的一个抽象概念，它包含了控制器、运算器、缓存等。同时Java内存模型规定，线程对共享变量的操作必须在自己的本地内存中进行，不能直接在主内存中操作共享变量。这种内存模型会出现什么问题呢？，

1. 线程A获取到共享变量X的值，此时本地内存A中没有X的值，所以加载主内存中的X值并缓存到本地内存A中，线程A修改X的值为1，并将X的值刷新到主内存中，这时主内存及本地内存A中的X的值都为1。 
2. 线程B需要获取共享变量X的值，此时本地内存B中没有X的值，加载主内存中的X值并缓存到本地内存B中，此时X的值为1。线程B修改X的值为2，并刷新到主内存中，此时主内存及本地内存B中的X值为2，本地内存A中的X值为1。 
3. 线程A再次获取共享变量X的值，此时本地内存中存在X的值，所以直接从本地内存中A获取到了X为1的值，但此时主内存中X的值为2，到此出现了所谓**内存不可见**的问题。 

该问题Java内存模型是通过`synchronized`关键字和`volatile`关键字就可以解决。



### 为什么代码会重排序？

计算机在执行程序的过程中，编译器和处理器通常会对指令进行重排序，这样做的目的是为了提高性能。具体可以看下面这个例子。

```
int a = 1;int b = 2;int a1 = a;int b1 = b;int a2 = a + a;int b2 = b + b;......
```

像这段代码，不断地交替读取a和b，会导致寄存器频繁交替存储a和b，使得代码性能下降，可对其进入如下重排

```
int a = 1;int b = 2;int a1 = a;int a2 = a + a;int b1 = b;int b2 = b + b;......
```

按照这样地顺序执行代码便可以避免交替读取a和b，这就是重排序地意义。

指令重排序一般分为**编译器优化重排、指令并行重拍和内存系统重排**三种。

- 编译器优化重排：**编译器在不改变单线程程序语义的情况下，可以对语句的执行顺序进行重新[排序]()**。 
- 指令并行重排：现代处理器多采用指令级并行技术来将多条指令重叠执行**。对于不存在数据依赖的程序，处理器可以对机器指令的执行顺序进行重新排列。** 
- 内存系统重排：因为处理器使用缓存和读/写缓冲区，使得加载（load）和存储（store）看上去像是在乱序执行。 

注：简单解释下数据依赖性：如果两个操作访问了同一个变量，并且这两个操作有一个是写操作，这两个操作之间就会存在数据依赖性，例如：

```
a = 1;b = a;
```

如果对这两个操作的执行顺序进行重排序的话，那么结果就会出现问题。

> 其实，这三种指令重排说明了一个问题，就是指令重排在单线程下可以提高代码的性能，但在多线程下可以会出现一些问题。



### 重排序会引发什么问题？

前面已经说过了，在单线程程序中，重排序并不会影响程序的运行结果，而在多线程场景下就不一定了。可以看下面这个经典的例子，该示例出自《Java并发编程的艺术》。

```
class ReorderExample{    
	int a = 0;    
	boolean flag = false;    
	public void writer(){        
		a = 1;              // 操作1        
		flag = true;        // 操作2  
        public void reader(){        
			if(flag){          // 操作3            
			int i = a + a; // 操作4        
		}  
	}    
}
```

假设线程1先执行`writer()`方法，随后线程2执行`reader()`方法，最后程序一定会得到正确的结果吗？

答案是不一定的，如果代码按照下图的执行顺序执行代码则会出现问题。

![图片说明](https://uploadfiles.nowcoder.com/images/20210917/975641190_1631836649414/192FAAFCD46A1A7CC25A5870CA93505A) 

操作1和操作2进行了重排序，线程1先执行`flag=true`，然后线程2执行操作3和操作4，线程2执行操作4时不能正确读取到`a`的值，导致最终程序运行结果出问题。这也说明了在多线程代码中，重排序会破坏多线程程序的语义。







### voliatile的实现原理？

### volatile实现内存可见性原理

`volatile`可以保证内存可见性的关键是`volatile`的读/写实现了**缓存一致性**，缓存一致性的主要内容为：

- 每个处理器会通过嗅探总线上的数据来查看自己的数据是否过期**，一旦处理器发现自己缓存对应的内存地址被修改，就会将当前处理器的缓存设为无效状态**。此时，如果处理器需要获取这个数据需重新从主内存将其读取到本地内存。 
- 当处理器写数据时，**如果发现操作的是共享变量，会通知其他处理器将该变量的缓存设为无效状态**。 



### volatile实现有序性原理

为了实现`volatile`的内存语义，编译器在生成字节码时会通过插入**内存屏障**来禁止指令重排序。

#### asifserial规则和happensbefore规则？

区别：

- as-if-serial定义：**无论编译器和处理器如何进行重排序，单线程程序的执行结果不会改变**。 

- happens-before定义：一个操作happens-before另一个操作，表**示第一个的操作结果对第二个操作可见，并且第一个操作的执行顺序也在第二个操作之前**。但这并不意味着Java虚拟机必须按照这个顺序来执行程序。**如果重排序的后的执行结果与按happens-before关系执行的结果一致，Java虚拟机也会允许重排序的发生**。 
- happens-before关系保证了**同步的多线程程序的执行结果**不被改变，as-if-serial保证了**单线程内程序的执行结果**不被改变。 

相同点：**happens-before和as-if-serial的作用都是在不改变程序执行结果的前提下，提高程序执行的并行度**。

内存屏障：内存屏障是一种CPU指令，它的作用是**对该指令前和指令后的一些操作产生一定的约束，保证一些操作按顺序执行**。

### Java虚拟机插入内存屏障的策略

Java内存模型把内存屏障分为4类，如下表所示：

| 屏障类型            | 指令示例                 | 说明                                                         |
| ------------------- | ------------------------ | ------------------------------------------------------------ |
| LoadLoad Barriers   | Load1;LoadLoad;Load2     | 保证Load1数据的读取先于Load2及后续所有读取指令的执行         |
| StoreStore Barriers | Store1;StoreStore;Store2 | 保证Store1数据刷新到主内存先于Store2及后续所有存储指令       |
| LoadStore Barriers  | Load1;LoadStore;Store2   | 保证Load1数据的读取先于Store2及后续的所有存储指令刷新到主内存 |
| StoreLoad Barriers  | Store1;StoreLoad;Load2   | 保证Store1数据刷新到主内存先于Load2及后续所有读取指令的执行  |

注：StoreLoad Barriers同时具备其他三个屏障的作用，它会使得该屏障之前的所有内存访问指令完成之后，才会执行该屏障之后的内存访问命令。

Java内存模型对编译器指定的`volatile`重[排序]()规则为：

- 当第一个操作是`volatile`读时，无论第二个操作是什么都不能进行重[排序]()。 
- 当第二个操作是`volatile`写时，无论第一个操作是什么都不能进行重[排序]()。 
- 当第一个操作是`volatile`写，第二个操作为`volatile`读时，不能进行重[排序]()。 

根据`volatile`重[排序]()规则，Java内存模型采取的是保守的屏障插入策略，`volatile`写是在前面和后面分别插入内存屏障，`volatile`读是在后面插入两个内存屏障，具体如下：

- `volatile`读：在每个`volatile`读后面分别插入LoadLoad屏障及LoadStore屏障（根据`volatile`重[排序]()规则第一条），如下图所示 

![图片说明](https://uploadfiles.nowcoder.com/images/20210917/975641190_1631836668674/22D1731649299A8B8D877B0CB3DD4B22) 

 LoadLoad屏障的作用：禁止上面的所有普通读操作和上面的`volatile`读操作进行重[排序]()。

 LoadStore屏障的作用：禁止下面的普通写和上面的`volatile`读进行重[排序]()。

- `volatile`写：在每个`volatile`写前面插入一个StoreStore屏障（为满足`volatile`重[排序]()规则第二条），在每个`volatile`写后面插入一个StoreLoad屏障（为满足`volatile`重[排序]()规则第三条），如下图所示

  ![图片说明](https://uploadfiles.nowcoder.com/images/20210917/975641190_1631836680766/41CC93A488C029344D263331CB3A198D) 

  StoreStore屏障的作用：禁止上面的普通写和下面的`volatile`写重[排序]()

  StoreLoad屏障的作用：防止上面的`volatile`写与下面可能出现的`volatile`读/写重[排序]()。



### 编译器对内存屏障插入策略的优化

> 因为Java内存模型所采用的屏障插入策略比较保守，所以在实际的执行过程中，只要不改变`volatile`读/写的内存语义，编译器通常会省略一些不必要的内存屏障。

指令序列示意图如下：

![图片说明](https://uploadfiles.nowcoder.com/images/20210917/975641190_1631836718946/6F5AC7ADB4D2A1223263AE6E1A57CA21) 

从上图可以看出，通过指令优化一共省略了两个内存屏障（虚线表示），省略第一个内存屏障LoadStore的原因是最后的普通写不可能越过第二个`volatile`读，省略第二个内存屏障LoadLoad的原因是下面没有涉及到普通读的操作。



### volatile能使一个非原子操作变成一个原子操作吗？

`volatile`只能保证可见性和有序性，但可以保证64位的`long`型和`double`型变量的原子性。

对于32位的虚拟机来说，每次原子读写都是32位的，会将`long`和`double`型变量拆分成两个32位的操作来执行，这样`long`和`double`型变量的读写就不能保证原子性了，而通过`volatile`修饰的long和double型变量则可以保证其原子性。



### volatile、synchronized的区别？

synchronized 关键字和 volatile 关键字是两个互补的存在，⽽不是对⽴的存在

- `volatile`主要是保证**内存的可见性**，即变量在寄存器中的内存是不确定的，需要从主存中读取。`synchronized`主要是解决**多个线程访问资源的同步性**。 

- `volatile`仅可以保证数据的可见性，不能保证数据的原子性。`synchronized`可以保证数据的可见性和原子性。 

- `volatile`不会造成线程的阻塞，`synchronized`会造成线程的阻塞。 

- volatile 关键字是线程同步的轻量级实现，所以 volatile 性能肯定比 synchronized 关键字要好。但是 volatile 关键字只能用于变量而 synchronized 关键字可以修饰⽅法以及代码块。

  



## ConcurrentHashMap * * *

### ConcurrentHashMap 底层具体实现

* JDK1.7

  在JDK1.7中，`ConcurrentHashMap`采用`Segment`数组 + `HashEntry`数组的方式进行实现。`Segment`实现了`ReentrantLock`，所以`Segment`有锁的性质，`HashEntry`用于存储键值对。一个`ConcurrentHashMap`包含着一个`Segment`数组，一个`Segment`包含着一个`HashEntry`数组，`HashEntry`是一个[链表]()结构，如果要获取`HashEntry`中的元素，要先获得`Segment`的锁

* JDK1.8

  JDK1.8是`HashMap`类似的结构，Node数组+[链表]()/[红黑树]()，采用`CAS`+`synchronized`来保证线程安全。当[链表]()长度大于8，[链表]()转化为[红黑树]()。在JDK1.8中`synchronized`只锁[链表]()或[红黑树]()的头节点，是一种相比于`segment`更为细粒度的锁，锁的竞争变小，所以效率更高。

  

  1.实现结构上的不同，JDK1.7是基于Segment实现的，JDK1.8是基于Node数组+[链表]()/[红黑树]()实现的。

  2.保证线程安全方面：JDK1.7采用了分段锁的机制，当一个线程占用锁时，会锁住一个Segment对象，不会影响其他Segment对象。JDK1.8则是采用了**CAS和`synchronize`**的方式来保证线程安全发生哈希冲突时，使用synchronized，没有发生时使用cas 。

### ConcurrentHashMap结构中变量使用volatile和final修饰有什么作用？

 `final`修饰变量可以保证变量不需要同步就可以被访问和共享，`volatile`可以保证内存的可见性，配合CAS操作可以在不加锁的前提支持并发。

### ConcurrentHashMap有什么缺点？

因为`ConcurrentHashMap`在更新数据时只会锁住部分数据，并不会将整个表锁住，读取的时候也并不能保证读取到最近的更新，只能保证读取到已经顺利插入的数据。

### ConCurrentHashMap 的key，value是否可以为null？为什么？HashMap中的key、value是否可以为null？

`ConCurrentHashMap`中的`key`和`value`为`null`会出现空指针异常，而`HashMap`中的`key`和`value`值是可以为`null`的。

原因如下：`ConCurrentHashMap`是在多线程场景下使用的，如果`ConcurrentHashMap.get(key)`的值为`null`，那么无法判断到底是`key`对应的`value`的值为`null`还是不存在对应的`key`值。而在单线程场景下的`HashMap`中，可以使用`containsKey(key)`来判断到底是不存在这个`key`还是`key`对应的`value`的值为`null`。**在多线程的情况下使用`containsKey(key)`来做这个判断是存在问题的，因为在`containsKey(key)`和`ConcurrentHashMap.get(key)`两次调用的过程中，`key`的值已经发生了改变。**

### ConcurrentHashMap迭代器是强一致性还是弱一致性？

与HashMap不同的是，`ConcurrentHashMap`迭代器是弱一致性。

这里解释一下弱一致性是什么意思，当`ConcurrentHashMap`的迭代器创建后，会遍历[哈希表]()中的元素，在遍历的过程中，[哈希表]()中的元素可能发生变化，如果这部分变化发生在已经遍历过的地方，迭代器则不会反映出来，如果这部分变化发生在未遍历过的地方，迭代器则会反映出来。换种说法就是**`put()`方法将一个元素加入到底层数据结构后，`get()`可能在某段时间内还看不到这个元素**。

这样的设计主要是为`ConcurrenthashMap`的性能考虑，**如果想做到强一致性，就要到处加锁，性能会下降很多**。所以`ConcurrentHashMap`是支持在迭代过程中，向map中添加元素的，而**`HashMap`这样操作则会抛出异常**。



## ThreadLocal * * *

### ThreadLocal 了解么？

如果**想实现每⼀个线程都有自己的专属本地变量**该如何解决呢？ JDK 中提供的`ThreadLocal` 类正是为了解决这样的问题。`ThreadLocal` 类主要解决的就是让每个线程绑定⾃⼰的值，可以将 `ThreadLocal` 类形象的⽐喻成存放数据的盒子，盒子中可以存储每个线程的私有数据。**如果你创建了⼀个 `ThreadLocal` 变量，那么访问这个变量的每个线程都会有这个变量的本地副本**，这也是 `ThreadLocal` 变量名的由来。他们可以使⽤ get() 和 set()方法来获取默认值或将其值更改为当前线程所存的副本的值，从而避免了线程安全问题。  



### 什么是ThreadLocal？有哪些应用场景？

`ThreadLocal`是 JDK java.lang 包下的一个类，`ThreadLocal`为变量在每个线程中都创建了一个副本，那么每个线程可以访问自己内部的副本变量，并且不会和其他线程的局部变量冲突，实现了线程间的数据隔离。

`ThreadLocal`的应用场景主要有以下几个方面：

- 保存线程上下文信息，在需要的地方可以获取 (线程内的资源共享)
- **线程间数据隔离** 
- **数据库连接池**：解决数据库事务，保证多线程状态执行事务操作拿到的是同一个连接。



### ThreadLocal原理和内存泄露？

`ThreadLocal`的原理可以概括为下图：

![图片说明](https://uploadfiles.nowcoder.com/images/20210917/975641190_1631836786239/C624F7AF291E1F5787F73056CBA2236B) 

从上图可以看出每个线程都有一个`ThreadLocalMap`，`ThreadLocalMap`中保存着所有的`ThreadLocal`，而`ThreadLocal`本身只是一个引用本身并不保存值，值都是保存在`ThreadLocalMap`中的，其中**`ThreadLocal`为`ThreadLocalMap`中的`key`**。其中图中的虚线表示弱引用。

这里简单说下Java中的引用类型，Java的引用类型主要分为强引用、软引用、弱引用和虚引用。

- 强引用：发生 gc 的时候不会被回收。 
- 软引用：有用但不是必须的对象，在发生内存溢出之前会被回收。 
- 弱引用：有用但不是必须的对象，在下一次GC时会被回收。 
- 虚引用：无法通过虚引用获得对象，虚引用的用途是在 gc 时返回一个通知，具有虚引用的对象配合引用队列使用，GC前无法访问对象，GC之后通过访问队列获取对象。 

```java
static class Entry extends WeakReference<ThreadLocal<?>> {
    /** The value associated with this ThreadLocal. */
    Object value;
    Entry(ThreadLocal<?> k, Object v) {
        super(k);
        value = v;
    }
}
```

[面试官，ThreadLocal 你别再问了！ - 云+社区 - 腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/1551859)



### **为什么ThreadLocal会发生内存泄漏呢？**

因为`ThreadLocal`中的**`key`是弱引用**，而**`value`是强引用**。当`ThreadLocal`没有被强引用时，在进行垃圾回收时，`key`会被清理掉，而`value`不会被清理掉，这时**如果不做任何处理，`value`将永远不会被回收，产生内存泄漏**。



### **如何解决ThreadLocal的内存泄漏？**

其实在`ThreadLocal`在设计的时候已经考虑到了这种情况，在调用`set()`、`get()`、`remove()`等方法时就会清理掉`key`为`null`的记录，所以在**使用完`ThreadLocal`后最好手动调用`remove()`方法**。



### **为什么要将key设计成ThreadLocal的弱引用？**

Entry的key被设计为弱引用就是**为了让程序自动的对访问不到的数据进行回收提醒**，所以，在访问不到的数据被回收之前，内存泄漏确实是存在的，但是我们不用担心，就算我们不调用remove，`ThreadLocalMap`在内部的set，get和扩容时都会清理掉泄漏的Entry，内存泄漏完全没必要过于担心

### ThreadLocalMap和HashMap

另外说一点，`HashMap`是使用**拉链法**解决hash冲突的，`ThreadLocalMap`是使用**线性探测**解决hash冲突的（内部只维护`Entry`数组，没有链表）。所以，源码中在清**除泄漏的Entry时，会进行rehash，防止数组的当前位置为null后，有hash冲突的Entry访问不到的问题。**

[谈谈`ThreadLocal`为什么被设计为弱引用 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/304240519)



## 线程池 * * *

### 什么是线程池？为什么使用线程池

线程池是一种多线程处理形式，处理过程中将任务提交到线程池，任务的执行交给线程池来管理。

**为什么使用线程池？**

- **降低资源消耗**，通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 
- **提高响应速度**，当任务到达时，任务可以不需要等到线程创建就立即执行。 
- **提高线程的可管理性**，线程是稀缺资源，如果无限制地创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以统一分配。 

### 创建线程池的几种方法

《阿里巴巴 Java 开发⼿册》中强制线程池不允许使⽤ Executors 去创建，⽽是通过`ThreadPoolExecutor` 的⽅式，这样的处理⽅式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险
Executors 返回线程池对象的弊端如下：
`FixedThreadPool `和 `SingleThreadExecutor` ： 允许**请求的队列**长度为`Integer.MAX_VALUE` ，可能堆积大量的请求，从而导致 OOM。
`CachedThreadPool` 和 `ScheduledThreadPool` ： 允许**创建的线程数量**为`Integer.MAX_VALUE` ，可能会创建大量线程，从而导致 OOM。  

线程池的常用创建方式主要有两种，通过**Executors工厂方法创建**和**通过new `ThreadPoolExecutor`**方法创建。

- Executors工厂方法创建，在工具类 Executors 提供了一些静态的工厂方法  
  - **`newCachedThreadPool`** 创建一个可缓存线程池，如果线程池长度超过处理需要， 可灵活回收空闲线程， 若无可回收， 则新建线程。
  - **`newFixedThreadPool`** 创建一个定长线程池，可控制线程最大并发数， 超出的线程会在队列中等待。
  - **`newScheduledThreadPool`** 创建一个定长线程池，支持定时及周期性任务执行。
  - **`newSingleThreadExecutor`** 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务， 保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。

- `new ThreadPoolExecutor` 方法创建： 通过`new` `ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue workQueue，ThreadFactory threadFactory,RejectedExecutionHandler handler)    `自定义创建 

```
	Executors.newFiexedThreadPool(3);
	//或者
	new ThreadPoolExecutor(corePoolSize, maximumPoolSize, keepAliveTime, TimeUnit unit,workQueue, threadFactory, handler);
```

  

### ThreadPoolExecutor构造函数的重要参数分析

三个比较重要的参数：

- **`corePoolSize`** ：核心线程数，定义了最小可以同时运行的线程数量。 
- **`maximumPoolSize`** ：线程中允许存在的最大工作线程数量 
- **`workQueue`**：存放任务的阻塞队列。新来的任务会先判断当前运行的线程数是否到达核心线程数，如果到达的话，任务就会先放到阻塞队列。 

其他参数：

- **`keepAliveTime`**：当线程池中的数量大于核心线程数时，如果没有新的任务提交，核心线程外的线程不会立即销毁，而是会等到时间超过`keepAliveTime`时才会被销毁。 
- **`unit`** ：`keepAliveTime` 参数的时间单位。 
- **`threadFactory`**：为线程池提供创建新线程的线程工厂。 
- **`handler`** ：线程池任务队列超过`maxinumPoolSize` 之后的拒绝策略 

### ThreadPoolExecutor的饱和策略（拒绝策略）

当同时运行的线程数量达到最大线程数量并且阻塞队列也已经放满了任务时，`ThreadPoolExecutor`会指定一些饱和策略。主要有以下四种类型：

- `AbortPolicy`策略：该策略会直接抛出异常拒绝新任务 
- `CallerRunsPolicy`策略：当线程池无法处理当前任务时，会将该任务交由提交任务的线程来执行。 
- `DiscardPolicy`策略：直接丢弃新任务。 
- `DiscardOleddestPolicy`策略：丢弃最早的未处理的任务请求。 

### 线程池的执行流程

核心线程 ：固定线程数 可闲置 不会被销毁 ThreadPoolExecutor的allowCoreThreadTimeOut属性设置为true时，keepAliveTime同样会作用于核心线程

非核心线程数：非核心线程闲置时的超时时长，超过这个时长，非核心线程就会被回收

创建线程池创建后提交任务的流程如下图所示：

![图片说明](https://uploadfiles.nowcoder.com/images/20210917/975641190_1631836804382/9E1CDD1864EB6C294D72EA5DFFC2C59B) 

### execute()方法和submit()方法的区别

> 这个地方首先要知道Runnable接口和Callable接口的区别，之前有写到过

`execute()`和`submit()`的区别主要有两点：

- `execute()`方法只能执行`Runnable` 类型的任务。`submit()`方法可以执行`Runnable`和 `Callable`类型的任务。 
- `submit()`方法可以返回持有计算结果的`Future`对象，同时还可以抛出异常，而`execute()`方法不可以。 

换句话说就是，`execute()`方法用于提交不需要返回值的任务，`submit()`方法用于需要提交返回值的任务。

### 线程池阻塞队列

| 线程池              | 阻塞队列            |
| ------------------- | ------------------- |
| FixedThreadPool     | LinkedBlockingQueue |
| SingleThreadPool    | LinkedBlockingQueue |
| CachedThreadPool·   | SynchronousQueue    |
| ScheduledThreadPool | DelayedWorkQueue    |

### LinkedBlockingQueue

第一种阻塞队列是 LinkedBlockingQueue，它的容量是 Integer.MAX_VALUE，为 231 -1 ，是一个非常大的值，可以认为是无界队列。FixedThreadPool 和 SingleThreadExecutor 线程池的线程数是固定的，所以没有办法增加特别多的线程来处理任务，这时就需要 LinkedBlockingQueue 这样一个没有容量限制的阻塞队列来存放任务。

### SynchronousQueue

SynchronousQueue不是一个真正的队列，没法保存任务，它是**一种在线程之间进行移交的机制**。如果要将一个元素放入SynchronousQueue中，必须有另一个线程正在等待接受这个元素。如果没有线程等待，所有线程都在忙碌，并且池中的线程数尚未达到最大，那么ThreadPoolExecutor将创建一个新的线程。否则根据饱和策略，这个任务将被拒绝。


### DelayedWorkQueue

ScheduledThreadPool 这种线程池的最大特点就是可以延迟执行任务，比如说一定时间后执行任务或是每隔一定的时间执行一次任务。DelayedWorkQueue 的特点是内部元素并不是按照放入的时间排序，而是会按照延迟的时间长短对任务进行排序，内部采用的是“堆”的数据结构（堆的应用之一就是 **优先级队列**）。

#### 线程池调优

1、**任务性质不同的任务**可以用**不同规模的线程池**分开处理。**CPU 密集型任务配置** 尽可能小的线程，**IO 密集型任务**则由于线程并不是一直在执行任务，则配置尽可能多的线程

2、调整额外线程存活时间

3、选择合适线程池队列

4、选择合适饱和策略

## CAS * * *

### 什么是CAS?

CAS即`CompareAndSwap`，翻译成中文即**比较并替换**。Java中可以通过CAS操作来保证原子性，原子性就是不可被中断的一些列操作或者一个操作，简单来说就是一系列操作，要么全部完成，要么失败，不能被中断。

CAS核心算法：执行函数：CAS(V，E，N) 

```
V表示准备要被更新的变量       
E表示我们提供的 期望的值
N表示新值 ，准备更新V的值
```

算法思路：V是共享变量，我们拿着自己准备的这个E，去跟V去比较，如果E == V ，说明当前没有其它线程在操作，所以，我们把N 这个值 写入对象的 V 变量中。如果 E ！= V ，说明我们准备的这个E，已经过时了，所以我们要重新准备一个最新的E ，去跟V 比较，比较成功后才能更新 V的值为N。
<img src="https://img-blog.csdn.net/20170701155737036?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvamF2YXplamlhbg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="è¿éåå¾çæè¿°"  />



### CAS存在的问题

> 其中ABA问题是面试中比较常见的问题

- ABA问题 

在CAS的[算法]()流程中，首先要先比较V的值和E的值，如果相等则进行更新。ABA问题是指，E表示的这个旧值本来是A，然后变成了B，后来又变成了A，但这时有线程来更新，发现E表示的值是A，则直接进行更新了，这样肯定是不对的，但又该怎么解决呢？

ABA的问题的解决方式：ABA的解决方法也很简单，就是利用**版本号**。给变量加上一个版本号，每次变量更新的时候就把版本号加1，这样即使E的值从A—>B—>A，版本号也发生了变化，这样就解决了CAS出现的ABA问题。基于CAS的乐观锁也是这个实现原理。

- 循环时间过长导致开销太大 

CAS自旋时间过长会给CPU带来非常大的开销

- 只能保证一个**共享变量的原子操作** 

在操作一个共享变量时，可以通过CAS的方式保证操作的原子性，但如果对多个共享变量进行操作时，CAS则无法保证操作的原子性，这时候就需要用锁了。在看《Java并发编程的艺术》时，里面提到了一个办法可以参考一下，就是将多个共享变量合并成一个共享变量来操作。比如，有两个共享变量`i=2,j=a`，合并成`ij=2a`，然后用CAS来操作`ij`



### CAS的优点

在并发量不是很大时提高效率。



## Atomic 原子类

> 原子操作类是**CAS和Volatile**在Java中的应用，从JDK1.5开始提供了`java.util.concurrent.atomic`包，这个包中的原子操作类提供了一种用法简单、性能高效、线程安全地更新一个变量的方式。Atomic包里的类基本都是使用`Unsafe`实现的包装类。在我们这里 Atomic 是指⼀个操作是不可中断的。即使是在多个线程⼀起执行的时候，⼀个操作⼀旦开始，就不会被其他线程干扰。所以，所谓原子类说简单点就是具有原子/原子操作特征的类。  

JUC包中的4种原子类

- 基本类型

  使用原子的方式更新基本类型  

  - `AtomicInteger`：整形原子类 
  - `AtomicLong`：长整型原子类 
  - `AtomicBoolean`：布尔型原子类 

- 数组类型

  使用原子的方式更新数组里的某个元素  

  - `AtomicIntegerArray`：整形数组原子类 
  - `AtomicLongArray`：长整形数组原子类 
  - `AtomicReferenceArray`：引用类型数组原子类 

- 引用类型

  - `AtomicReference`：引用类型原子类，存在ABA问题 
  - `AtomicStampedReference`：原子更新带有版本号的引用类型。该类将整数值与引用关联起来，可用于原子的更新数据和数据的版本号，可以解决使用CAS进行原子更新时可能出现的ABA问题。 
  - `AtomicMarkableReference`：原子更新带有标记位的引用类型 

- 原子更新字段类

  - `AtomicIntegerFieldUpdater`：原子更新整型的字段的更新器。 
  - `AtomicLongFieldUpdater`：原子更新长整型字段的更新器。 
  - `AtomicReferenceFieldUpdater`：引用类型更新器原子类 

### CAS是如何应用在Atomic
```java
  // setup to use Unsafe.compareAndSwapInt for updates
  public class AtomicInteger extends Number implements java.io.Serializable {
      private static final Unsafe unsafe = Unsafe.getUnsafe();
        private static final long valueOffset;

      static {
        try {
          valueOffset = unsafe.objectFieldOffset
            (AtomicInteger.class.getDeclaredField("value"));
        } catch (Exception ex) { throw new Error(ex); }
      }

      private volatile int value;
}
```

- **value是一个volatile的int值**。通过volatile的可见性，可以保证有一个线程更新了，其他线程可以得到最新值。

- **Usafe类**，进行真正CAS操作的类

  以incrementAndGet()方法为例,它调用了Unsafe的getAndAddInt方法，具体操作如下。

  首先通过一次volatile读，读到最新的v值，之后再通过compareAndSwapInt这个方法，进行CAS操作。如果CAS操作失败，会返回false，while循环继续执行，进行自旋，重新尝试CAS操作。如果CAS更新操作成功，返回原值。



  


## AQS * *

### 什么是AQS？

AQS的全称是`AbstractQueuedSynchronizer`抽象队列同步器，**是一个用来构建锁和同步器的框架**，像`ReentrantLock`，`Semaphore`，`FutureTask`都是基于AQS实现的。

### AQS的原理

[通过一个生活中的案例场景，揭开并发包底层AQS的神秘面纱 - 云+社区 - 腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/1582528)

AQS的原理图如下：

![图片说明](https://uploadfiles.nowcoder.com/images/20210917/975641190_1631836827287/BCA91CF50E2696B8136634AAAAC6EDF9) 

从上图可以看出，AQS是维护了**一个共享资源和一个FIFO的线程等待队列**。

AQS的工作流程：当被请求的共享资源空闲，则将请求资源的线程设为有效的工作线程，同时锁定共享资源。如果被请求的资源已经被占用了，AQS就用过队列实现了一套线程阻塞等待以及唤醒时锁分配的机制。这个机制 AQS 是用CLH 队列锁实现的，即将暂时获取不到锁的线程加人到队列中。

这个队列是通过CLH队列实现的，从上图可以看出，该队列是一个双向队列，由Node结点组成，每个Node结点维护一个prev引用和next引用，这两个引用分别指向自己结点的前驱结点和后继结点，同时AQS还维护两个指针Head和Tail，分别指向队列的头部和尾部。

通过`volatile`来保证state的线程可见性

```
private volatile int state;//共享变量，使⽤volatile修饰保证线程可⻅性
```

状态信息通过 protected 类型的 `getState`， `setState`， `compareAndSetState` 进行操作  

```
//返回同步状态的当前值
protected final int getState() {
	return state;
}
// 设置同步状态的值
protected final void setState(int newState) {
	state = newState;
}
//原⼦地（CAS操作）将同步状态值设置为给定值update如果当前同步状态的值等于expect（期望值）
protected final boolean compareAndSetState(int expect, int update) {
	return unsafe.compareAndSwapInt(this, stateOffset, expect, update);
}
```



### AQS的资源共享方式有哪些？

- Exclusive：独占，只有一个线程可以执行，例如`ReentrantLock` 
- Share：共享，多个线程可同时执行，如`Semaphore/CountDownLatch` 

`ReentrantReadWriteLock` 可以看成是组合式，因为 `ReentrantReadWriteLock` 也就是读写锁允许多个线程同时对某⼀资源进⾏读。
不同的自定义同步器争⽤共享资源的⽅式也不同。⾃定义同步器在实现时只需要实现共享资源state 的获取与释放⽅式即可，⾄于具体线程等待队列的维护（如获取资源失败⼊队/唤醒出队等）， AQS 已经在顶层实现好了。  



### 如何使用AQS自定义同步器？

AQS的底层使用了模板方法模式，自定义同步器只需要两步：第一，继承`AbstractQueuedSynchronizer`，第二，重写以下几种方法：

```
isHeldExclusively()：该线程是否正在独占资源。只有用到condition才需要去实现它。 
tryAcquire(int)：独占方式，尝试获取资源。 
tryRelease(int)：独占方式，尝试释放资源。 
tryAcquireShared(int)：共享方式，尝试获取资源。负数表示失败，0表示成功，但无剩余可用资源，正数表示成功并且有剩余资源 
tryReleaseShared(int)：共享方式，尝试释放资源 
```

下面举例说明，以独占式的`ReentrantLock`为例，`state`初始状态为0，表示未锁定状态。A线程进行`lock()`时，会调用`tryAcquire()`独占该锁并将`state+1`。此后，其他线程再调用`tryAcquire()`时就会失败，直到A线程`unlock()`到state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A线程自己是可以重复获取此锁的（state会累加），这就是可重入的概念。但要注意，获取多少次就要释放多么次，这样才能保证state是能回到零态的。

![](C:/Users/50131/Desktop/春招笔记/images/w5yjowlvrb.jpeg)

再以共享使得`CountDownLatch`以例，任务分为N个子线程去执行，state也初始化为N（注意N要与线程个数一致）。这N个子线程是并行执行的，每个子线程执行完后`countDown()`一次，state会CAS减1。等到所有子线程都执行完后(即state=0)，会`unpark()`主调用线程，然后主调用线程就会从`await()`函数返回，继续后余动作。

一般来说，自定义同步器要么是独占方法，要么是共享方式，他们也只需实现`tryAcquire-tryRelease`、`tryAcquireShared-tryReleaseShared`中的一种即可。但AQS也支持自定义同步器同时实现独占和共享两种方式，如`ReentrantReadWriteLock`。
推荐两篇 AQS 原理和相关源码分析的⽂章：
http://www.cnblogs.com/waterystone/p/4920797.html
https://www.cnblogs.com/chengxiao/archive/2017/07/24/7141160.html



### AQS 组件总结

- Semaphore (信号量)-允许多个线程同时访问： `synchronized` 和 `ReentrantLock` 都是⼀次只允许⼀个线程访问某个资源， Semaphore (信号量)可以指定多个线程同时访问某个资源。

```
public class SemaphoreTest {
    private static final int COUNT = 40;
    private static Executor executor = Executors.newFixedThreadPool(COUNT);
    private static Semaphore semaphore = new Semaphore(10);
    public static void main(String[] args) {
        for (int i=0; i< COUNT; i++) {
            executor.execute(new ThreadTest.Task());
        }
    }

    static class Task implements Runnable {
        @Override
        public void run() {
            try {
                //读取文件操作
                semaphore.acquire();
                // 存数据过程
                semaphore.release();
            } catch (InterruptedException e) {
                e.printStackTrace();
            } finally {
            }
        }
    }
}
```

- `CountDownLatch `（倒计时器）：CountDownLatch是一个计数器闭锁，主要的功能就是通过await()方法来阻塞住当前线程，然后等待计数器减少到0了，再唤起这些线程继续执行。

```
//百米赛跑，4名运动员选手到达场地等待裁判口令，裁判一声口令，选手听到后同时起跑，当所有选手到达终点，裁判进行汇总排名
package com.example.demo.CountDownLatchDemo;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;

public class CountdownLatchTest2 {
    public static void main(String[] args) {
        ExecutorService service = Executors.newCachedThreadPool();
        final CountDownLatch cdOrder = new CountDownLatch(1);
        final CountDownLatch cdAnswer = new CountDownLatch(4);
        for (int i = 0; i < 4; i++) {
            Runnable runnable = new Runnable() {
                @Override
                public void run() {
                    try {
                        System.out.println("选手" + Thread.currentThread().getName() + "正在等待裁判发布口令");
                        cdOrder.await();
                        System.out.println("选手" + Thread.currentThread().getName() + "已接受裁判口令");
                        Thread.sleep((long) (Math.random() * 10000));
                        System.out.println("选手" + Thread.currentThread().getName() + "到达终点");
                        cdAnswer.countDown();
                    } catch (InterruptedException e) {
                        e.printStackTrace();
                    }
                }
            };
            service.execute(runnable);
        }
        try {
            Thread.sleep((long) (Math.random() * 10000));
            System.out.println("裁判"+Thread.currentThread().getName()+"即将发布口令");
            cdOrder.countDown();
            System.out.println("裁判"+Thread.currentThread().getName()+"已发送口令，正在等待所有选手到达终点");
            cdAnswer.await();
            System.out.println("所有选手都到达终点");
            System.out.println("裁判"+Thread.currentThread().getName()+"汇总成绩排名");
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
        service.shutdown();
    }
}
```

- `CyclicBarrier` (循环栅栏)：`CyclicBarrier` 和`CountDownLatch` 非常类似，它也可以实现线程间的技术等待，但是它的功能比 `CountDownLatch` 更加复杂和强⼤。主要应⽤场景和`CountDownLatch` 类似。 `CyclicBarrier` 的字⾯意思是可循环使用（ Cyclic ）的屏障（ Barrier ）。它要做的事情是，让⼀组线程到达⼀个屏障（也可以叫同步点）时被阻塞，直到最后⼀个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活。 `CyclicBarrier` 默认的构造方法是 `CyclicBarrier(int parties)` ，其参数表示屏障拦截的线程数量，每个线程调⽤ await() 方法告诉 `CyclicBarrier` 我已经到达了屏障，然后当前线程被阻塞。 

```
public class CyclicBarrierDemo {

    static class TaskThread extends Thread {
        CyclicBarrier barrier;     
        public TaskThread(CyclicBarrier barrier) {
            this.barrier = barrier;
        }
        
        @Override
        public void run() {
            try {
                Thread.sleep(1000);
                System.out.println(getName() + " 到达栅栏 A");
                barrier.await();
                System.out.println(getName() + " 冲破栅栏 A");
                
                Thread.sleep(2000);
                System.out.println(getName() + " 到达栅栏 B");
                barrier.await();
                System.out.println(getName() + " 冲破栅栏 B");
            } catch (Exception e) {
                e.printStackTrace();
            }
        }
    }
    
    public static void main(String[] args) {
        int threadNum = 5;
        CyclicBarrier barrier = new CyclicBarrier(threadNum, new Runnable() {        
        @Override
        public void run() {
                System.out.println(Thread.currentThread().getName() + " 完成最后任务");
            }
        });
        
        for(int i = 0; i < threadNum; i++) {
            new TaskThread(barrier).start();
        }
    }
    
}
```

打印结果

```
Thread-1 到达栅栏 A
Thread-3 到达栅栏 A
Thread-0 到达栅栏 A
Thread-4 到达栅栏 A
Thread-2 到达栅栏 A
Thread-2 完成最后任务
Thread-2 冲破栅栏 A
Thread-1 冲破栅栏 A
Thread-3 冲破栅栏 A
Thread-4 冲破栅栏 A
Thread-0 冲破栅栏 A
Thread-4 到达栅栏 B
Thread-0 到达栅栏 B
Thread-3 到达栅栏 B
Thread-2 到达栅栏 B
Thread-1 到达栅栏 B
Thread-1 完成最后任务
Thread-1 冲破栅栏 B
Thread-0 冲破栅栏 B
Thread-4 冲破栅栏 B
Thread-2 冲破栅栏 B
Thread-3 冲破栅栏 B
```

### CyclicBarrier 与 CountDownLatch 区别

- CountDownLatch 是一次性的，CyclicBarrier 是可循环利用的
- CountDownLatch 参与的线程的职责是不一样的，有的在倒计时，有的在等待倒计时结束。CyclicBarrier 参与的线程职责是一样的。



### 用过 CountDownLatch 么？什么场景下用的？

`CountDownLatch` 的作⽤就是 允许 count 个线程阻塞在⼀个地方，直到所有线程的任务都执⾏完毕。之前在项⽬中，有⼀个使⽤多线程读取多个⽂件处理的场景，我用到了 `CountDownLatch` 。具体场景是下面这样的：
我们要读取处理 6 个⽂件，这 6 个任务都是没有执行顺序依赖的任务，但是我们**需要返回给用户的时候将这几个文件的处理的结果进行统计整理。**
为此我们定义了⼀个线程池和 count 为 6 的 `CountDownLatch` 对象 。使用线程池处理读取任务，每⼀个线程处理完之后就将 count-1，调⽤ `CountDownLatch` 对象的 await() 方法，直到所有文件读取完之后，才会接着执行后面的逻辑。
伪代码：

```
public class CountDownLatchExample1 {
// 处理⽂件的数量
    private static final int threadCount = 6;
    public static void main(String[] args) throws InterruptedException {
    // 创建⼀个具有固定线程数量的线程池对象（推荐使⽤构造⽅法创建）
    ExecutorService threadPool = Executors.newFixedThreadPool(10);
    final CountDownLatch countDownLatch = new CountDownLatch(threadCount);
    for (int i = 0; i < threadCount; i++) {
        final int threadnum = i;
        threadPool.execute(() -> {
            try {
                //处理⽂件的业务操作
                ......
            } catch (InterruptedException e) {
                e.printStackTrace();
            } finally {
            	//表示⼀个⽂件已经被完成
            	countDownLatch.countDown();
            }
        });
    }
    countDownLatch.await();
    threadPool.shutdown();
    System.out.println("finish");
}
```

有没有可以改进的地⽅呢？
可以使用`CompletableFuture` 类来改进！ Java8 的 `CompletableFuture` 提供了很多对多线程友好的方法，使⽤它可以很⽅便地为我们编写多线程程序，什么异步、串行、并行或者等待所有线程执行完任务什么的都非常方便。上面的代码还可以接续优化，当任务过多的时候，把每⼀个 task 都列出来不太现实，可以考虑通过循环来添加任务。

```
CompletableFuture<Void> task1 = CompletableFuture.supplyAsync(()->{
		//⾃定义业务操作
	});
	......
CompletableFuture<Void> task6 = CompletableFuture.supplyAsync(()->{
		//⾃定义业务操作
	});
	......
CompletableFuture<Void> headerFuture = CompletableFuture.allOf(task1,.....,task6);
try {
	headerFuture.join();
} catch (Exception ex) {
	......
}
System.out.println("all done. ");
```

# 数据结构

### DFS、BFS、深度优先遍历、广度优先遍历

**深度优先遍历**从某个顶点出发，首先访问这个顶点，然后找出刚访问这个结点的第一个未被访问的邻结点，然后再以此邻结点为顶点，继续找它的下一个新的顶点进行访问，重复此步骤，直到所有结点都被访问完为止。

**广度优先遍历**从某个顶点出发，首先访问这个顶点，然后找出这个结点的所有未被访问的邻接点，访问完后再访问这些结点中第一个邻接点的所有结点，重复此方法，直到所有结点都被访问完为止。

# 计算机网络

## OSI 七层模型

**OSI 七层模型** 是国际标准化组织提出一个网络分层模型，其大体结构以及每一层提供的功能如下图所示：

![osi七层模型](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/osi%E4%B8%83%E5%B1%82%E6%A8%A1%E5%9E%8B.png)

每一层都专注做一件事情，并且每一层都需要使用下一层提供的功能比如传输层需要使用网络层提供的路由和寻址功能，这样传输层才知道把数据传输到哪里去。

**OSI 的七层体系结构概念清楚，理论也很完整，但是它比较复杂而且不实用，而且有些功能在多个层中重复出现。**

上面这种图可能比较抽象，再来一个比较生动的图片。下面这个图片是我在国外的一个网站上看到的，非常赞！

![osi七层模型2](https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/github/javaguide/osi%E4%B8%83%E5%B1%82%E6%A8%A1%E5%9E%8B2.png)

## TCP/IP 四层模型

**TCP/IP 四层模型** 是目前被广泛采用的一种模型,我们可以将 TCP / IP 模型看作是 OSI 七层模型的精简版本，由以下 4 层组成：

1. 应用层
2. 传输层
3. 网络层
4. 网络接口层

## **DNS的工作流程**

**主机向本地域名服务器的查询一般是采用递归查询，而本地域名服务器向根域名的查询一般是采用迭代查询。**

> 递归查询主机向本地域名发送查询请求报文，而本地域名服务器不知道该域名对应的IP地址时，本地域名会继续向根域名发送查询请求报文，不是通知主机自己向根域名发送查询请求报文。迭代查询是，本地域名服务器向根域名发出查询请求报文后，根域名不会继续向顶级域名服务器发送查询请求报文，而是通知本地域名服务器向顶级域名发送查询请求报文。
>
> 简单来说，递归查询就是，小明问了小红一个问题，小红不知道，但小红是个热心肠，小红就去问小王了，小王把答案告诉小红后，小红又去把答案告诉了小明。迭代查询就是，小明问了小红一个问题，小红也不知道，然后小红让小明去问小王，小明又去问小王了，小王把答案告诉了小明。

1. 在浏览器中输入www.baidu.com域名，操作系统会先检查自己本地的hosts文件是否有这个域名的映射关系，如果有，就先调用这个IP地址映射，完成域名解析。
2. 如果hosts文件中没有，则查询本地DNS解析器缓存，如果有，则完成地址解析。 
3. 如果**本地DNS解析器缓存中没有，则去查找本地DNS服务器**，如果查到，完成解析。 
4. 如果没有，则**本地服务器会向根域名服务器发起查询请求**。根域名服务器会告诉本地域名服务器**去查询哪个顶级域名服务器。** 
5. 本地域名服务器向顶级域名服务器发起查询请求，**顶级域名服务器会告诉本地域名服务器去查找哪个权限域名服务器。** 
6. 本地域名服务器向权限域名服务器发起查询请求，权限域名服务器告诉本地域名服务器www.baidu.com所对应的IP地址。
7. 本地域名服务器告诉主机www.baidu.com所对应的IP地址。

## ARP的工作流程

链路层协议，地址解析协议

1. 在局域网内，主机A要向主机B发送IP数据报时，首先会在主机A的ARP缓存表中查找是否有主机B的IP地址及其对应的MAC地址，如果有，直接将数据包发到这个MAC地址。 
2. 如果主机A的ARP缓存表中没有主机B的IP地址及所对应的MAC地址，主机A会在局域网内**广播发送**一个ARP请求分组。局域网内的所有主机都会收到这个ARP请求分组。 
3. 主机B在看到主机A发送的ARP请求分组中有自己的IP地址，会像主机A以单播的方式发送一个带有自己MAC地址的响应分组。 
4. 主机A收到主机B的ARP响应分组后，会在ARP缓存表中写入主机B的IP地址及其IP地址对应的MAC地址。 
5. 如果主机A和主机B不在同一个局域网内，即使知道主机B的MAC地址也是不能直接通信的，必须通过路由器转发到主机B的局域网才可以通过主机B的MAC地址找到主机B。并且主机A和主机B已经可以通信的情况下，主机A的ARP缓存表中存放的并不是主机B的IP地址及主机B的MAC地址，而是主机B的IP地址及该通信链路上的下一跳路由器的MAC地址。这就是上图中的源IP地址和目的IP地址一直不变，而MAC地址却随着链路的不同而改变。 
6. 如果主机A和主机B不在同一个局域网，参考上图中的主机H1和主机H2，这时主机H1需要先广播找到路由器R1的MAC地址，再由R1广播找到路由器R2的MAC地址，最后R2广播找到主机H2的MAC地址，建立起通信链路。 

## TCP与UDP

| 是否面向连接 | 可靠性   | 传输形式 | 传输效率   | 消耗资源 | 应用场景 | 首部字节      |       |
| ------------ | -------- | -------- | ---------- | -------- | -------- | ------------- | ----- |
| TCP          | 面向连接 | 可靠     | 字节流     | 慢       | 多       | 文件/邮件传输 | 20~60 |
| UDP          | 无连接   | 不可靠   | 数据报文段 | 快       | 少       | 视频/语音传输 | 8     |

> 有时候面试还会问到TCP的首部都包含什么

- TCP首部(图片来源于网络)：

  前20个字节是固定的，后面有4n个字节是根据需而增加的选项，所以TCP首部最小长度为20字节。

  ![图片说明](https://uploadfiles.nowcoder.com/images/20210914/975641190_1631577730779/1ABB2DC3D76311944FFDBE9980FBAADD) 

- UDP首部(图片来源于网络)：

  UDP的首部只有8个字节，**源端口号、目的端口号、长度和校验和**各两个字节。

  ![图片说明](https://uploadfiles.nowcoder.com/images/20210914/975641190_1631577759129/A6016270CE2083EF7C421A20AB3C24F7) 



### TCP协议如何保证可靠传输

> 主要有校验和、序列号、超时重传、流量控制及拥塞避免等几种方法。

- 校验和：在发送算和接收端分别计算数据的校验和，如果两者不一致，则说明数据在传输过程中出现了差错，TCP将丢弃和不确认此报文段。

- 序列号：TCP会对每一个发送的字节进行编号，接收方接到数据后，会对发送方发送确认应答(ACK报文)，并且这个ACK报文中带有相应的确认编号，告诉发送方，下一次发送的数据从编号多少开始发。如果发送方发送相同的数据，接收端也可以通过序列号判断出，直接将数据丢弃。

  ![图片说明](https://uploadfiles.nowcoder.com/images/20210914/975641190_1631577777393/FCB97D0A1FC0EAE2253E40F922B2BBC1) 

- 超时重传：在上面说了序列号的作用，但如果发送方在发送数据后一段时间内（可以设置重传计时器规定这段时间）没有收到确认序号ACK，那么发送方就会重新发送数据。

  这里发送方没有收到ACK可以分两种情况，如果是发送方发送的数据包丢失了，接收方收到发送方重新发送的数据包后会马上给发送方发送ACK；如果是接收方之前接收到了发送方发送的数据包，而返回给发送方的ACK丢失了，这种情况，发送方重传后，接收方会直接丢弃发送方冲重传的数据包，然后再次发送ACK响应报文。

  如果数据被重发之后还是没有收到接收方的确认应答，则进行再次发送。此时，等待确认应答的时间将会以2倍、4倍的指数函数延长，直到最后关闭连接。

  #### ARQ(自动重传)协议

  - 停止-等待ARQ

  基本原理就是每发完一个分组就停止发送，等待对方确认（回复 ACK）。如果过了一段时间（超时时间后），还是没有收到 ACK 确认，说明没有发送成功，需要重新发送

  - 连续ARQ

  发送方维持一个发送窗口，凡位于发送窗口内的分组可以连续发送出去，而不需要等待对方确认。接收方一般采用累积确认，对按序到达的最后一个分组发送确认，表明到这个分组为止的所有分组都已经正确收到了。
  缺点:若第N个分组丢失，需要回退N步。

  #### 流量控制：如果发送端发送的数据太快，接收端来不及接收就会出现丢包问题。为了解决这个问题，TCP协议利用了滑动窗口进行了流量控制。在TCP首部有一个16位字段大小的窗口，窗口的大小就是接收端接收数据缓冲区的剩余大小。接收端会在收到数据包后发送ACK报文时，将自己的窗口大小填入ACK中，发送方会根据ACK报文中的窗口大小进而控制发送速度。如果窗口大小为零，发送方会停止发送数据。

  #### 拥塞控制：如果网络出现拥塞，则会产生丢包等问题，这时发送方会将丢失的数据包继续重传，网络拥塞会更加严重，所以在网络出现拥塞时应注意控制发送方的发送数据，降低整个网络的拥塞程度。拥塞控制主要有四部分组成：慢开始、拥塞避免、快重传、快恢复，如下图(图片来源于网络)。

  ![图片说明](https://uploadfiles.nowcoder.com/images/20210914/975641190_1631577903764/4960906A9EF2ACC29E7A589BB21904A8) 这里的发送方会维护一个拥塞窗口的状态变量，它和流量控制的滑动窗口是不一样的，滑动窗口是根据接收方数据缓冲区大小确定的，而拥塞窗口是根据网络的拥塞情况动态确定的，一般来说发送方真实的发送窗口为滑动窗口和拥塞窗口中的最小值。

  1. 慢开始：为了避免一开始发送大量的数据而产生网络阻塞，会先初始化cwnd为1，当收到ACK后到下一个传输轮次，cwnd为2，以此类推成指数形式增长。

  2. 拥塞避免：因为cwnd的数量在慢开始是指数增长的，为了防止cwnd数量过大而导致网络阻塞，会**设置一个慢开始的门限值ssthresh**，当cwnd>=ssthresh时，进入到拥塞避免阶段，cwnd每个传输轮次加1。但网络出现超时，会将门限值ssthresh变为出现超时cwnd数值的一半，cwnd重新设置为1，如上图，在第12轮出现超时后，cwnd变为1，ssthresh变为12。

  3. 快重传：发送发收到三个连续的重复确认，就将相应报文段立即重传，不等计时器超时再重传。。
     
  4. 快恢复：从上上图圈4可以看到，当发送收到三个重复的ACK，会进行快重传和快恢复。快恢复是指将ssthresh设置为发生快重传时的cwnd数量的一半，而cwnd不是设置为1而是设置为为门限值ssthresh，你并开始拥塞避免阶段。

### TCP的三次握手及四次挥手　＊＊＊

> 必考题

在介绍三次握手和四次挥手之前，先介绍一下TCP头部的一些常用字段。

- 序号：seq，占32位，用来标识从发送端到接收端发送的字节流。 
- 确认号：ack，占32位，只有ACK标志位为1时，确认序号字段才有效，ack=seq+1。 
- 标志位：  
  - SYN：发起一个新连接。 
  - FIN：释放一个连接。 
  - ACK：确认序号有效。 

#### 三次握手

过程:
一次握手:客户端发送带有SYN标志的数据包至服务器
二次握手:服务端发送带有SYN/ACK标志的数据包
三次握手:客户端发送带有ACK标志的数据包



****

#### 四次挥手

过程
客户端-发送一个 FIN，用来关闭客户端到服务器的数据传送
服务器-收到这个 FIN，它发回一 个 ACK，此时客户端不再发送数据，但服务器端还可能有数据要发送至客户端。 服务器-客户端数据发送完毕后，关闭与客户端的连接，发送一个 FIN 给客户端
客户端-发回 ACK 报文确认，并将确认序号设置为收到序号加 1

#### 为什么TCP连接的时候是3次？两次是否可以？

**三次握手确认收发双方发送接收功能正常，第一次客户端什么都不能确认，服务器确认客户端发送正常，自己接收正常，第二次客户端确认自己发送接收正常，服务器发送接收正常，服务器仍只确认客户端发送正常，自己接收正常；第三次服务器确认客户端发送接收正常，自己发送接收正常**

不可以，主要从以下两方面考虑（假设客户端是首先发起连接请求）：

1. 假设建立TCP连接仅需要两次握手，那么如果第二次握手时，服务端返回给客户端的确认报文丢失了，客户端这边认为服务端没有和他建立连接，而服务端却以为已经和客户端建立了连接，并且可能向服务端已经开始向客户端发送数据，但客户端并不会接收这些数据，浪费了资源。如果是三次握手，不会出现双方连接还未完全建立成功就开始发送数据的情况。 
2. 如果服务端接收到了一个早已失效的来自客户端的连接请求报文，会向客户端发送确认报文同意建立TCP连接。但因为客户端并不需要向服务端发送数据，所以此次TCP连接没有意义并且浪费了资源。 

#### 为什么TCP连接的时候是3次，关闭的时候却是4次？

因为需要确保通信双方都能通知对方释放连接，假设客服端发送完数据向服务端发送释放连接请求，服务端返回给客户端确认报文后，服务端向客户端发送的数据还没有发送完。当服务端发送完数据后还需要向客户端发送释放连接请求，客户端返回确认报文，TCP连接彻底关闭。所以断开TCP连接需要客户端和服务端分别通知对方并分别收到确认报文，一共需要四次。

#### TIME_WAIT和CLOSE_WAIT的区别在哪?

TIME_WAIT 是**主动关闭链接**时形成的，等待2MSL时间，约4分钟。主要是防止最后一个ACK丢失。  由于TIME_WAIT 的时间会非常长，因此server端应尽量减少主动关闭连接

CLOSE_WAIT是**被动关闭连接**形成的。服务器端收到客户端发送的FIN，发送ACK后进入CLOSE_WAIT状态。

#### 为什么客户端发出第四次挥手的确认报文后要等2MSL的时间才能释放TCP连接？

MSL的意思是报文的最长寿命，可以从两方面考虑：

1. 客户端发送第四次挥手中的报文后，再经过2MSL，可使本次TCP连接中的所有报文全部消失，不会出现在下一个TCP连接中。 
2. 考虑丢包问题，如果**第四挥手发送的报文在传输过程中丢失了**，那么服务端没收到确认ack报文就会重发第三次挥手的报文。如果客户端发送完第四次挥手的确认报文后直接关闭，而这次报文又恰好丢失，则会**造成服务端无法正常关闭**。 

#### 如果已经建立了连接，但是客户端突然出现故障了怎么办？

如果TCP连接已经建立，在通信过程中，客户端突然故障，那么服务端不会一直等下去，过一段时间就关闭连接了。具体原理是TCP有一个保活机制，主要用在服务器端，用于检测已建立TCP链接的客户端的状态，防止因客户端崩溃或者客户端网络不可达，而服务器端一直保持该TCP链接，占用服务器端的大量资源(因为Linux系统中可以创建的总TCP链接数是有限制的)。

保活机制原理：设置TCP保活机制的保活时间[keep]()Idle，即在TCP链接超过该时间没有任何数据交互时，发送保活探测报文；设置保活探测报文的发送时间间隔[keep]()Interval；设置保活探测报文的总发送次数[keep]()Count。如果在[keep]()Count次的保活探测报文均没有收到客户端的回应，则服务器端即关闭与客户端的TCP链接。

具体细节请看这篇博客[TCP通信过程中异常情况整理](https://blog.csdn.net/yyc1023/article/details/80242815)。

#### TCP长连接短连接

TCP协议中有长连接和短连接之分。短连接环境下,**数据交互完毕后,主动释放连接**; 长连接的环境下,**进行一次数据交互后,很长一段时间内即使无数据交互时,它会一直维护这个连接**，缺点：客户端可能意外断电、死机、崩溃、重启,还是中间路由网络无故断开,这些TCP连接并未来得及正常释放,那么,连接的另一方并不知道对端的情况。

### HTTP 与 HTTPS 的区别

|              | HTTP               | HTTPS                                                   |
| ------------ | ------------------ | ------------------------------------------------------- |
| 端口         | 80                 | 443                                                     |
| 安全性       | 无加密，安全性较差 | 有加密机制，安全性较高                                  |
| 资源消耗     | 较少               | 由于加密处理，资源消耗更多                              |
| 是否需要证书 | 不需要             | 需要                                                    |
| 协议         | 运行在TCP协议之上  | 运行在SSL(安全套接字协议)协议之上，SSL运行在TCP协议之上 |

#### SSL/TLS

因为HTTP在传输数据时使用的是明文（虽然说POST提交的数据时放在报体里看不到的，但是还是可以通过抓包工具窃取到）是不安全的，HTTPS协议使用SSL在发送方把原始数据进行加密，然后在接受方进行解密，加密和解密需要发送方和接受方通过交换共知的密钥来实现。

所处位置：SSL协议位于TCP/IP协议与各种应用层协议之间，SSL协议可分为两层：SSL记录协议，SSL握手协议

SSL握手协议（SSL Handshake Protocol）：它建立在SSL记录协议之上，用于在实际的数据传输开始前，通讯双方进行身份认证、协商加密算法、交换加密密钥等。SSL记录协议（SSL Record Protocol）：它建立在可靠的传输协议（如TCP）之上，为高层协议提供数据封装、压缩、加密等基本功能的支持。

**TLS握手过程：**

（先进行TCP三次握手）客户端给客户端发送请求，服务端发送**证书（CA数字签名产生）和自己的公钥至客户端**，客户端生成一个预主密钥用服务端公钥加密发送至服务端，服务端**使用私钥解密后**再和客户端利用先前交换得到的随机数和预主密钥**生成主密钥**，至此是非对称加密阶段，接下来用生成的主密钥进行对称加密通信。

注：证书由CA第三方数字签名

1. 设有服务器 S，客户端 C，和第三方信赖机构 CA。
2. 服务器信任 CA，**CA 是知道 服务器公钥的**，CA 向服务器颁发证书。并附上 **CA 私钥对消息摘要的加密签名**。
3. 服务器获得 CA 颁发的证书，将该证书传递给 C。
4. 客户端 获得 服务器 的证书，信任 CA 并知晓 CA 公钥，使用 **CA 公钥**对 服务器 证书的签名解密，同时对消息进行散列处理，得到摘要。比较摘要，验证 S 证书的真实性。
5. 如果 C 验证 S 证书是真实的，则信任 S 的公钥（在 S 证书中）。





### GET和POST区别

- 作用

  GET用于获取资源，POST用于传输实体主体

- 参数位置

  **GET的参数放在URL中**，POST的参数存储在实体主体中，并且GET方法提交的**请求的URL中的数据做多是2048字节**，POST请求没有大小限制。

- 安全性

  GET方法因为参数放在URL中，安全性相对于POST较差一些

- 幂等性

  GET方法是具有幂等性的，而POST方法不具有幂等性。这里幂等性指客户端连续发出多次请求，收到的结果都是一样的.

### HTTP 1.0、HTTP 1.1及HTTP 2.0的主要区别是什么

HTTP 1.0和HTTP 1.1的区别

- **长连接**

  HTTP 1.1支持长连接和请求的流水线操作。长连接是指不在需要每次请求都重新建立一次连接，HTTP 1.0默认使用短连接，每次请求都要重新建立一次TCP连接，资源消耗较大。请求的流水线操作是指客户端在收到HTTP的响应报文之前可以先发送新的请求报文，不支持请求的流水线操作需要等到收到HTTP的响应报文后才能继续发送新的请求报文。

- **缓存处理**

  **强缓存**：从本地缓存里面找，找到了就判断缓存是否过期。

  http1.0使用expires指定资源到期的时间，是服务器端的具体的时间点。受限于本地时间，如果修改了本地时间，可能会造成缓存失效。 http1.1使用Cache-Control控制网页缓存(不是具体时间相对时间，倒计时)。

  

  **对比缓存**：每次都会进行网络请求，只不过服务器会根据他的字段的数据判断他是否需要更新数据

  在HTTP 1.0中主要使用header中的**Last-Modified/If-Modified-Since**
  Last-Modified：服务器在响应请求时，告诉浏览器资源的最后修改时间。

  If-Modified-Since:服务器收到请求后发现有头If-Modified-Since 则与被请求资源的最后修改时间进行比对。If-Modified-Since中存放的是上一次获得该资源时，响应体中的Last-Modified的值。 若资源的最后修改时间大于If-Modified-Since，说明资源又被改动过，则响应整片资源内容，返回状态码200； 若资源的最后修改时间小于或等于If-Modified-Since，说明资源无新修改，则响应HTTP 304，告知浏览器继续使用所保存的cache。

  

  HTTP 1.1引入了**Entity tag，If-None-Match**等更多可供选择的缓存头来控制缓存策略。

  Etag：服务器响应请求时，告诉浏览器当前资源在服务器的唯一标识
  If-None-Match：服务器收到请求后发现有头If-None-Match 则与被请求资源的唯一标识进行比对， 不同，说明资源又被改动过，则响应整片资源内容，返回状态码200； 相同，说明资源无新修改，则响应HTTP 304，告知浏览器继续使用所保存的cache。

- 错误状态码

  在HTTP 1.1新增了24个错误状态响应码

- HOST域

  在HTTP 1.0 中认为每台服务器都会绑定唯一的IP地址，所以，请求中的URL并没有传递主机名。但后来一台服务器上可能存在多个虚拟机，它们共享一个IP地址，所以HTTP 1.1中请求消息和响应消息都应该支持Host域。

- **带宽优化及网络连接的使用**

  在HTTP 1.0中会存在浪费带宽的现象，主要是因为不支持断点续传功能，客户端只是需要某个对象的一部分，服务端却将整个对象都传了过来。**在HTTP 1.1中请求头引入了range头域，它支持只请求资源的某个部分**，返回的状态码为206。

HTTP 2.0的新特性

- 在**应用层和传输层**之间增加一个二进制分帧层。在二进制分帧层上，HTTP 2.0 会将所有传输的信息分割为更小的消息和帧,并对它们采用二进制格式的编码 ，其中HTTP1.x的首部信息会被封装到Headers帧，而我们的request body则封装到Data帧里面。
- 多路复用：HTTP 1.0的模式是，短连接，建立连接请求数据完毕之后就立即关闭连接；**后来1.x是长连接**，采用了keep-alive保活模式使得可以复用连接不断开，可以利用这次连接继续请求数据。但是始终会有一个缺点，**就是你必须等待服务器返回上一次的请求数据你才可以进行下一次的请求**。多路复用具体实现每一个request对应一个id，一个连接上可以有多个request，每个连接的request可以随机混在一起，这样接收方可以根据request的id将request归属到各自不同的服务端请求里。 
- header压缩：在HTTP 1.x中，header携带大量信息，并且每次都需要重新发送，HTTP 2.0采用编码的方式减小了header的大小，同时通信双方各自缓存一份header fields表，避免了header的重复传输。 
- 服务端推送：客户端在请求一个资源时，会把相关资源一起发给客户端，这样客户端就不需要再次发起请求。 

### Session、Cookie和Token的主要区别　＊＊＊

HTTP协议是无状态的，即服务器无法判断用户身份。Session和Cookie可以用来进行身份辨认。

- Cookie

  Cookie是保存在客户端一个小数据块，其中包含了用户信息。当客户端向服务端发起请求，服务端会像客户端浏览器发送一个Cookie，客户端会把Cookie存起来，当下次客户端再次请求服务端时，会携带上这个Cookie，服务端会通过这个Cookie来确定身份。

- Session

  Session是通过Cookie实现的，和Cookie不同的是，Session是存在服务端，负责记录用户的状态。 典型的场景是购物车，当你要添加商品到购物车的时候，系统不知道是哪个用户操作的，因为 HTTP 协议是无状态的。**服务端给特定的用户创建特定的 Session 之后就可以标识这个用户并且跟踪这个用户了。**

- Token

  客户端在浏览器第一次访问服务端时，服务端生成的一串字符串作为Token发给客户端浏览器，下次浏览器在访问服务端时携带token即可无需验证用户名和密码，token无需存放在服务器

  | 存放位置 | 占用空间     | 安全性 | 应用场景 |                    |
  | -------- | ------------ | ------ | -------- | ------------------ |
  | Cookie   | 客户端浏览器 | 小     | 较低     | 一般存放配置信息   |
  | Session  | 服务端       | 多     | 较高     | 存放较为重要的信息 |

[注意！JWT不是万能的，入坑需谨慎！ - 云+社区 - 腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/1536885)



### 如果客户端禁止 cookie 能实现 session 还能用吗？

可以，Session的作用是在服务端来保持状态，通过session_id来进行确认身份，但session_id一般是通过Cookie来进行传递的。如果Cooike被禁用了，可以通过在URL中传递session_id。



### 在浏览器中输⼊url地址到显示主页的过程

> | 过程                       | 协议                                                  |
> | -------------------------- | ----------------------------------------------------- |
> | 浏览器查找域名IP           | DNS                                                   |
> | 浏览器向服务器发送HTTP请求 | TCP、IP、OSPF(路由选择)、ARP(ip地址转为mac地址)、HTTP |
> | 服务器处理请求，生成HTML   |                                                       |
> | 服务器发送HTML             |                                                       |
> | 浏览器浏览，关闭连接       |                                                       |

## ICMP

ICMP协议是一种面向无连接的协议，它属于**网络层**协议，主要用于在**主机与路由器之间传递控制信息**，包括报告错误、交换受限控制和状态信息等。

**ICMP协议大致可分为两类**：

- 查询报文类型：ping查询、子网掩码查询、时间戳查询
- 差错报文类型：
  差错报文主要产生于当数据传送发送错误的时候。
  它包括：目标不可达（网络不可达、主机不可达、协议不可达、端口不可达、禁止分片等）、超时、参数问题、重定向（网络重定向、主机重定向等）等等。
  差错报文通常包含了引起错误的IP数据包的第一个分片的IP首部，加上该分片数据部分的前8个字节。
  当传送IP数据包发生错误的时候（例如 主机不可达），ICMP协议就会把错误信息封包，然后传送回源主机，那么源主机就知道该怎么处理了。

### Ping过程

它只是利用 ICMP 回显请求和回显应答报文，而不用经过传输层（TCP/UDP）。原理是：利用网络上机器IP地址的唯一性，给目标IP地址发送一个ICMP echo@数据包，并等待接收echo回应数据包。通过对方回复的数据包来确定两台网络机器是否连接相通，时延是多少。

（1）假设有两个主机，主机A（192.168.0.1）和主机B（192.168.0.2），现在我们要监测主机A和主机B之间网络是否可达，那么我们在主机A上输入命令：ping 192.168.0.2
（2）此时，ping命令会在主机A上构建一个 ICMP的请求数据包，然后 ICMP协议会将这个数据包以及目标IP（192.168.0.2）等信息一同交给IP层协议。
（3）IP层协议得到这些信息后，将源地址（即本机IP）、目标地址（即目标IP：192.168.0.2）、再加上一些其它的控制信息，构建成一个IP数据包。
（4）IP数据包构建完成后，还不够，还需要加上MAC地址，因此，还需要通过ARP映射表找出目标IP所对应的MAC地址。当拿到了目标主机的MAC地址和本机MAC后，一并交给数据链路层，组装成一个数据帧，依据以太网的介质访问规则，将它们传送出出去。
（5）当主机B收到这个数据帧之后，会首先检查它的目标MAC地址是不是本机，如果是就接收下来处理，接收之后会检查这个数据帧，将数据帧中的IP数据包取出来，交给本机的IP层协议，然后IP层协议检查完之后，再将ICMP数据包取出来交给ICMP协议处理，当这一步也处理完成之后，就会构建一个ICMP应答数据包，回发给主机A
（6）在一定的时间内，如果主机A收到了应答包，则说明它与主机B之间网络可达，如果没有收到，则说明网络不可达。除了监测是否可达以外，还可以利用应答时间和发起时间之间的差值，计算出数据包的延迟耗时。

#  操作系统

## 1 什么是操作系统，系统调用

**1.1 什么是操作系统**
操作系统是一个运行在计算机上的软件程序，用于管理计算机软件与硬件资源的程序，屏蔽了硬件层的复杂性，操作系统的内核是操作系统的核心部分，负责系统内存管理，硬件设备管理，文件管理，应用程序管理。
**1.2 什么是系统调用**
大多数我们运行的程序在用户态，当需要调用系统态级别的功能时，需要系统调用。  
用户态:用户态运行的进程可以直接读取用户程序的数据。  
系统态:系统态运行的进程或程序几乎可以访问计算机的任何资源。  

按功能可分为:设备管理，文件管理，内存管理，进程通信，进程控制

## 2 进程线程

#### 2.1 进程线程的区别

进程是计算机程序的一次运行活动，是系统进行资源分配和调度的基本单位。
一个进程可以产生多个线程，一个进程中可以有多个线程，多个线程共享进程的堆和方法区 (JDK1.8之后的元空间)资源，但是每个线程有自己的程序计数器、虚拟机栈 和 本地方法栈。
各进程间是独立的，线程之间共享所在进程的资源，不是相互独立的。线程执行开销小，但不利于维护，进程与之相反。



**区别：**
a）进程是资源分配的最小单位，线程是任务执行的最小单位。
b）进程有自己的独立地址空间，每启动一个进程，系统就会为它分配地址空间，建立数据表来维护代码段、堆栈段和数据段，这种操作非常昂贵。而**线程是共享进程中的堆和方法区资源，因此 CPU 切换一个线程的花费远比进程要小很多**，同时创建一个线程的开销也比进程要小很多。线程执行开销小，但不利于资源的管理和保护；而进程正相反。
c）**线程之间的通信更方便**，同一进程下的线程共享全局变量、静态变量等数据，而进程之间的通信需要以通信的方式（IPC)进行。不过如何处理好同步与互斥是编写多线程程序的难点。
d）线程和进程最大的不同在于基本上各**进程是独立的，而各线程则不一定**，因为同一进程中的线程极有可能会相互影响。但是多进程程序更健壮，多线程程序只要有一个线程死掉，整个进程也死掉了，而一个进程死掉并不会对另外一个进程造成影响，因为进程有自己独立的地址空间。


**操作系统如何实现线程？**

1. 内核实现线程
2. 用户实现线程
3. 混合实现


**为什么要区分用户态和内核态？他们直接是如何转换的？**
用户态(user mode) : 用户态运行的进程或可以直接读取用户程序的数据。
内核态(kernel mode):可以简单的理解系统态运行的进程或程序几乎可以访问计算机的任何资源，不受限制。

为了系统调用的安全性考虑，内核态的操作具有最高的访问控制级别，直接由用户操作可能影响硬件资源安全性。

用户态切换到内核态方式：
系统调用: 系统调用是用户态主动要求切换到内核态的一种方式， 用户应用程序通过操作系统调用内核为上层应用程序开放的接口来执行程序。
异常：当 cpu 在执行用户态的应用程序时，发生了某些不可知的异常。 于是当前用户态的应用进程切换到处理此异常的内核的程序中去。
硬件设备的中断: 当硬件设备完成用户请求后，会向 cpu 发出相应的中断信号，这时 cpu 会暂停执行下一条即将要执行的指令，转而去执行与中断信号对应的应用程序， 如果先前执行的指令是用户态下程序的指令，那么这个转换过程也是用户态到内核态的转换。


#### 2.2 进程状态

创建状态(new) ：进程正在被创建，尚未到就绪状态。
就绪状态(ready) ：进程已处于准备运行状态，即进程获得了除了处理器之外的一切所需资源，一旦得到处理器资源(处理器分配的时间片)即可运行。
运行状态(running) ：进程正在处理器上上运行(单核 CPU 下任意时刻只有一个进程处于运行状态)。
阻塞状态(waiting) ：又称为等待状态，进程正在等待某一事件而暂停运行如等待某资源为可用或等待 IO 操作完成。即使处理器空闲，该进程也不能运行。
结束状态(terminated) ：进程正在从系统中消失。可能是进程正常结束或其他原因中断退出运行

#### 2.3 进程通信

IPC指进程间通信。通信是进程间传递信息的方式，同步是定义进程执行的先后顺序

1. 管道：需要先建立连接，半双工通信，FIFO，存在于内存中。
2. 命名管道：需要先建立连接，半双工通信，FIFO，通过命名可以访问非亲缘进程，存在于磁盘或文件系统。
3. 信号量：信号量是一个计数器，用于多进程对共享数据的访问，信号量的意图在于进程间同步。这种通信方式主要用于解决与同步相关的问题并避免竞争条件。
4. 消息队列：存放在内核中的共享消息链表，信息需要占用CPU时间进行复制。
5. 共享内存：使得多个进程可以访问同一块内存空间，不同进程可以及时看到对方进程中对共享内存中数据的更新。这种方式需要依靠某种同步操作，如互斥锁和信号量等。
6. 套接字：主要用于在客户端和服务器之间通过网络进行通信。套接字是支持 TCP/IP 的网络通信的基本操作单元，可以看做是不同主机之间的进程进行双向通信的端点，简单的说就是通信的两方的一种约定，用套接字中的相关函数来完成通信过程。

#### 2.4 线程同步

- 互斥量:采用互斥对象机制，只有拥有互斥对象的线程才有访问公共资源的权限
- 信号量:它允许同一时刻多个线程访问同一资源，但是需要控制同一时刻访问此资源的最大线程数量
- 事件:通过通知操作的方式来保持多线程同步。

#### 2.5 进程调度算法

- 先到先服务FCFS:从就绪队列中选择先到队列的进程执行。  
- 短作业优先:选择就绪队列中估计运行时间最短的进程先执行，忽略的长进程。
- 时间片轮转调度：每个进程被分配一个时间段
- 多级反馈队列：既能使高优先级的作业得到响应又能使短作业（进程）迅速完成。
- 优先级调度:为每个进程分配优先级，优先级最高的进程先执行

#### 2.6 死锁

多个进程可以竞争有限数量的资源。多个进程相互申请对方所持有的资源，形成一条循环等待链，陷入阻塞，形成死锁。  
**条件**  
互斥:资源只能互斥访问,即一次只有一个进程可以使用。如果另一进程申请该资源，那么必须等待直到该资源被释放为止。
保持等待:申请需要资源的同时不释放所持有的资源  
非抢占:资源无法被抢占,只能持有资源的进程完成任务后，资源才会被释放。 
循环等待:有一组等待进程 {P0, P1,..., Pn}， P0 等待的资源被 P1 占有，P1 等待的资源被 P2 占有，......，Pn-1 等待的资源被 Pn 占有，Pn 等待的资源被 P0 占有。

**解决死锁的方法**
解决死锁的方法可以从多个角度去分析，一般的情况下，有预防，避免，检测和解除四种。

- **预防** 
  1.破坏互斥条件：资源可以同时访问，但有很多资源往往是不能同时访问的，限制比较大。
  2.破坏死锁产生的第二个条件（占有并等待）：静态分配策略，一个进程必须在执行前就申请到它所需要的全部资源，并且知道它所要的资源都得到满足之后才开始执行。
  3.破坏第三个条件 非抢占 ：申请的资源得不到满足时则立即释放当前所拥有的全部资源，也就是说可以采用 剥夺式调度算法，但剥夺式调度方法目前一般仅适用于 主存资源 和 处理器资源 的分配，并不适用于所以的资源，会导致资源利用率下降。
  4.破坏了产生死锁的第四个条件(循环等待)：层次分配策略，所有的资源被分成了多个层次，一个进程得到某一次的一个资源后，它只能再申请较高一层的资源；当一个进程要释放某层的一个资源时，必须先释放所占用的较高层的资源。

- **避免** 系统在分配资源时，根据资源的使用情况提前做出预测，使进程都达到一个安全状态，从而避免死锁的发生。   **银行家算法**给每个进程分配资源。
- **检测**是指系统设有专门的机构，当死锁发生时，该机构能够检测死锁的发生，并精确地确定与死锁有关的进程和资源。  进程资源分配图检测最终哪些进程会发生死锁。
- 解除 是与检测相配套的一种措施，用于将进程从死锁状态下解脱出来。
  1.立即结束所有进程的执行，重新启动操作系统
  2.撤销涉及死锁的所有进程，解除死锁后继续运行
  3.逐个撤销涉及死锁的进程，回收其资源直至死锁解除。
  4.抢占资源 ：从涉及死锁的一个或几个进程中抢占资源，把夺得的资源再分配给涉及死锁的进程直至死锁解除。

**Java中如何避免死锁**？

1. 读写分离（copyonwrite）、减小锁粒度：弱化互斥条件
2. 加锁时限：破坏不剥夺条件
3. 顺序加锁：破坏循环等待
4. 死锁监测：ThreadMXBean实现死锁监测

#### 2.7 协程

**线程进程都是同步机制，而协程则是异步。**
协程(协同程序): 同一时间只能执行某个协程。开辟多个协程开销不大。协程适合对某任务进行分时处理。
线程: 同一时间可以同时执行多个线程。开辟多条线程开销很大。线程适合多任务同时处理。

区别：
（1）协程既不是进程也不是线程，协程仅仅是一个特殊的函数，协程它进程和进程不是一个维度的。
（2）对操作系统而言，线程是最小的执行单元，进程是最小的资源管理单元。无论是进程还是线程，都是由操作系统所管理的。**协程不是被操作系统内核所管理的，而是完全由程序所控制，也就是在用户态执行。**
（3）协程是一种用户态的轻量级线程，协程的调度完全由用户控制。**协程拥有自己的寄存器上下文和栈，局部变量。但也与其他协同程序共享全局变量等信息。**协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈，直接操作栈则基本没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快。
（4）线程是抢占式，而协程是非抢占式的，所以需要用户自己释放使用权来切换到其他协程，因此同一时间其实只有一个协程拥有运行权，相当于单线程的能力。



## 3 内存管理

#### 3.1 内存管理介绍

负责内存的分配与回收，逻辑地址转化为物理地址。

#### 3.2 内存管理机制（块、页、段）

- 连续分配管理:为一个用户程序分配一个连续的内存空间。  
  **块式管理**:将内存分为固定大小的几个块，每个块只包含一个进程。很容易造成浪费。  
- 非连续分配管理:允许为用户程序分配离散的内存空间  
  **页式管理**:主存分为大小相等且固定的一页一页的形式，页较小，提高内存利用率。页式管理通过页表对应逻辑地址和物理地址。  
  **段式管理**:段式管理把主存分为一段段的，段是有实际意义的，每个段定义了一组逻辑信息，例如,有主程序段 MAIN、子程序段 X、数据段 D 及栈段 S 等。 段式管理通过段表对应逻辑地址和物理地址。  
  **段页式管理**:段页式管理机制结合了段式管理和页式管理的优点。简单来说段页式管理机制就是把主存先分成若干段，每个段又分成若干页。


#### 3.3 分页与分段区别

共同点：均是非连续分配方式，提高了内存利用率。
不同点：页的大小是固定的，段的大小不固定由运行的程序决定。页是物理单位，段是逻辑单位。分段能更好满足用户需求。

#### 3.4 快表与多级页表

快表利用了缓存思想。相当于页表的缓存,把快表理解为一种特殊的高速缓冲存储器（Cache），其中的内容是页表的一部分或者全部内容。  
使用快表后，地址转换过程如下:  
根据虚拟地址中页号首先查快表，如果命中，直接获取相应页的物理地址。如果不在快表中，访问内存中的页表，同时将该记录添加至快表中，当快表满时，按照一定策略淘汰页表中的记录。

多级页表避免把全部页表一直放在内存中占用过多空间，特别是那些根本就不需要的页表就不需要保留在内存中。多级页表属于时间换空间的典型场景(访问两次内存，第一次一级页表，第二次二级页表到物理地址)。



#### 3.5 cpu寻址与虚拟内存空间

- cpu寻址  
  CPU 需要将虚拟地址翻译成物理地址，这样才能访问到真实的物理内存。 实际上完成虚拟地址转换为物理地址转换的硬件是 CPU 中含有一个被称为 内存管理单元（Memory Management Unit, MMU） 的硬件，MMU中存放的是一级页表的基地址(页表中可以存放磁盘上的地址值——虚拟内存技术)。
- 虚拟内存空间  
  如果直接把物理地址暴露出来的话会带来严重问题，比如可能对操作系统造成伤害以及给同时运行多个程序造成困难。  
  程序可以使用一系列相邻的虚拟地址来访问物理内存中不相邻的内存缓冲区。  
  不同进程使用的虚拟地址彼此隔离。一个进程中的代码无法更改正在由另一进程或操作系统使用的物理内存。

#### 3.6 逻辑(虚拟)地址和物理地址

比如在 C 语言中，指针里面存储的数值就可以理解成为内存里的一个地址，这个地址也就是我们说的逻辑地址，逻辑地址由操作系统决定。物理地址指的是真实物理内存中地址，更具体一点来说就是内存地址寄存器中的地址。物理地址是内存单元真正的地址。


## 4 虚拟内存

#### 4.1 什么是虚拟内存

虚拟内存是一种内存管理技术，可以让程序可以拥有超过系统物理内存大小的可用内存空间。另外，虚拟内存为每个进程提供了一个一致的、私有的地址空间，它让每个进程产生了一种自己在独享主存的错觉。
而实际上，它通常是被分隔成多个物理内存碎片，还有一部分存储在外部磁盘存储器上，在需要时进行数据交换。

#### 4.2 局部性原理

时间局部性:如果程序中的某条指令一旦执行，不久以后该指令可能再次执行；如果某数据被访问过，不久以后该数据可能再次被访问。  
空间局部性:一旦程序访问了某个存储单元，在不久之后，其附近的存储单元也将被访问。  

#### 4.3 虚拟内存技术实现

虚拟内存的实现需要建立在离散分配的内存管理方式的基础上。
**请求分页:** 建立在分页管理之上，为了支持虚拟存储器功能而增加了请求调页功能和页面置换功能。在作业开始运行之前，仅装入当前要执行的部分页即可运行。假如在作业运行的过程中发现要访问的页面不在内存，则由处理器通知操作系统按照对应的页面置换算法将相应的页面调入到主存  
**请求分段:** 建立在分段存储管理之上，增加了请求调段功能、分段置换功能。请求分段储存管理方式就如同请求分页储存管理方式一样，在作业开始运行之前，仅装入当前要执行的部分段即可运行；在执行过程中，可使用请求调入中断动态装入要访问但又不在内存的程序段。  
请求段页式

#### 4.4 页面置换算法

OPT 页面置换算法（最佳页面置换算法）：最佳(Optimal, OPT)置换算法所选择的被淘汰页面将是以后永不使用的，或者是在最长时间内不再被访问的页面,这样可以保证获得最低的缺页率。(理想算法，实际无法预测未来页的使用情况)  
FIFO（First In First Out） 页面置换算法（先进先出页面置换算法） : 总是淘汰最先进入内存的页面，即选择在内存中驻留时间最久的页面进行淘汰。  缺点：频繁换入换出，影响效率。
LRU （Least Recently Used）页面置换算法（最近最久未使用页面置换算法） ：LRU算法赋予每个页面一个访问字段，用来记录一个页面自上次被访问以来所经历的时间 T，当须淘汰一个页面时，选择现有页面中其 T 值最大的，即最近最久未使用的页面予以淘汰。  
LFU （Least Frequently Used）页面置换算法（最少使用页面置换算法）: 该置换算法选择在之前时期使用最少的页面作为淘汰页。淘汰频率最小的，使用计数器。

# Linux

## Linux目录

### Linux 的目录结构

Linux 文件系统的结构层次鲜明，就像一棵倒立的树，最顶层是其根目录： ![Linux的目录结构](https://javaguide.cn/assets/Linux%E7%9B%AE%E5%BD%95%E6%A0%91.5c5a8ebb.png)

### 目录说明

- **/bin：** 存放二进制可执行文件(ls、cat、mkdir 等)，常用命令一般都在这里；
- **/sbin:** 存放二进制可执行文件，只有 root 才能访问。这里存放的是系统管理员使用的系统级别的管理命令和程序。如 ifconfig 等；
- **/home：** 存放所有用户文件的根目录，是用户主目录的基点，比如用户 user 的主目录就是/home/user，可以用~user 表示；
- **/usr ：** 用于存放系统应用程序；
- **/opt：** 额外安装的可选应用程序包所放置的位置。一般情况下，我们可以把 tomcat 等都安装到这里；
- **/root：** 超级用户（系统管理员）的主目录（特权阶级^o^）；
- **/dev：** 用于存放设备文件；
- **/mnt：** 系统管理员安装临时文件系统的安装点，系统提供这个目录是让用户临时挂载其他的文件系统；
- **/boot：** 存放用于系统引导时使用的各种文件；
- **/lib ：** 存放着和系统运行相关的库文件 ；
- **/tmp：** 用于存放各种临时文件，是公用的临时文件存储点；
- **/etc：** 存放系统管理和配置文件；
- **/proc：** 虚拟文件系统目录，是系统内存的映射。可直接访问这个目录来获取系统信息；

## Linux命令

###  目录切换命令

- **`cd usr`：** 切换到该目录下 usr 目录
- **`cd ..（或cd../）`：** 切换到上一层目录
- **`cd /`：** 切换到系统根目录
- **`cd ~`：** 切换到用户主目录
- **`cd -`：** 切换到上一个操作所在目录

### 目录的操作命令(增删改查)

- **`mkdir 目录名称`：** **增加目录**。
- **`ls/ll`**（ll 是 ls -l 的别名，ll 命令可以看到该目录下的所有目录和文件的详细信息）：**查看目录信息**。
- **`find 目录 参数`：** **寻找目录**（查）。示例：① 列出当前目录及子目录下所有文件和文件夹: `find .`；② 在`/home`目录下查找以.txt 结尾的文件名:`find /home -name "*.txt"` ,忽略大小写: `find /home -iname "*.txt"` ；③ 当前目录及子目录下查找所有以.txt 和.pdf 结尾的文件:`find . \( -name "*.txt" -o -name "*.pdf" \)`或`find . -name "*.txt" -o -name "*.pdf"`。
- **`mv 目录名称 新目录名称`：** **修改目录的名称**（改）。注意：mv 的语法不仅可以对目录进行重命名而且也可以对各种文件，压缩包等进行 重命名的操作。mv 命令用来对文件或目录重新命名，或者将文件从一个目录移到另一个目录中。后面会介绍到 mv 命令的另一个用法。
- **`mv 目录名称 目录的新位置`：** **移动目录的位置---剪切**（改）。注意：mv 语法不仅可以对目录进行剪切操作，对文件和压缩包等都可执行剪切操作。另外 mv 与 cp 的结果不同，mv 好像文件“搬家”，文件个数并未增加。而 cp 对文件进行复制，文件个数增加了。
- **`cp -r 目录名称 目录拷贝的目标位置`：** **拷贝目录**（改），-r 代表递归拷贝 。注意：cp 命令不仅可以拷贝目录还可以拷贝文件，压缩包等，拷贝文件和压缩包时不 用写-r 递归。
- **`rm [-rf] 目录` :** **删除目录/文件（**删）。注意：rm 不仅可以删除目录，也可以删除其他文件或压缩包，为了增强大家的记忆， 无论删除任何目录或文件，都直接使用`rm -rf` 目录/文件/压缩包。

### 文件的操作命令(增删改查)

- **`touch 文件名称`:** 文件的创建（增）。
- **`cat/more/less/tail 文件名称`** ：文件的查看（查） 。more会以一页一页的显示方便使用者逐页阅读，同时**more会一次性加载文件所有内容**，在 more 的时候，我们并没有办法向前面翻， 只能往后面看(更新后可以往前看)，但若使用了 less 时，就可以使用 [pageup] [pagedown] 等按 键的功能来往前往后翻看文件，更容易用来查看一个文件的内容！除此之外，在 less 里头可以拥有更多的搜索功能，不止可以向下搜，也可以向上搜。命令 `tail -f 文件` 可以对某个文件进行动态监控，例如 tomcat 的日志文件， 会随着程序的运行，日志会变化，可以使用 `tail -f catalina-2016-11-11.log` 监控 文 件的变化 。
- **`vim 文件`：** 修改文件的内容（改）。vim 编辑器是 Linux 中的强大组件，是 vi 编辑器的加强版，vim 编辑器的命令和快捷方式有很多，但此处不一一阐述，大家也无需研究的很透彻，使用 vim 编辑修改文件的方式基本会使用就可以了。在实际开发中，使用 vim 编辑器主要作用就是修改配置文件，下面是一般步骤： `vim 文件------>进入文件----->命令模式------>按i进入编辑模式----->编辑文件 ------->按Esc进入底行模式----->输入：wq/q!` （输入 wq 代表写入内容并退出，即保存；输入 q!代表强制退出不保存）。

### 压缩文件的操作命令

**1）打包并压缩文件：**

Linux 中的打包文件一般是以.tar 结尾的，压缩的命令一般是以.gz 结尾的。而一般情况下打包和压缩是一起进行的，打包并压缩后的文件的后缀名一般.tar.gz。 命令：`tar -zcvf 打包压缩后的文件名 要打包压缩的文件` ，其中：

- z：调用 gzip 压缩命令进行压缩
- c：打包文件
- v：显示运行过程
- f：指定文件名

比如：假如 test 目录下有三个文件分别是：aaa.txt bbb.txt ccc.txt，如果我们要打包 test 目录并指定压缩后的压缩包名称为 test.tar.gz 可以使用命令：**`tar -zcvf test.tar.gz aaa.txt bbb.txt ccc.txt` 或 `tar -zcvf test.tar.gz /test/`**

**2）解压压缩包：**

命令：`tar -xvf 压缩文件`

其中：x：代表解压

示例：

- 将 /test 下的 test.tar.gz 解压到当前目录下可以使用命令：**`tar -xvf test.tar.gz`**
- 将 /test 下的 test.tar.gz 解压到根目录/usr 下:**`tar -xvf test.tar.gz -C /usr`**（- C 代表指定解压的位置）

### Linux 的权限命令

**修改文件/目录的权限的命令：`chmod`**

示例：修改/test 下的 aaa.txt 的权限为文件所有者有全部权限，文件所有者所在的组有读写权限，其他用户只有读的权限。

**`chmod u=rwx(读、写、执行),g=rw,o=r aaa.txt`** 或者 **`chmod 764 aaa.txt`**

- **所有者(u)** ：一般为文件的创建者，谁创建了该文件，就天然的成为该文件的所有者，用 `ls ‐ahl` 命令可以看到文件的所有者 也可以使用 chown 用户名 文件名来修改文件的所有者 。
- **文件所在组(g)** ：当某个用户创建了一个文件后，这个文件的所在组就是该用户所在的组用 `ls ‐ahl`命令可以看到文件的所有组也可以使用 chgrp 组名 文件名来修改文件所在的组。
- **其它组(o)** ：除开文件的所有者和所在组的用户外，系统的其它用户都是文件的其它组。

### Linux 用户管理

**Linux 用户管理相关命令:**

- `useradd 选项 用户名`:添加用户账号
- `userdel 选项 用户名`:删除用户帐号
- `usermod 选项 用户名`:修改帐号
- `passwd 用户名`:更改或创建用户的密码
- `passwd -S 用户名` :显示用户账号密码信息
- `passwd -d 用户名`: 清除用户密码

### 其他常用命令

- **`pwd`：** 显示当前所在位置

- `sudo + 其他命令`：以系统管理者的身份执行指令，也就是说，经由 sudo 所执行的指令就好像是 root 亲自执行。

- **`grep 要搜索的字符串 要搜索的文件 --color`：** 搜索命令，--color 代表高亮显示

- **ps -ef** 查看**当前系统正在运行进程**，**通常结合 grep 命令查看某进程的状态。 ps -ef|grep redis**

- **`kill -9 进程的pid`：** 杀死进程（-9 表示强制终止。）

  先用 ps 查找进程，然后用 kill 杀掉
  
# git
## git常用命令

### 代码提交和同步

- git add，变为已暂存状态，**存入暂存区**
- git commit，变为已提交状态，**把暂存区的代码存入本地仓库**
-  git push，变为已推送状态，**本地仓库的代码方上传到GitHub上**

### 代码撤销和撤销同步

```bash
git diff # 列出所有的修改
git checkout # 撤销项目下所有的修改
git reset # 暂存区的修改恢复到工作区
```

### 分支操作

```bash
git branch # 查看当前有哪些分支
git merge # 合并分支
```

### 版本回退

```bash
git log # 历史版本
git reset --hard HEAD^ #回退到上一个版本
git reflog # 版本前进
```

# Maven

著作权归https://pdai.tech所有。 链接：https://www.pdai.tech/md/devops/tool/tool-maven.html

## 依赖原则

### 依赖路径最短优先原则

```html
A -> B -> C -> X(1.0)
A -> D -> X(2.0)  
```

**ABC指模块**，由于 X(2.0) 路径最短，所以使用 X(2.0)。

### 声明顺序优先原则

```html
A -> B -> X(1.0)
A -> C -> X(2.0)
```

在 POM 中最先声明的优先，上面的两个依赖如果先声明 B，那么最后使用 X(1.0)。

### 覆写优先原则

子 POM 内声明的依赖优先于父 POM 中声明的依赖。

### 解决依赖冲突

1.使用 `mvn dependency:tree` 查看依赖树，判断加载的是什么版本的jar包

2.根据依赖原则来调整依赖在 POM 文件的声明顺序。

### 解决互相依赖

**合并互相依赖的模块生成模块D**，我们把它当做一个辅助构建模块，然后让**互相依赖的模块都依赖于D模块**。





# SSM框架

## Spring MVC篇 

###  1、什么是Spring MVC ？简单介绍下你对springMVC的理解? 

 SpringMVC是一个基于Java的实现了MVC设计模式的请求驱动类型的轻量级Web框架，通过把Model，View，Controller分离，将web层进行职责解耦，把复杂的web应用分成逻辑清晰的几部分，简化开发，减少出错，方便组内开发人员之间的配合。 

###  2、SpringMVC的流程？ 

 （1）用户发送请求至前端控制器DispatcherServlet； （2） DispatcherServlet收到请求后，调用HandlerMapping处理器映射器，请求获取Handle； （3）处理器映射器根据请求url找到具体的处理器，生成处理器对象及处理器拦截器(如果有则生成)一并返回给DispatcherServlet； （4）DispatcherServlet 调用 HandlerAdapter处理器适配器； （5）HandlerAdapter 经过适配调用 具体处理器(Handler，也叫后端控制器)； （6）Handler执行完成返回ModelAndView； （7）HandlerAdapter将Handler执行结果ModelAndView返回给DispatcherServlet； （8）DispatcherServlet将ModelAndView传给ViewResolver视图解析器进行解析； （9）ViewResolver解析后返回具体View； （10）DispatcherServlet对View进行渲染视图（即将模型数据填充至视图中） （11）DispatcherServlet响应用户。 




###  3、Springmvc的优点: 

 （1）可以支持各种视图技术,而不仅仅局限于JSP； 

 （2）与Spring框架集成（如IoC容器、AOP等）； 

 （3）清晰的角色分配：前端控制器(dispatcherServlet) , 请求到处理器映射（handlerMapping), 处理器适配器（HandlerAdapter), 视图解析器（ViewResolver）。 

 （4）支持各种请求资源的映射策略。 

###  4、Spring MVC的主要组件？ 

 （1）前端控制器 DispatcherServlet（不需要程序员开发） 

 作用：接收请求、响应结果，相当于转发器，有了DispatcherServlet 就减少了其它组件之间的耦合度。 

 （2）处理器映射器HandlerMapping（不需要程序员开发） 

 作用：根据请求的URL来查找Handler 

 （3）处理器适配器HandlerAdapter 

 注意：在编写Handler的时候要按照HandlerAdapter要求的规则去编写，这样适配器HandlerAdapter才可以正确的去执行Handler。 

 （4）处理器Handler（需要程序员开发） 

 （5）视图解析器 ViewResolver（不需要程序员开发） 

 作用：进行视图的解析，根据视图逻辑名解析成真正的视图（view） 

 （6）视图View（需要程序员开发jsp） 

 View是一个接口， 它的实现类支持不同的视图类型（jsp，freemarker，pdf等等） 

###  5、springMVC和struts2的区别有哪些? 

 （1）springmvc的入口是一个servlet即前端控制器（DispatchServlet），而struts2入口是一个filter过虑器（StrutsPrepareAndExecuteFilter）。 

 （2）springmvc是基于方法开发(一个url对应一个方法)，请求参数传递到方法的形参，可以设计为单例或多例(建议单例)，struts2是基于类开发，传递参数是通过类的属性，只能设计为多例。 

 （3）Struts采用值栈存储请求和响应的数据，通过OGNL存取数据，springmvc通过参数解析器是将request请求内容解析，并给方法形参赋值，将数据和视图封装成ModelAndView对象，最后又将ModelAndView中的模型数据通过reques域传输到页面。Jsp视图解析器默认使用jstl。 

###  6、SpringMVC怎么样设定重定向和转发的？ 

 （1）转发：在返回值前面加"forward:"，譬如"forward:user.do?name=method4" 

 （2）重定向：在返回值前面加"redirect:"，譬如"redirect:[http://www.baidu.com](https://link.zhihu.com/?target=http%3A/www.baidu.com)" 

###  7、SpringMvc怎么和AJAX相互调用的？ 

 通过Jackson框架就可以把Java里面的对象直接转化成Js可以识别的Json对象。具体步骤如下 ： 

 （1）加入Jackson.jar 

 （2）在配置文件中配置json的映射 

 （3）在接受Ajax方法里面可以直接返回Object,List等,但方法前面要加上@ResponseBody注解。 

###  8、如何解决POST请求中文乱码问题，GET的又如何处理呢？ ***

 （1）解决post请求乱码问题： 

 在web.xml中配置一个CharacterEncodingFilter过滤器，设置成utf-8；

```
<filter>

<filter-name>CharacterEncodingFilter</filter-name>

<filter-class>org.springframework.web.filter.CharacterEncodingFilter</filter-class>

<init-param>

<param-name>encoding</param-name>

<param-value>utf-8</param-value>

</init-param>

</filter>

<filter-mapping>

<filter-name>CharacterEncodingFilter</filter-name>

<url-pattern>/*</url-pattern>

</filter-mapping>
```

（2）get请求中文参数出现乱码解决方法有两个：

①修改tomcat配置文件添加编码与工程编码一致，如下：

```
<ConnectorURIEncoding="utf-8"connectionTimeout="20000"port="8080"protocol="HTTP/1.1"redirectPort="8443"/>
```

②另外一种方法对参数进行重新编码：

```
StringuserName =newString(request.getParamter("userName").getBytes("ISO8859-1"),"utf-8")
```

ISO8859-1是tomcat默认编码，需要将tomcat编码后的内容按utf-8编码。 

###  9、Spring MVC的异常处理 ？ 

 答：可以将异常抛给Spring框架，由Spring框架来处理；我们只需要配置简单的异常处理器，在异常处理器中添视图页面即可。 

###  10、SpringMvc的控制器是不是单例模式,如果是,有什么问题,怎么解决？ 

 答：是单例模式,所以在多线程访问的时候有线程安全问题,不要用同步,会影响性能的,解决方案是在控制器里面不能写字段。 

###  11、 SpringMVC常用的注解有哪些？ 

 @RequestMapping：用于处理请求 url 映射的注解，可用于类或方法上。用于类上，则表示类中的所有响应请求的方法都是以该地址作为父路径。 

 @RequestBody：注解实现接收http请求的json数据，将json转换为java对象。 

 @ResponseBody：注解实现将conreoller方法返回对象转化为json对象响应给客户。 

###  12、SpingMvc中的控制器的注解一般用那个,有没有别的注解可以替代？ 

 答：一般用@Controller注解,也可以使用@RestController,@RestController注解相当于@ResponseBody ＋ @Controller,表示是表现层,除此之外，一般不用别的注解代替。 

###  13、如果在拦截请求中，我想拦截get方式提交的方法,怎么配置？ 

 答：可以在@RequestMapping注解里面加上method=RequestMethod.GET。 

###  14、怎样在方法里面得到Request,或者Session？ 

 答：直接在方法的形参中声明request,SpringMvc就自动把request对象传入。 

###  15、如果想在拦截的方法里面得到从前台传入的参数,怎么得到？ 

 答：直接在形参里面声明这个参数就可以,但必须名字和传过来的参数一样。 

###  16、如果前台有很多个参数传入,并且这些参数都是一个对象的,那么怎么样快速得到这个对象？ 

 答：直接在方法中声明这个对象,SpringMvc就自动会把属性赋值到这个对象里面。 

###  17、SpringMvc中函数的返回值是什么？ 

 答：返回值可以有很多类型,有String, ModelAndView。ModelAndView类把视图和数据都合并的一起的，但一般用String比较好。 

###  18、SpringMvc用什么对象从后台向前台传递数据的？ 

 答：通过ModelMap对象,可以在这个对象里面调用put方法,把对象加到里面,前台就可以通过el表达式拿到。 

###  19、怎么样把ModelMap里面的数据放入Session里面？ 

 答：可以在类上面加上@SessionAttributes注解,里面包含的字符串就是要放入session里面的key。 

###  20、SpringMvc里面拦截器是怎么写的： 

 有两种写法,一种是实现HandlerInterceptor接口，另外一种是继承适配器类，接着在接口方法当中，实现处理逻辑；然后在SpringMvc的配置文件中配置拦截器即可：

```
<!-- 配置SpringMvc的拦截器 -->

<mvc:interceptors>
 
    <!-- 配置一个拦截器的Bean就可以了 默认是对所有请求都拦截 -->
 
    <bean id="myInterceptor" class="com.zwp.action.MyHandlerInterceptor"></bean>
 
    <!-- 只针对部分请求拦截 -->
 
    <mvc:interceptor>
 
   <mvc:mapping path="/modelMap.do" />
 
   <bean class="com.zwp.action.MyHandlerInterceptorAdapter" />
 
    </mvc:interceptor>
 
</mvc:interceptors>
```



## Spring篇 

###  1、Spring是什么? 

 Spring是一个轻量级的IoC和AOP容器框架。是为Java应用程序提供基础的一套框架，目的是用于简化企业应用程序的开发，它使得开发者只需要关心业务需求。常见的配置方式有三种：基于XML的配置、基于注解的配置、基于Java的配置。 

 主要由以下几个模块组成： 

 Spring Core：核心类库，提供IOC服务； 

 Spring Context：提供框架式的Bean访问方式，以及企业级功能（JNDI、定时任务等）； 

 Spring AOP：AOP服务； 

 Spring DAO：对JDBC的抽象，简化了数据访问异常的处理； 

 Spring ORM：对现有的ORM框架的支持； 

 Spring Web：提供了基本的面向Web的综合特性，例如多方文件上传； 

 Spring MVC：提供面向Web应用的Model-View-Controller实现。 

###  2、Spring 的优点？ 

 （1）spring属于低侵入(想要去除此框架或者模块时，代码的改动大小)式设计，代码的污染极低； 

 （2）spring的IOC将对象之间的依赖关系交由框架处理，减低组件的耦合性； 

 （3）Spring提供了AOP技术([面向切面编程](https://baike.baidu.com/item/面向切面编程/6016335)，通过[预编译](https://baike.baidu.com/item/预编译/3191547)方式和运行期间动态代理实现程序功能的统一维护的一种技术)，用于将那些与业务无关，但却对多个却为业务模块所共同调用的逻辑或责任（例如事务处理、日志管理、权限控制等）,抽取并封装为一个可重用的模块,减少系统中的重复代码，降低了模块间的耦合度。

 （4）spring对于主流的应用框架提供了集成支持。 

###  3、Spring的AOP理解： 

 OOP面向对象，允许开发者定义纵向的关系，但并适用于定义横向的关系，导致了大量代码的重复，而不利于各个模块的重用。 

 AOP，一般称为面向切面，**作为面向对象的一种补充，用于将那些与业务无关，但却对多个却为业务模块所共同调用的逻辑或责任（例如事务处理、日志管理、权限控制等）,抽取并封装为一个可重用的模块，这个模块被命名为“切面”（Aspect），减少系统中的重复代码，降低了模块间的耦合度**，同时提高了系统的可维护性。可用于权限认证、日志、事务处理。 

 AOP实现的关键在于 代理模式，AOP代理主要分为静态代理和动态代理。静态代理的代表为AspectJ；动态代理则以Spring AOP为代表。 

 （1）AspectJ是静态代理的增强，所谓静态代理，就是AOP框架会在编译阶段生成AOP代理类，因此也称为编译时增强，他会在编译阶段将AspectJ(切面)织入到Java字节码中，运行的时候就是增强之后的AOP对象。 

 （2）Spring AOP使用的动态代理，所谓的动态代理就是说AOP框架不会去修改字节码，而是每次运行时在内存中临时为方法生成一个AOP对象，这个AOP对象包含了目标对象的全部方法，并且在特定的切点做了增强处理，并回调原对象的方法。 

 **Spring AOP中的动态代理主要有两种方式，JDK动态代理和CGLIB动态代理：** 

 ①JDK动态代理只提供接口的代理，不支持类的代理。核心InvocationHandler接口和Proxy类，InvocationHandler通过invoke()方法反射来调用目标类中的代码，动态地将横切逻辑和业务编织在一起；接着，Proxy利用 InvocationHandler动态创建一个符合某一接口的的实例, 生成目标类的代理对象。 

 ②如果代理类没有实现接口，那么Spring AOP会选择使用CGLIB来动态代理目标类。CGLIB（Code Generation Library），是一个代码生成的类库，可以在运行时动态的生成指定类的一个子类对象，并覆盖其中特定方法并添加增强代码，从而实现AOP。CGLIB是通过继承的方式做的动态代理，因此如果某个类被标记为final，那么它是无法使用CGLIB做动态代理的。 

 （3）静态代理与动态代理区别在于生成AOP代理对象的时机不同，相对来说AspectJ的静态代理方式具有更好的性能，但是AspectJ需要特定的编译器进行处理，而Spring AOP则无需特定的编译器处理。 

 InvocationHandler 的 invoke(Objectproxy,Methodmethod,Object[] args)：proxy是最终生成的代理实例;method 是被代理目标实例的某个具体方法;args 是被代理目标实例某个方法的具体入参, 在方法反射调用时使用。 

###  4、Spring的IoC理解： 

 （1）IOC就是控制反转，**是指创建对象的控制权的转移，以前创建对象的主动权和时机是由程序把控的，而现在这种权力转移到Spring容器中**，并由容器根据配置文件去创建实例和管理各个实例之间的依赖关系，对象与对象之间松散耦合，也利于功能的复用。DI依赖注入，和控制反转是同一个概念的不同角度的描述，即 应用程序在运行时依赖IoC容器来动态注入对象需要的外部资源。 

 （2）最直观的表达就是，IOC让对象的创建不用去new了，可以由spring自动生产，使用java的反射机制，根据配置文件在运行时动态的去创建对象以及管理对象，并调用对象的方法的。 

 （3）Spring的IOC有三种注入方式 ：构造器注入、setter方法注入、根据注解注入。 

 IoC让相互协作的组件保持松散的耦合，而AOP编程允许你把遍布于应用各层的功能分离出来形成可重用的功能组件。 

###  5、BeanFactory和ApplicationContext有什么区别？ 

 BeanFactory和ApplicationContext是Spring的两大核心接口，都可以当做Spring的容器。其中**ApplicationContext是BeanFactory的子接口。** 

 （1）BeanFactory：是Spring里面最底层的接口，包含了各种Bean的定义，读取bean配置文档，管理bean的加载、实例化，控制bean的生命周期，维护bean之间的依赖关系。ApplicationContext接口作为BeanFactory的派生，除了提供BeanFactory所具有的功能外，还提供了更完整的框架功能： 

 ①继承MessageSource，因此**支持国际化**。 

 ②统一的资源文件访问方式。 

 ③提供在监听器中注册bean的事件。 

 ④**同时加载多个配置文件**。 

 ⑤载入多个（有继承关系）上下文 ，使得每一个上下文都专注于一个特定的层次，比如应用的web层。 

 （2）①BeanFactroy**采用的是延迟加载形式来注入Bean的，即只有在使用到某个Bean时(调用getBean())，才对该Bean进行加载实例化。**这样，我们就不能发现一些存在的Spring的配置问题。如果Bean的某一个属性没有注入，BeanFacotry加载后，直至第一次使用调用getBean方法才会抛出异常。 

 ②ApplicationContext，**它是在容器启动时，一次性创建了所有的Bean。这样，在容器启动时，我们就可以发现Spring中存在的配置错误，这样有利于检查所依赖属性是否注入。**ApplicationContext启动后预载入所有的单实例Bean，通过预载入单实例bean ,确保当你需要的时候，你就不用等待，因为它们已经创建好了。 

 ③相对于基本的BeanFactory，**ApplicationContext 唯一的不足是占用内存空间**。当应用程序配置Bean较多时，程序启动较慢。 

 （3）BeanFactory通常以编程的方式被创建，ApplicationContext还能以声明的方式创建，如使用ContextLoader。 

 （4）BeanFactory和ApplicationContext都支持BeanPostProcessor、BeanFactoryPostProcessor的使用，但两者之间的区别是：BeanFactory需要手动注册，而ApplicationContext则是自动注册。 

###  6、请解释Spring Bean的生命周期？ 



 Spring上下文中的Bean生命周期也类似，如下： 

 （1）实例化Bean： 

 对于BeanFactory容器，当客户向容器请求一个尚未初始化的bean时，或初始化bean的时候需要注入另一个尚未初始化的依赖时，容器就会调用createBean进行实例化。对于ApplicationContext容器，当容器启动结束后，通过获取BeanDefinition对象中的信息，实例化所有的bean。 

 （2）设置对象属性（依赖注入）： 

 实例化后的对象被封装在BeanWrapper对象中，紧接着，Spring根据BeanDefinition中的信息 以及 通过BeanWrapper提供的设置属性的接口完成依赖注入。 

 （3）处理Aware接口： 

 接着，Spring会检测该对象是否实现了xxxAware接口，并将相关的xxxAware实例注入给Bean： 

 ①如果这个Bean已经实现了BeanNameAware接口，会调用它实现的setBeanName(String beanId)方法，此处传递的就是Spring配置文件中Bean的id值； 

 ②如果这个Bean已经实现了BeanFactoryAware接口，会调用它实现的setBeanFactory()方法，传递的是Spring工厂自身。 

 ③如果这个Bean已经实现了ApplicationContextAware接口，会调用setApplicationContext(ApplicationContext)方法，传入Spring上下文； 

 （4）BeanPostProcessor： 

 如果想对Bean进行一些自定义的处理，那么可以让Bean实现了BeanPostProcessor接口，那将会调用postProcessBeforeInitialization(Object obj, String s)方法。 

 （5）InitializingBean与init-method： 

 如果Bean在Spring配置文件中配置了 init-method 属性，则会自动调用其配置的初始化方法。 

 （6）如果这个Bean实现了BeanPostProcessor接口，将会调用postProcessAfterInitialization(Object obj, String s)方法；由于这个方法是在Bean初始化结束时调用的，所以可以被应用于内存或缓存技术； 

 以上几个步骤完成后，Bean就已经被正确创建了，之后就可以使用这个Bean了。 

 （7）DisposableBean： 

 当Bean不再需要时，会经过清理阶段，如果Bean实现了DisposableBean这个接口，会调用其实现的destroy()方法； 

 （8）destroy-method： 

 最后，如果这个Bean的Spring配置中配置了destroy-method属性，会自动调用其配置的销毁方法。 

###  7、解释Spring支持的几种bean的作用域。 

 Spring容器中的bean可以分为5个范围： 

 （1）singleton：默认，每个容器中只有一个bean的实例，单例的模式由BeanFactory自身来维护。 

 （2）prototype：为每一个bean请求提供一个实例。 

 （3）request：为每一个网络请求创建一个实例，在请求完成以后，bean会失效并被垃圾回收器回收。 

 （4）session：与request范围类似，确保每个session中有一个bean的实例，在session过期后，bean会随之失效。 

 （5）global-session：全局作用域，global-session和Portlet应用相关。当你的应用部署在Portlet容器中工作时，它包含很多portlet。如果你想要声明让所有的portlet共用全局的存储变量的话，那么这全局变量需要存储在global-session中。全局作用域与Servlet中的session作用域效果相同。 

###  8、Spring框架中的单例Beans是线程安全的么？ 

 大部分的Spring bean是**无状态的，无状态就是一次操作，不能保存数据。无状态对象(Stateless Bean)，就是没有实例变量的对象 .不能保存数据，是不变类，所以在某种程度上说Spring的单例bean是线程安全的。**如果你的bean有多种状态的话（比如 View Model 对象），就需要自行保证线程安全。最浅显的解决办法就是将多态bean的作用域由“singleton”变更为“prototype”。  

###  9、Spring如何处理线程并发问题？ 

 在一般情况下，只有无状态的Bean才可以在多线程环境下共享，在Spring中，绝大部分Bean都可以声明为singleton作用域，因为Spring对一些Bean中非线程安全状态采用ThreadLocal进行处理，解决线程安全问题。 

 ThreadLocal和线程同步机制都是为了解决多线程中相同变量的访问冲突问题。同步机制采用了“时间换空间”的方式，仅提供一份变量，不同的线程在访问前需要获取锁，没获得锁的线程则需要排队。而ThreadLocal采用了“空间换时间”的方式。 

 ThreadLocal会为每一个线程提供一个独立的变量副本，从而隔离了多个线程对数据的访问冲突。因为每一个线程都拥有自己的变量副本，从而也就没有必要对该变量进行同步了。ThreadLocal提供了线程安全的共享对象，在编写多线程代码时，可以把不安全的变量封装进ThreadLocal。 

###  10-1、Spring基于xml注入bean的几种方式： 

 （1）Set方法注入； 

 （2）构造器注入：①通过index设置参数的位置；②通过type设置参数类型； 

 （3）静态工厂注入； 

 （4）实例工厂； 

###  10-2、Spring的自动装配： 

 在spring中，对象无需自己查找或创建与其关联的其他对象，由容器负责把需要相互协作的对象引用赋予各个对象，使用autowire来配置自动装载模式。 

 在Spring框架xml配置***有5种自动装配： 

 （1）no：默认的方式是不进行自动装配的，通过手工设置**ref属性**来进行装配bean。 

 （2）byName：通过bean的名称进行自动装配，如果一个bean的 property 与另一bean 的name 相同，就进行自动装配。 

 （3）byType：通过参数的数据类型进行自动装配。 

 （4）constructor：利用构造函数进行装配，并且构造函数的参数通过byType进行装配。 

 （5）autodetect：自动探测，如果有构造方法，通过 construct的方式自动装配，否则使用 byType的方式自动装配。 

 基于注解的方式： 

 使用**@Autowired注解来自动装配指定的bean(byType和byName)**。在使用@Autowired注解之前需要在Spring配置文件进行配置，<context:annotation-config />。在启动spring IoC时，容器自动装载了一个AutowiredAnnotationBeanPostProcessor后置处理器，当容器扫描到@Autowired、@Resource或@Inject时，就会在IoC容器自动查找需要的bean，并装配给该对象的属性。在使用@Autowired时，首先在容器中查询**对应类型**的bean： 

 如果查询结果刚好为一个，就将该bean装配给@Autowired指定的数据； 

 如果查询的结果不止一个，那么@Autowired会**根据名称**来查找； 

 如果上述查找的结果为空，那么会抛出异常。解决方法时，使用required=false。 

@Autowired可用于：构造函数、成员变量、Setter方法 

#### 注解@Autowired和@Resource之间的区别 

 (1) @Autowired默认是按照**类型**装配注入的，默认情况下它要求依赖对象必须存在（可以设置它required属性为false）。 

 (2) @Resource默认是按照**名称**来装配注入的，只有当找不到与名称匹配的bean才会按照类型来装配注入。 

#### 注解@Component 和 @Bean 的区别是什么？

1. `@Component` 注解**作用于类**，而`@Bean`注解**作用于方法**。
2. `@Component`通**常是通过类路径扫描来自动侦测以及自动装配到 Spring 容器中**（我们可以使用 `@ComponentScan` 注解定义要扫描的路径从中找出标识了需要装配的类自动装配到 Spring 的 bean 容器中）。`@Bean` 注解通常是我们在标有该注解的方法中定义产生这个 bean,`@Bean`告诉了 Spring 这是某个类的实例，当我需要用它的时候还给我。
3. `@Bean` 注解比 `@Component` 注解的自定义性更强，而且很多地方我们只能通过 `@Bean` 注解来注册 bean。比如当我们引用第三方库中的类需要装配到 `Spring`容器时，则只能通过 `@Bean`来实现。

### 将一个类声明为 bean 的注解有哪些?

我们一般使用 `@Autowired` 注解自动装配 bean，要想把类标识成可用于 `@Autowired` 注解自动装配的 bean 的类,采用以下注解可实现：

- `@Component` ：通用的注解，可标注任意类为 `Spring` 组件。如果一个 Bean 不知道属于哪个层，可以使用`@Component` 注解标注。
- `@Repository` : 对应持久层即 Dao 层，主要用于数据库相关操作。
- `@Service` : 对应服务层，主要涉及一些复杂的逻辑，需要用到 Dao 层。
- `@Controller` : 对应 Spring MVC 控制层，主要用户接受用户请求并调用 Service 层返回数据给前端页面。

###  11、Spring 框架中都用到了哪些设计模式？ 

 （1）工厂模式：BeanFactory就是简单工厂模式的体现，用来创建对象的实例，**工厂对象决定创建哪一种产品类的实例**； 

 （2）单例模式：Bean默认为单例模式，减少系统性能开消，减少内存开支，避免资源多重占用。 

 （3）代理模式：Spring的AOP功能用到了JDK的动态代理和CGLIB字节码生成技术； 

 （4）模板方法：用来解决代码重复的问题。比如.RestTemplate,JmsTemplate,JpaTemplate。 

 （5）观察者模式：定义对象键一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都会得到通知被制动更新，如Spring中listener的实现--ApplicationListener。 

###  12、Spring事务的实现方式和实现原理： 

 Spring事务的本质其实就是数据库对事务的支持，没有数据库的事务支持，spring是无法提供事务功能的。真正的数据库层的事务提交和回滚是通过binlog或者redo log实现的。 

#### （1）Spring事务的种类： 

 spring支持编程式事务管理和声明式事务管理两种方式：

​		①编程式事务管理使用TransactionTemplate。

​		②**声明式事务管理建立在AOP之上的。其本质是通过AOP功能，对方法前后进行拦截，将事务处理的功能编织到拦截的方法中**，也就是在目标方法开始之前加入一个事务，在执行完目标方法之后根据执行情况提交或者回滚事务。 

**声明式事务最大的优点就是不需要在业务逻辑代码中掺杂事务管理的代码，只需在配置文件中做相关的事务规则声明或通过@Transactional注解的方式**，便可以将事务规则应用到业务逻辑中。 

 声明式事务管理要优于编程式事务管理，这正是spring倡导的非侵入式的开发方式，使业务代码不受污染，只要加上注解就可以获得完全的事务支持。**唯一不足地方是，最细粒度只能作用到方法级别，无法做到像编程式事务那样可以作用到代码块级别**。 

#### （2）spring的事务传播行为： 

 spring事务的传播行为说的是，当多个事务同时存在的时候，spring如何处理这些事务的行为。

```
① PROPAGATION_REQUIRED：如果当前没有事务，就创建一个新事务，如果当前存在事务，就加入该事务，该设置是最常用的设置。

② PROPAGATION_SUPPORTS：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就以非事务执行。‘

③ PROPAGATION_MANDATORY：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就抛出异常。

④ PROPAGATION_REQUIRES_NEW：创建新事务，无论当前存不存在事务，都创建新事务。

⑤ PROPAGATION_NOT_SUPPORTED：以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。

⑥ PROPAGATION_NEVER：以非事务方式执行，如果当前存在事务，则抛出异常。

⑦ PROPAGATION_NESTED：如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则按REQUIRED属性执行。
复制代码
```

#### （3）Spring中的隔离级别：

```
①ISOLATION_DEFAULT：这是个PlatfromTransactionManager默认的隔离级别，使用数据库默认的事务隔离级别。

②ISOLATION_READ_UNCOMMITTED：读未提交，允许另外一个事务可以看到这个事务未提交的数据。

③ISOLATION_READ_COMMITTED：读已提交，保证一个事务修改的数据提交后才能被另一事务读取，而且能看到该事务对已有记录的更新。

④ISOLATION_REPEATABLE_READ：可重复读，保证一个事务修改的数据提交后才能被另一事务读取，但是不能看到该事务对已有记录的更新。

⑤ISOLATION_SERIALIZABLE：一个事务在执行的过程中完全看不到其他事务对数据库所做的更新。
```

### 13、Spring框架中有哪些不同类型的事件？ 

 Spring 提供了以下5种标准的事件： 

 （1）上下文更新事件（ContextRefreshedEvent）：在调用ConfigurableApplicationContext 接口中的refresh()方法时被触发。 

 （2）上下文开始事件（ContextStartedEvent）：当容器调用ConfigurableApplicationContext的Start()方法开始/重新开始容器时触发该事件。 

 （3）上下文停止事件（ContextStoppedEvent）：当容器调用ConfigurableApplicationContext的Stop()方法停止容器时触发该事件。 

 （4）上下文关闭事件（ContextClosedEvent）：当ApplicationContext被关闭时触发该事件。容器被关闭时，其管理的所有单例Bean都被销毁。 

 （5）请求处理事件（RequestHandledEvent）：在Web应用中，当一个http请求（request）结束触发该事件。 

 如果一个bean实现了ApplicationListener接口，当一个ApplicationEvent 被发布以后，bean会自动被通知。 

###  14、解释一下Spring AOP里面的几个名词： 

 （1）切面（Aspect）：通知和切点的集合，通知指被抽取的公共模块(日志记录、性能监控)，可能会横切多个对象。在Spring AOP中，切面可以使用通用类（基于模式的风格）或者在普通类中以@AspectJ注解来实现。 

 （2）连接点（Join point）：能够插入切面的一个点，在Spring AOP中，一个连接点总是代表一个方法的执行。 

 （3）通知（Advice）：在切面的某个特定的连接点（Join point）上执行的动作。通知有各种类型，其中包括“around”、“before”和“after”等通知。许多AOP框架，包括Spring，都是以拦截器做通知模型，并维护一个以连接点为中心的拦截器链。 

 （4）切入点（Pointcut）：切入点是指 指定拦截的方法。通过切入点表达式，指定拦截的方法，比如指定拦截add*、search*。 

 （5）引入（Introduction）：（也被称为内部类型声明（inter-type declaration））。声明额外的方法或者某个类型的字段。Spring允许引入新的接口（以及一个对应的实现）到任何被代理的对象。例如，你可以使用一个引入来使bean实现IsModified接口，以便简化缓存机制。 

 （6）目标对象（Target Object）：被一个或者多个切面（aspect）所通知（advise）的对象。也有人把它叫做被通知（adviced）对象。既然Spring AOP是通过运行时代理实现的，这个对象永远是一个被代理（proxied）对象。 

 （7）织入（Weaving）：指把切面应用到目标对象来创建新的代理对象的过程。Spring是在运行时完成织入。 

 切入点（pointcut）和连接点（join point）匹配的概念是AOP的关键，这使得AOP不同于其它仅仅提供拦截功能的旧技术。切入点使得定位通知（advice）可独立于OO层次。例如，一个提供声明式事务管理的around通知可以被应用到一组横跨多个对象中的方法上（例如服务层的所有业务操作）。 

![img](https://uploadfiles.nowcoder.com/files/20211224/687814481_1640353870270/v2-d6b1b537430a9d4ccb2d05f795abb1dc_720w.jpg)

###  15、Spring通知有哪些类型？ 

 （1）前置通知（Before advice）：在某连接点（join point）之前执行的通知，但这个通知不能阻止连接点前的执行（除非它抛出一个异常）。 

 （2）返回后通知（After returning advice）：在某连接点（join point）正常完成后执行的通知：例如，一个方法没有抛出任何异常，正常返回。 

 （3）抛出异常后通知（After throwing advice）：在方法抛出异常退出时执行的通知。 

 （4）后通知（After (finally) advice）：当某连接点退出的时候执行的通知（不论是正常返回还是异常退出）。 

 （5）环绕通知（Around Advice）：包围一个连接点（join point）的通知，如方法调用。这是最强大的一种通知类型。环绕通知可以在方法调用前后完成自定义的行为。它也会选择是否继续执行连接点或直接返回它们自己的返回值或抛出异常来结束执行。环绕通知是最常用的一种通知类型。大部分基于拦截的AOP框架，例如Nanning和JBoss4，都只提供环绕通知。

```
同一个aspect，不同advice的执行顺序：

①没有异常情况下的执行顺序：

around before advice
before advice
target method 执行
around after advice
after advice
afterReturning

②有异常情况下的执行顺序：

around before advice
before advice
target method 执行
around after advice
after advice
afterThrowing:异常发生
java.lang.RuntimeException: 异常发生
```



## Mybatis篇 

###  1、什么是Mybatis？ 

 （1）Mybatis是一个半ORM（对象关系映射）框架，它内部封装了JDBC，开发时只需要关注SQL语句本身，不需要花费精力去处理加载驱动、创建连接、创建statement等繁杂的过程。程序员直接编写原生态sql，可以严格控制sql执行性能，灵活度高。 

 （2）MyBatis 可以使用XML 或注解来配置和映射原生信息，将POJO映射成数据库中的记录，避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。 

 （3）通过xml 文件或注解的方式将要执行的各种 statement 配置起来，并通过java对象和 statement中sql的动态参数进行映射生成最终执行的sql语句，最后由mybatis框架执行sql并将结果映射为java对象并返回。（从执行sql到返回result的过程）。 

###  2、Mybaits的优点： 

 （1）基于SQL语句编程，相当灵活，不会对应用程序或者数据库的现有设计造成任何影响，SQL写在XML里，解除sql与程序代码的耦合，便于统一管理；提供XML标签，支持编写动态SQL语句，并可重用。 

 （2）与JDBC相比，减少了50%以上的代码量，消除了JDBC大量冗余的代码，不需要手动开关连接； 

 （3）很好的与各种数据库兼容（因为MyBatis使用JDBC来连接数据库，所以只要JDBC支持的数据库MyBatis都支持）。 

 （4）能够与Spring很好的集成； 

 （5）提供映射标签，支持对象与数据库的ORM字段关系映射；提供对象关系映射标签，支持对象关系组件维护。 

###  3、MyBatis框架的缺点： 

 （1）SQL语句的编写工作量较大，尤其当字段多、关联表多时，对开发人员编写SQL语句的功底有一定要求。 

 （2）SQL语句依赖于数据库，导致数据库移植性差，不能随意更换数据库。 

###  4、MyBatis框架适用场合： 

 （1）MyBatis专注于SQL本身，是一个足够灵活的DAO层解决方案。 

 （2）对性能的要求很高，或者需求变化较多的项目，如互联网项目，MyBatis将是不错的选择。 

###  5、MyBatis与Hibernate有哪些不同？ 

 （1）Mybatis和hibernate不同，它不完全是一个ORM框架，因为MyBatis需要程序员自己编写Sql语句。 

 （2）Mybatis直接编写原生态sql，可以严格控制sql执行性能，灵活度高，非常适合对关系数据模型要求不高的软件开发，因为这类软件需求变化频繁，一但需求变化要求迅速输出成果。但是灵活的前提是mybatis无法做到数据库无关性，如果需要实现支持多种数据库的软件，则需要自定义多套sql映射文件，工作量大。 

 （3）Hibernate对象/关系映射能力强，数据库无关性好，对于关系模型要求高的软件，如果用hibernate开发可以节省很多代码，提高效率。 

###  6、#{}和${}的区别是什么？ ***

 \#{}是预编译处理，${}是字符串替换。 

 Mybatis在处理#{}时，会将sql中的#{}替换为?号，调用**PreparedStatement的set方法来赋值**； 

 Mybatis在处理${}时，就是把 {} 替换成变量的值。 

 使用**#{}**可以有效的防止SQL注入，提高系统安全性。

#### SQL注入

前端传入的数据由于没做判断，拼接到sql语句中当作sql语句的一部分执行，导致数据库受损。 

**存在注入可能的情况**

1 url中get请求传参

2 搜索框post请求发送数据

3 http请求头部字段

**检测方法**

xray/burp漏洞扫描器

###  7、当实体类中的属性名和表中的字段名不一样 ，怎么办 ？ 

 第1种： 通过在查询的sql语句中定义字段名的别名，让字段名的别名和实体类的属性名一致。

```
<select id=”selectorder” parametertype=”int” resultetype=”me.gacl.domain.order”>
   select order_id id, order_no orderno ,order_price price form orders where order_id=#{id};
    </select>
```

第2种： 通过来映射字段名和实体类属性名的一一对应的关系。

```
<select id="getOrder" parameterType="int" resultMap="orderresultmap">
   select * from orders where order_id=#{id}
    </select>
 
   <resultMap type=”me.gacl.domain.order” id=”orderresultmap”>
    <!–用id属性来映射主键字段–>
    <id property=”id” column=”order_id”>
 
    <!–用result属性来映射非主键字段，property为实体类属性名，column为数据表中的属性–>
   <result property = “orderno” column =”order_no”/>
    <result property=”price” column=”order_price” />
    </reslutMap>
```

### 8、 模糊查询like语句该怎么写?

第1种：在Java代码中添加sql通配符。

```
string wildcardname = “%smi%”;
    list<name> names = mapper.selectlike(wildcardname);
 
    <select id=”selectlike”>
 select * from foo where bar like #{value}
    </select>
```

第2种：在sql语句中拼接通配符，会引起sql注入

```
string wildcardname = “smi”;
    list<name> names = mapper.selectlike(wildcardname);
 
    <select id=”selectlike”>
 select * from foo where bar like "%"#{value}"%"
    </select>
```

### 9、通常一个Xml映射文件，都会写一个Dao接口与之对应，请问，这个Dao接口的工作原理是什么？Dao接口里的方法，参数不同时，方法能重载吗？ 

 Dao接口即Mapper接口。接口的全限名，就是映射文件中的namespace的值；接口的方法名，就是映射文件中Mapper的Statement的id值；接口方法内的参数，就是传递给sql的参数。 

 Mapper接口里的方法，是不能重载的，因为是使用 全限名+方法名 的保存和寻找策略。Mapper接口的工作原理是JDK动态代理，Mybatis运行时会使用JDK动态代理为Mapper接口生成代理对象proxy，代理对象会拦截接口方法，转而执行MapperStatement所代表的sql，然后将sql执行结果返回。 

###  10、Mybatis是如何进行分页的？分页插件的原理是什么？ 

 Mybatis使用RowBounds对象进行分页，它是针对ResultSet结果集执行的内存分页，而非物理分页。可以在sql内直接书写带有物理分页的参数来完成物理分页功能，也可以使用分页插件来完成物理分页。 

 分页插件的基本原理是使用Mybatis提供的插件接口，实现自定义插件，在插件的拦截方法内拦截待执行的sql，然后重写sql，根据dialect方言，添加对应的物理分页语句和物理分页参数。 

###  11、Mybatis是如何将sql执行结果封装为目标对象并返回的？都有哪些映射形式？ 

 第一种是使用resultMap标签，逐一定义数据库列名和对象属性名之间的映射关系。 

 第二种是使用sql列的别名功能，将列的别名书写为对象属性名。 

 有了列名与属性名的映射关系后，Mybatis通过反射创建对象，同时使用反射给对象的属性逐一赋值并返回，那些找不到映射关系的属性，是无法完成赋值的。 

###  12、如何执行批量插入? 

 首先,创建一个简单的insert语句:

```
<insert id=”insertname”>
    insert into names (name) values (#{value})
    </insert>
```

然后在java代码中像下面这样执行批处理插入:

```
list<string> names = new arraylist();
    names.add(“fred”);
    names.add(“barney”);
    names.add(“betty”);
    names.add(“wilma”);
 
    // 注意这里 executortype.batch
    sqlsession sqlsession = sqlsessionfactory.opensession(executortype.batch);
    try {
 namemapper mapper = sqlsession.getmapper(namemapper.class);
 for (string name : names) {
 mapper.insertname(name);
 }
 sqlsession.commit();
    }catch(Exception e){
 e.printStackTrace();
 sqlSession.rollback(); 
 throw e; 
}
finally {
    sqlsession.close();
    }
```

### 13、如何获取自动生成的(主)键值? 

 insert 方法总是返回一个int值 ，这个值代表的是插入的行数。 

 如果采用自增长策略，自动生成的键值在 insert 方法执行完后可以被设置到传入的参数对象中。 

 示例：

```
<insert id=”insertname” usegeneratedkeys=”true” keyproperty=”id”>
 insert into names (name) values (#{name})
</insert>
name name = new name();
    name.setname(“fred”);
 
    int rows = mapper.insertname(name);
    // 完成后,id已经被设置到对象中
    system.out.println(“rows inserted = ” + rows);
    system.out.println(“generated key value = ” + name.getid());
```

### 14、在mapper中如何传递多个参数?

```
1）第一种：
//DAO层的函数
Public UserselectUser(String name,String area);  
//对应的xml,#{0}代表接收的是dao层中的第一个参数，#{1}代表dao层中第二参数，更多参数一致往后加即可。
<select id="selectUser"resultMap="BaseResultMap">  
    select *  fromuser_user_t   whereuser_name = #{0} anduser_area=#{1}  
</select>  
（2）第二种： 使用 @param 注解:
public interface usermapper {
   user selectuser(@param(“username”) string username,@param(“hashedpassword”) string hashedpassword);
}
然后,就可以在xml像下面这样使用(推荐封装为一个map,作为单个参数传递给mapper):
<select id=”selectuser” resulttype=”user”>
 select id, username, hashedpassword
 from some_table
 where username = #{username}
 and hashedpassword = #{hashedpassword}
</select>
（3）第三种：多个参数封装成map
try{
//映射文件的命名空间.SQL片段的ID，就可以调用对应的映射文件中的SQL
//由于我们的参数超过了两个，而方法中只有一个Object参数收集，因此我们使用Map集合来装载我们的参数
Map<String, Object> map = new HashMap();
 map.put("start", start);
 map.put("end", end);
 return sqlSession.selectList("StudentID.pagination", map);
 }catch(Exception e){
 e.printStackTrace();
 sqlSession.rollback();
    throw e; }
finally{
 MybatisUtil.closeSqlSession();
 }
```

### 15、Mybatis的Xml映射文件中，不同的Xml映射文件，id是否可以重复？ 

 不同的Xml映射文件，如果配置了namespace，那么id可以重复；如果没有配置namespace，那么id不能重复； 

 原因就是namespace+id是作为Map<String, MapperStatement>的key使用的，如果没有namespace，就剩下id，那么，id重复会导致数据互相覆盖。有了namespace，自然id就可以重复，namespace不同，namespace+id自然也就不同。 

 但是，在以前的Mybatis版本的namespace是可选的，不过新版本的namespace已经是必须的了。 

###  16、为什么说Mybatis是半自动ORM映射工具？它与全自动的区别在哪里？ 

 Hibernate属于全自动ORM映射工具，使用Hibernate查询关联对象或者关联集合对象时，可以根据对象关系模型直接获取，所以它是全自动的。而Mybatis在查询关联对象或关联集合对象时，需要手动编写sql来完成，所以，称之为半自动ORM映射工具。 

###  17、 一对一、一对多的关联查询 ？

```
<mapper namespace="com.lcb.mapping.userMapper">  
    <!--association  一对一关联查询 -->  
    <select id="getClass" parameterType="int" resultMap="ClassesResultMap">  
    select * from class c,teacher t where c.teacher_id=t.t_id and c.c_id=#{id}  
    </select>  
 
    <resultMap type="com.lcb.user.Classes" id="ClassesResultMap">  
   <!-- 实体类的字段名和数据表的字段名映射 -->  
   <id property="id" column="c_id"/>  
    <result property="name" column="c_name"/>  
    <association property="teacher" javaType="com.lcb.user.Teacher">  
    <id property="id" column="t_id"/>  
    <result property="name" column="t_name"/>  
    </association>  
    </resultMap>  
 
 
    <!--collection  一对多关联查询 -->  
    <select id="getClass2" parameterType="int" resultMap="ClassesResultMap2">  
    select * from class c,teacher t,student s where c.teacher_id=t.t_id and c.c_id=s.class_id and c.c_id=#{id}  
    </select>  
 
    <resultMap type="com.lcb.user.Classes" id="ClassesResultMap2">  
    <id property="id" column="c_id"/>  
    <result property="name" column="c_name"/>  
    <association property="teacher" javaType="com.lcb.user.Teacher">  
    <id property="id" column="t_id"/>  
    <result property="name" column="t_name"/>  
    </association>  
 
   <collection property="student" ofType="com.lcb.user.Student">  
    <id property="id" column="s_id"/>  
    <result property="name" column="s_name"/>  
    </collection>  
    </resultMap>  
</mapper>
```

### 18、MyBatis实现一对一有几种方式?具体怎么操作的？ 

 有联合查询和嵌套查询,联合查询是几个表联合查询,只查询一次,通过在resultMap里面配置association节点配置一对一的类就可以完成； 

 嵌套查询是先查一个表，根据这个表里面的结果的 外键id，去再另外一个表里面查询数据,也是通过association配置，但另外一个表的查询通过select属性配置。 

###  19、MyBatis实现一对多有几种方式,怎么操作的？ 

 有联合查询和嵌套查询。联合查询是几个表联合查询,只查询一次,通过在**resultMap里面的collection节点配置一对多的类就可以完成**；嵌套查询是先查一个表,根据这个表里面的 结果的外键id,去再另外一个表里面查询数据,也是通过配置collection,但另外一个表的查询通过select节点配置。 

###  20、Mybatis是否支持延迟加载？如果支持，它的实现原理是什么？ 

 答：Mybatis仅支持association关联对象和collection关联集合对象的延迟加载，association指的就是一对一，collection指的就是一对多查询。在Mybatis配置文件中，可以配置是否启用延迟加载lazyLoadingEnabled=true|false。 

 它的原理是，使用CGLIB创建目标对象的代理对象，当调用目标方法时，进入拦截器方法，比如调用a.getB().getName()，拦截器invoke()方法发现a.getB()是null值，那么就会单独发送事先保存好的查询关联B对象的sql，把B查询上来，然后调用a.setB(b)，于是a的对象b属性就有值了，接着完成a.getB().getName()方法的调用。这就是延迟加载的基本原理。 

 当然了，不光是Mybatis，几乎所有的包括Hibernate，支持延迟加载的原理都是一样的。 

###  21、Mybatis的一级、二级缓存: 

 1）一级缓存: 基于 PerpetualCache 的 HashMap 本地缓存，其存储作用域为 Session，当 Session flush 或 close 之后，该 Session 中的所有 Cache 就将清空，默认打开一级缓存。 

 2）二级缓存与一级缓存其机制相同，默认也是采用 PerpetualCache，HashMap 存储，不同在于其存储作用域为 Mapper(Namespace)，并且可自定义存储源，如 Ehcache。默认不打开二级缓存，要开启二级缓存，使用二级缓存属性类需要实现Serializable序列化接口(可用来保存对象的状态),可在它的映射文件中配置 ； 

 3）对于缓存数据更新机制，当某一个作用域(一级缓存 Session/二级缓存Namespaces)的进行了C/U/D 操作后，默认该作用域下所有 select 中的缓存将被 clear 掉并重新更新，如果开启了二级缓存，则只根据配置判断是否刷新。 

###  22、什么是MyBatis的接口绑定？有哪些实现方式？ 

 接口绑定，就是在MyBatis中任意定义接口,然后把接口里面的方法和SQL语句绑定,我们直接调用接口方法就可以,这样比起原来了SqlSession提供的方法我们可以有更加灵活的选择和设置。 

 接口绑定有两种实现方式,一种是通过注解绑定，就是在接口的方法上面加上@Select、@Update等注解，里面包含Sql语句来绑定；另外一种就是通过xml里面写SQL来绑定,在这种情况下,要指定xml映射文件里面的namespace必须为接口的全路径名。当Sql语句比较简单时候,用注解绑定,当SQL语句比较复杂时候,用xml绑定,一般用xml绑定的比较多。 

###  23、使用MyBatis的mapper接口调用时有哪些要求？ 

 ①Mapper接口方法名和mapper.xml中定义的每个sql的id相同； 

 ②Mapper接口方法的输入参数类型和mapper.xml中定义的每个sql 的parameterType的类型相同； 

 ③Mapper接口方法的输出参数类型和mapper.xml中定义的每个sql的resultType的类型相同； 

 ④Mapper.xml文件中的namespace即是mapper接口的类路径。 

###  24、Mapper编写有哪几种方式？ 

 第一种：接口实现类继承SqlSessionDaoSupport：使用此种方法需要编写mapper接口，mapper接口实现类、mapper.xml文件。 

 （1）在sqlMapConfig.xml中配置mapper.xml的位置

```
<mappers>
  <mapper resource="mapper.xml文件的地址" />
  <mapper resource="mapper.xml文件的地址" />
</mappers>
```

（2）定义mapper接口

（3）实现类集成SqlSessionDaoSupport

mapper方法中可以this.getSqlSession()进行数据增删改查。

（4）spring 配置

```
<bean id=" " class="mapper接口的实现">
  <property name="sqlSessionFactory" ref="sqlSessionFactory"></property>
</bean>
```

第二种：使用org.mybatis.spring.mapper.MapperFactoryBean： 

 （1）在sqlMapConfig.xml中配置mapper.xml的位置，如果mapper.xml和mappre接口的名称相同且在同一个目录，这里可以不用配置

```
<mappers>
  <mapper resource="mapper.xml文件的地址" />
  <mapper resource="mapper.xml文件的地址" />
</mappers>
```

（2）定义mapper接口： 

 ①mapper.xml中的namespace为mapper接口的地址 

 ②mapper接口中的方法名和mapper.xml中的定义的statement的id保持一致 

 ③Spring中定义

```
<bean id="" class="org.mybatis.spring.mapper.MapperFactoryBean">
  <property name="mapperInterface"  value="mapper接口地址" />
  <property name="sqlSessionFactory" ref="sqlSessionFactory" />
</bean>
```

第三种：使用mapper扫描器： 

 （1）mapper.xml文件编写： mapper.xml中的namespace为mapper接口的地址； mapper接口中的方法名和mapper.xml中的定义的statement的id保持一致； 如果将mapper.xml和mapper接口的名称保持一致则不用在sqlMapConfig.xml中进行配置。 

 （2）定义mapper接口： 注意mapper.xml的文件名和mapper的接口名称保持一致，且放在同一个目录 

 （3）配置mapper扫描器：

```
<bean class="org.mybatis.spring.mapper.MapperScannerConfigurer">
  <property name="basePackage" value="mapper接口包地址"></property>
 <property name="sqlSessionFactoryBeanName" value="sqlSessionFactory"/>
</bean>
```

（4）使用扫描器后从spring容器中获取mapper的实现对象。 

###  25、简述Mybatis的插件运行原理，以及如何编写一个插件。 

 答：Mybatis仅可以编写针对ParameterHandler、ResultSetHandler、StatementHandler、Executor这4种接口的插件，Mybatis使用JDK的动态代理，为需要拦截的接口生成代理对象以实现接口方法拦截功能，每当执行这4种接口对象的方法时，就会进入拦截方法，具体就是InvocationHandler的invoke()方法，当然，只会拦截那些你指定需要拦截的方法。 

 编写插件：实现Mybatis的Interceptor接口并复写intercept()方法，然后在给插件编写注解，指定要拦截哪一个接口的哪些方法即可，记住，别忘了在配置文件中配置你编写的插件。

# 分布式和系统设计面试题

## 分布式

### 1.分布式事务是什么？为什么会有分布式事务？

分布式事务是指需要满足事务ACID特性的操作发生在不同服务器上。 

分布式事务产生的原因是为了解决同一系统中不同服务器之间的数据一致性问题。

### 2.分布式事务解决方案有哪些？各自的优缺点是什么？

分布式事务的协议包括：2PC，3PC，TCC等

分布式事务的解决方案包括：

- 刚性事务：严格保证ACID性质，但系统开销大 
  - 全局事务 
- 柔性事务：保证原子性和持久性，最终一致性 
  - 本地消息表（异步确保）缺点：需要维护消息表 
  - TCC（2PC补偿）可用于强一致性需求的场景，缺点：需要编写补偿代码 
  - 队列消息（定期校对）时间不敏感的场景，缺点：需要MQ支持，否则要编写相应代码 

### 3.分布式Session有哪些解决方案？

1. Session复制：利用Tomcat配置实现，将服务器A的Session复制到服务器B，服务器B的复制到A。
2. 粘性Session：利用Hash算法将用户绑定到某个服务器，但服务器宕机时Session会失效
3. 客户端存储：使用JWT，由header+payload+signature构成
   - 优点：易于扩展，无状态
   - 缺点：安全性差，一次性（难以废除和续签）
4. Session共享：使用Redis等数据库专门存储Session



## 系统设计

### 1.如何设计一个幂等的接口？

- 调用者提供一个唯一的请求标识，该标识可以确定一个只会执行一次的工作单元。
- 接收者在执行工作前需要根据请求标识检验该工作单元是否已经执行过，如果执行过直接查询执行后的相应信息，否则执行该工作

















 





# 如何实现一个IOC容器?

​		IOC(Inversion of Control),意思是控制反转，不是什么技术，而是一种设计思想，IOC意味着将你设计好的对象交给容器控制，而不是传统的在你的对象内部直接控制。

​		在传统的程序设计中，我们直接在对象内部通过new进行对象创建，是程序主动去创建依赖对象，而IOC是有专门的容器来进行对象的创建，即IOC容器来控制对象的创建。

​		在传统的应用程序中，我们是在对象中主动控制去直接获取依赖对象，这个是正转，反转是由容器来帮忙创建及注入依赖对象，在这个过程过程中，由容器帮我们查找级注入依赖对象，对象只是被动的接受依赖对象。

​		1、先准备一个基本的容器对象，包含一些map结构的集合，用来方便后续过程中存储具体的对象

​		2、进行配置文件的读取工作或者注解的解析工作，将需要创建的bean对象都封装成BeanDefinition对象存储在容器中

​		3、容器将封装好的BeanDefinition对象通过反射的方式进行实例化，完成对象的实例化工作

​		4、进行对象的初始化操作，也就是给类中的对应属性值就行设置，也就是进行依赖注入，完成整个对象的创建，变成一个完整的bean对象，存储在容器的某个map结构中

​		5、通过容器对象来获取对象，进行对象的获取和逻辑处理工作

​		6、提供销毁操作，当对象不用或者容器关闭的时候，将无用的对象进行销毁

# 说说你对Spring 的理解？

官网地址：https://spring.io/projects/spring-framework#overview

压缩包下载地址：https://repo.spring.io/release/org/springframework/spring/

源码地址：https://github.com/spring-projects/spring-framework

```tex
Spring makes it easy to create Java enterprise applications. It provides everything you need to embrace the Java language in an enterprise environment, with support for Groovy and Kotlin as alternative languages on the JVM, and with the flexibility to create many kinds of architectures depending on an application’s needs. As of Spring Framework 5.1, Spring requires JDK 8+ (Java SE 8+) and provides out-of-the-box support for JDK 11 LTS. Java SE 8 update 60 is suggested as the minimum patch release for Java 8, but it is generally recommended to use a recent patch release.

Spring supports a wide range of application scenarios. In a large enterprise, applications often exist for a long time and have to run on a JDK and application server whose upgrade cycle is beyond developer control. Others may run as a single jar with the server embedded, possibly in a cloud environment. Yet others may be standalone applications (such as batch or integration workloads) that do not need a server.

Spring is open source. It has a large and active community that provides continuous feedback based on a diverse range of real-world use cases. This has helped Spring to successfully evolve over a very long time.

Spring 使创建 Java 企业应用程序变得更加容易。它提供了在企业环境中接受 Java 语言所需的一切,，并支持 Groovy 和 Kotlin 作为 JVM 上的替代语言，并可根据应用程序的需要灵活地创建多种体系结构。 从 Spring Framework 5.0 开始，Spring 需要 JDK 8(Java SE 8+)，并且已经为 JDK 9 提供了现成的支持。

Spring支持各种应用场景， 在大型企业中, 应用程序通常需要运行很长时间，而且必须运行在 jdk 和应用服务器上，这种场景开发人员无法控制其升级周期。 其他可能作为一个单独的jar嵌入到服务器去运行，也有可能在云环境中。还有一些可能是不需要服务器的独立应用程序(如批处理或集成的工作任务)。

Spring 是开源的。它拥有一个庞大而且活跃的社区，提供不同范围的，真实用户的持续反馈。这也帮助Spring不断地改进,不断发展。
```

# 你觉得Spring的核心是什么？

​		spring是一个开源框架。

​		spring是为了简化企业开发而生的，使得开发变得更加优雅和简洁。

​		spring是一个**IOC**和**AOP**的容器框架。

​				IOC：控制反转

​				AOP：面向切面编程

​				容器：包含并管理应用对象的生命周期，就好比用桶装水一样，spring就是桶，而对象就是水

# 说一下使用spring的优势？

​		1、Spring通过DI、AOP和消除样板式代码来简化企业级Java开发

​		2、Spring框架之外还存在一个构建在核心框架之上的庞大生态圈，它将Spring扩展到不同的领域，如Web服务、REST、移动开发以及NoSQL

​		3、低侵入式设计，代码的污染极低

​		4、独立于各种应用服务器，基于Spring框架的应用，可以真正实现Write Once,Run Anywhere的承诺

​		5、Spring的IoC容器降低了业务对象替换的复杂性，提高了组件之间的解耦

​		6、Spring的AOP支持允许将一些通用任务如安全、事务、日志等进行集中式处理，从而提供了更好的复用

​		7、Spring的ORM和DAO提供了与第三方持久层框架的的良好整合，并简化了底层的数据库访问

​		8、Spring的高度开放性，并不强制应用完全依赖于Spring，开发者可自由选用Spring框架的部分或全部

# Spring是如何简化开发的？

​		基于POJO的轻量级和最小侵入性编程

​		通过依赖注入和面向接口实现松耦合

​		基于切面和惯例进行声明式编程

​		通过切面和模板减少样板式代码

# 说说你对Aop的理解？

​		AOP全称叫做 Aspect Oriented Programming  面向切面编程。它是为解耦而生的，解耦是程序员编码开发过程中一直追求的境界，AOP在业务类的隔离上，绝对是做到了解耦，在这里面有几个核心的概念：

- 切面（Aspect）: 指关注点模块化，这个关注点可能会横切多个对象。事务管理是企业级Java应用中有关横切关注点的例子。 在Spring AOP中，切面可以使用通用类基于模式的方式（schema-based approach）或者在普通类中以`@Aspect`注解（@AspectJ 注解方式）来实现。

- 连接点（Join point）: 在程序执行过程中某个特定的点，例如某个方法调用的时间点或者处理异常的时间点。在Spring AOP中，一个连接点总是代表一个方法的执行。

- 通知（Advice）: 在切面的某个特定的连接点上执行的动作。通知有多种类型，包括“around”, “before” and “after”等等。通知的类型将在后面的章节进行讨论。 许多AOP框架，包括Spring在内，都是以拦截器做通知模型的，并维护着一个以连接点为中心的拦截器链。

- 切点（Pointcut）: 匹配连接点的断言。通知和切点表达式相关联，并在满足这个切点的连接点上运行（例如，当执行某个特定名称的方法时）。切点表达式如何和连接点匹配是AOP的核心：Spring默认使用AspectJ切点语义。

- 引入（Introduction）: 声明额外的方法或者某个类型的字段。Spring允许引入新的接口（以及一个对应的实现）到任何被通知的对象上。例如，可以使用引入来使bean实现 `IsModified`接口， 以便简化缓存机制（在AspectJ社区，引入也被称为内部类型声明（inter））。

- 目标对象（Target object）: 被一个或者多个切面所通知的对象。也被称作被通知（advised）对象。既然Spring AOP是通过运行时代理实现的，那么这个对象永远是一个被代理（proxied）的对象。

- AOP代理（AOP proxy）:AOP框架创建的对象，用来实现切面契约（aspect contract）（包括通知方法执行等功能）。在Spring中，AOP代理可以是JDK动态代理或CGLIB代理。

- 织入（Weaving）: 把切面连接到其它的应用程序类型或者对象上，并创建一个被被通知的对象的过程。这个过程可以在编译时（例如使用AspectJ编译器）、类加载时或运行时中完成。 Spring和其他纯Java AOP框架一样，是在运行时完成织入的。

  这些概念都太学术了，如果更简单的解释呢，其实非常简单：

  任何一个系统都是由不同的组件组成的，每个组件负责一块特定的功能，当然会存在很多组件是跟业务无关的，例如日志、事务、权限等核心服务组件，这些核心服务组件经常融入到具体的业务逻辑中，如果我们为每一个具体业务逻辑操作都添加这样的代码，很明显代码冗余太多，因此我们需要将这些公共的代码逻辑抽象出来变成一个切面，然后注入到目标对象（具体业务）中去，AOP正是基于这样的一个思路实现的，通过动态代理的方式，将需要注入切面的对象进行代理，在进行调用的时候，将公共的逻辑直接添加进去，而不需要修改原有业务的逻辑代码，只需要在原来的业务逻辑基础之上做一些增强功能即可。

# 说说你对IOC的理解？

```
	IoC is also known as dependency injection (DI). It is a process whereby objects define their dependencies (that is, the other objects they work with) only through constructor arguments, arguments to a factory method, or properties that are set on the object instance after it is constructed or returned from a factory method. The container then injects those dependencies when it creates the bean. This process is fundamentally the inverse (hence the name, Inversion of Control) of the bean itself controlling the instantiation or location of its dependencies by using direct construction of classes or a mechanism such as the Service Locator pattern.
	IOC与大家熟知的依赖注入同理，. 这是一个通过依赖注入对象的过程 也就是说，它们所使用的对象，是通过构造函数参数，工厂方法的参数或这是从工厂方法的构造函数或返回值的对象实例设置的属性，然后容器在创建bean时注入这些需要的依赖。 这个过程相对普通创建对象的过程是反向的（因此称之为IoC），bean本身通过直接构造类来控制依赖关系的实例化或位置，或提供诸如服务定位器模式之类的机制。
```

​		如果这个过程比较难理解的话，那么可以想象自己找女朋友和婚介公司找女朋友的过程。如果这个过程能够想明白的话，那么我们现在回答上面的问题：

```
1、谁控制谁：在之前的编码过程中，都是需要什么对象自己去创建什么对象，有程序员自己来控制对象，而有了IOC容器之后，就会变成由IOC容器来控制对象，
2、控制什么：在实现过程中所需要的对象及需要依赖的对象
3、什么是反转：在没有IOC容器之前我们都是在对象中主动去创建依赖的对象，这是正转的，而有了IOC之后，依赖的对象直接由IOC容器创建后注入到对象中，由主动创建变成了被动接受，这是反转
4、哪些方面被反转：依赖的对象
```

# BeanFactory和ApplicationContext有什么区别

相同：

- Spring提供了两种不同的IOC 容器，一个是BeanFactory，另外一个是ApplicationContext，它们都是Java  interface，ApplicationContext继承于BeanFactory(ApplicationContext继承ListableBeanFactory。
- 它们都可以用来配置XML属性，也支持属性的自动注入。
- 而ListableBeanFactory继承BeanFactory)，BeanFactory 和 ApplicationContext 都提供了一种方式，使用getBean("bean name")获取bean。

不同：

- 当你调用getBean()方法时，BeanFactory仅实例化bean，而ApplicationContext 在启动容器的时候实例化单例bean，不会等待调用getBean()方法时再实例化。
- BeanFactory不支持国际化，即i18n，但ApplicationContext提供了对它的支持。
- BeanFactory与ApplicationContext之间的另一个区别是能够将事件发布到注册为监听器的bean。
- BeanFactory 的一个核心实现是XMLBeanFactory 而ApplicationContext  的一个核心实现是ClassPathXmlApplicationContext，Web容器的环境我们使用WebApplicationContext并且增加了getServletContext 方法。
- 如果使用自动注入并使用BeanFactory，则需要使用API注册AutoWiredBeanPostProcessor，如果使用ApplicationContext，则可以使用XML进行配置。
- 简而言之，BeanFactory提供基本的IOC和DI功能，而ApplicationContext提供高级功能，BeanFactory可用于测试和非生产使用，但ApplicationContext是功能更丰富的容器实现，应该优于BeanFactory

# 简述spring bean的生命周期？

![](C:/Users/y/Desktop/春招笔记/images/bean的生命周期.png)

1、实例化bean对象

​	通过反射的方式进行对象的创建，此时的创建只是在堆空间中申请空间，属性都是默认值

2、设置对象属性

​	给对象中的属性进行值的设置工作

3、检查Aware相关接口并设置相关依赖

​	如果对象中需要引用容器内部的对象，那么需要调用aware接口的子类方法来进行统一的设置

4、BeanPostProcessor的前置处理

​	对生成的bean对象进行前置的处理工作

5、检查是否是InitializingBean的子类来决定是否调用afterPropertiesSet方法

​	判断当前bean对象是否设置了InitializingBean接口，然后进行属性的设置等基本工作

6、检查是否配置有自定义的init-method方法

​	如果当前bean对象定义了初始化方法，那么在此处调用初始化方法

7、BeanPostProcessor后置处理

​	对生成的bean对象进行后置的处理工作

8、注册必要的Destruction相关回调接口

​	为了方便对象的销毁，在此处调用注销的回调接口，方便对象进行销毁操作

9、获取并使用bean对象

​	通过容器来获取对象并进行使用

10、是否实现DisposableBean接口

​	判断是否实现了DisposableBean接口，并调用具体的方法来进行对象的销毁工作

11、是否配置有自定义的destory方法

​	如果当前bean对象定义了销毁方法，那么在此处调用销毁方法

# spring支持的bean作用域有哪些？

① singleton

使用该属性定义Bean时，IOC容器仅创建一个Bean实例，IOC容器每次返回的是同一个Bean实例。

② prototype

使用该属性定义Bean时，IOC容器可以创建多个Bean实例，每次返回的都是一个新的实例。

③ request

该属性仅对HTTP请求产生作用，使用该属性定义Bean时，每次HTTP请求都会创建一个新的Bean，适用于WebApplicationContext环境。

④ session

该属性仅用于HTTP Session，同一个Session共享一个Bean实例。不同Session使用不同的实例。

⑤ global-session

该属性仅用于HTTP Session，同session作用域不同的是，所有的Session共享一个Bean实例。

# Spring框架中的单例Bean是线程安全的么？

​		Spring中的Bean对象默认是单例的，框架并没有对bean进行多线程的封装处理

​		如果Bean是有状态的，那么就需要开发人员自己来保证线程安全的保证，最简单的办法就是改变bean的作用域把singleton改成prototype，这样每次请求bean对象就相当于是创建新的对象来保证线程的安全

​		有状态就是由数据存储的功能

​		无状态就是不会存储数据，你想一下，我们的controller，service和dao本身并不是线程安全的，只是调用里面的方法，而且多线程调用一个实例的方法，会在内存中复制遍历，这是自己线程的工作内存，是最安全的。

​		因此在进行使用的时候，不要在bean中声明任何有状态的实例变量或者类变量，如果必须如此，也推荐大家使用ThreadLocal把变量变成线程私有，如果bean的实例变量或者类变量需要在多个线程之间共享，那么就只能使用synchronized，lock，cas等这些实现线程同步的方法了。

# spring框架中使用了哪些设计模式及应用场景

​		1.工厂模式，在各种BeanFactory以及ApplicationContext创建中都用到了

​		2.模版模式，在各种BeanFactory以及ApplicationContext实现中也都用到了

​		3.代理模式，Spring AOP 利用了 AspectJ AOP实现的! AspectJ AOP 的底层用了动态代理

​		4.策略模式，加载资源文件的方式，使用了不同的方法，比如：ClassPathResourece，FileSystemResource，ServletContextResource，UrlResource但他们都有共同的借口Resource；在Aop的实现中，采用了两种不同的方式，JDK动态代理和CGLIB代理

​		5.单例模式，比如在创建bean的时候。

​		6.观察者模式，spring中的ApplicationEvent，ApplicationListener,ApplicationEventPublisher

​		7.适配器模式，MethodBeforeAdviceAdapter,ThrowsAdviceAdapter,AfterReturningAdapter

​		8.装饰者模式，源码中类型带Wrapper或者Decorator的都是

# spring事务的实现方式原理是什么？

​		在使用Spring框架的时候，可以有两种事务的实现方式，一种是编程式事务，有用户自己通过代码来控制事务的处理逻辑，还有一种是声明式事务，通过@Transactional注解来实现。

​		其实事务的操作本来应该是由数据库来进行控制，但是为了方便用户进行业务逻辑的操作，spring对事务功能进行了扩展实现，一般我们很少会用编程式事务，更多的是通过添加@Transactional注解来进行实现，当添加此注解之后事务的自动功能就会关闭，有spring框架来帮助进行控制。

​		其实事务操作是AOP的一个核心体现，当一个方法添加@Transactional注解之后，spring会基于这个类生成一个代理对象，会将这个代理对象作为bean，当使用这个代理对象的方法的时候，如果有事务处理，那么会先把事务的自动提交给关系，然后去执行具体的业务逻辑，如果执行逻辑没有出现异常，那么代理逻辑就会直接提交，如果出现任何异常情况，那么直接进行回滚操作，当然用户可以控制对哪些异常进行回滚操作。

TransactionInterceptor

# spring事务的隔离级别有哪些？

​	spring中的事务隔离级别就是数据库的隔离级别，有以下几种：

​	read uncommitted

​	read committed

​	repeatable read

​	serializable

​	在进行配置的时候，如果数据库和spring代码中的隔离级别不同，那么以spring的配置为主。

# spring的事务传播机制是什么？

​		多个事务方法相互调用时，事务如何在这些方法之间进行传播,spring中提供了7中不同的传播特性，来保证事务的正常执行：

​		REQUIRED：默认的传播特性，如果当前没有事务，则新建一个事务，如果当前存在事务，则加入这个事务

​		SUPPORTS：当前存在事务，则加入当前事务，如果当前没有事务，则以非事务的方式执行

​		MANDATORY：当前存在事务，则加入当前事务，如果当前事务不存在，则抛出异常

​		REQUIRED_NEW：创建一个新事务，如果存在当前事务，则挂起改事务

​		NOT_SUPPORTED：以非事务方式执行，如果存在当前事务，则挂起当前事务

​		NEVER：不使用事务，如果当前事务存在，则抛出异常

​		NESTED：如果当前事务存在，则在嵌套事务中执行，否则REQUIRED的操作一样

​		NESTED和REQUIRED_NEW的区别：

​		REQUIRED_NEW是新建一个事务并且新开始的这个事务与原有事务无关，而NESTED则是当前存在事务时会开启一个嵌套事务，在NESTED情况下，父事务回滚时，子事务也会回滚，而REQUIRED_NEW情况下，原有事务回滚，不会影响新开启的事务

​		NESTED和REQUIRED的区别：

​		REQUIRED情况下，调用方存在事务时，则被调用方和调用方使用同一个事务，那么被调用方出现异常时，由于共用一个事务，所以无论是否catch异常，事务都会回滚，而在NESTED情况下，被调用方发生异常时，调用方可以catch其异常，这样只有子事务回滚，父事务不会回滚。

# spring事务什么时候会失效？

​		1、bean对象没有被spring容器管理

​		2、方法的访问修饰符不是public

​		3、自身调用问题

​		4、数据源没有配置事务管理器

​		5、数据库不支持事务

​		6、异常被捕获

​		7、异常类型错误或者配置错误

# 什么的是bean的自动装配，它有哪些方式？

​		bean的自动装配指的是bean的属性值在进行注入的时候通过某种特定的规则和方式去容器中查找，并设置到具体的对象属性中，主要有五种方式：

​		no – 缺省情况下，自动配置是通过“ref”属性手动设定，在项目中最常用
​		byName – 根据属性名称自动装配。如果一个bean的名称和其他bean属性的名称是一样的，将会自装配它。
​		byType – 按数据类型自动装配，如果bean的数据类型是用其它bean属性的数据类型，兼容并自动装配它。
​		constructor – 在构造函数参数的byType方式。
​		autodetect – 如果找到默认的构造函数，使用“自动装配用构造”; 否则，使用“按类型自动装配”。

# spring、springmvc、springboot的区别是什么？

​	spring和springMvc：

1. spring是一个一站式的轻量级的java开发框架，核心是控制反转（IOC）和面向切面（AOP），针对于开发的WEB层(springMvc)、业务层(Ioc)、持久层(jdbcTemplate)等都提供了多种配置解决方案；

2. springMvc是spring基础之上的一个MVC框架，主要处理web开发的路径映射和视图渲染，属于spring框架中WEB层开发的一部分；

  springMvc和springBoot：

  1、springMvc属于一个企业WEB开发的MVC框架，涵盖面包括前端视图开发、文件配置、后台接口逻辑开发等，XML、config等配置相对比较繁琐复杂；

  2、springBoot框架相对于springMvc框架来说，更专注于开发微服务后台接口，不开发前端视图，同时遵循默认优于配置，简化了插件配置流程，不需要配置xml，相对springmvc，大大简化了配置流程；

  总结：

  1、Spring 框架就像一个家族，有众多衍生产品例如 boot、security、jpa等等。但他们的基础都是Spring的ioc、aop等. ioc 提供了依赖注入的容器， aop解决了面向横切面编程，然后在此两者的基础上实现了其他延伸产品的高级功能；

  2、springMvc主要解决WEB开发的问题，是基于Servlet 的一个MVC框架，通过XML配置，统一开发前端视图和后端逻辑；

  3、由于Spring的配置非常复杂，各种XML、JavaConfig、servlet处理起来比较繁琐，为了简化开发者的使用，从而创造性地推出了springBoot框架，默认优于配置，简化了springMvc的配置流程；但区别于springMvc的是，springBoot专注于单体微服务接口开发，和前端解耦，虽然springBoot也可以做成springMvc前后台一起开发，但是这就有点不符合springBoot框架的初衷了；

# springmvc工作流程是什么？

​		当发起请求时被前置的控制器拦截到请求，根据请求参数生成代理请求，找到请求对应的实际控制器，控制器处理请求，创建数据模型，访问数据库，将模型响应给中心控制器，控制器使用模型与视图渲染视图结果，将结果返回给中心控制器，再将结果返回给请求者。

![img](C:/Users/y/Desktop/春招笔记/images/springmvc运行流程.jpg)

1、DispatcherServlet表示前置控制器，是整个SpringMVC的控制中心。用户发出请求，DispatcherServlet接收请求并拦截请求。
2、HandlerMapping为处理器映射。DispatcherServlet调用HandlerMapping,HandlerMapping根据请求url查找Handler。
3、返回处理器执行链，根据url查找控制器，并且将解析后的信息传递给DispatcherServlet
4、HandlerAdapter表示处理器适配器，其按照特定的规则去执行Handler。
5、执行handler找到具体的处理器
6、Controller将具体的执行信息返回给HandlerAdapter,如ModelAndView。
7、HandlerAdapter将视图逻辑名或模型传递给DispatcherServlet。
8、DispatcherServlet调用视图解析器(ViewResolver)来解析HandlerAdapter传递的逻辑视图名。
9、视图解析器将解析的逻辑视图名传给DispatcherServlet。
10、DispatcherServlet根据视图解析器解析的视图结果，调用具体的视图，进行试图渲染
11、将响应数据返回给客户端

# springmvc的九大组件有哪些？

1.HandlerMapping
根据request找到相应的处理器。因为Handler（Controller）有两种形式，一种是基于类的Handler，另一种是基于Method的Handler（也就是我们常用的）

2.HandlerAdapter
调用Handler的适配器。如果把Handler（Controller）当做工具的话，那么HandlerAdapter就相当于干活的工人

3.HandlerExceptionResolver
对异常的处理

4.ViewResolver
用来将String类型的视图名和Locale解析为View类型的视图

5.RequestToViewNameTranslator
有的Handler（Controller）处理完后没有设置返回类型，比如是void方法，这是就需要从request中获取viewName

6.LocaleResolver
从request中解析出Locale。Locale表示一个区域，比如zh-cn，对不同的区域的用户，显示不同的结果，这就是i18n（SpringMVC中有具体的拦截器LocaleChangeInterceptor）

7.ThemeResolver
主题解析，这种类似于我们手机更换主题，不同的UI，css等

8.MultipartResolver
处理上传请求，将普通的request封装成MultipartHttpServletRequest

9.FlashMapManager
用于管理FlashMap，FlashMap用于在redirect重定向中传递参数

# springboot自动配置原理是什么？

在之前的课程中我们讲解了springboot的启动过程，其实在面试过程中问的最多的可能是自动装配的原理，而自动装配是在启动过程中完成，只不过在刚开始的时候我们选择性的跳过了，下面详细讲解自动装配的过程。

1、在springboot的启动过程中，有一个步骤是创建上下文，如果不记得可以看下面的代码：

```java
public ConfigurableApplicationContext run(String... args) {
		StopWatch stopWatch = new StopWatch();
		stopWatch.start();
		ConfigurableApplicationContext context = null;
		Collection<SpringBootExceptionReporter> exceptionReporters = new ArrayList<>();
		configureHeadlessProperty();
		SpringApplicationRunListeners listeners = getRunListeners(args);
		listeners.starting();
		try {
			ApplicationArguments applicationArguments = new DefaultApplicationArguments(args);
			ConfigurableEnvironment environment = prepareEnvironment(listeners, applicationArguments);
			configureIgnoreBeanInfo(environment);
			Banner printedBanner = printBanner(environment);
			context = createApplicationContext();
			exceptionReporters = getSpringFactoriesInstances(SpringBootExceptionReporter.class,
					new Class[] { ConfigurableApplicationContext.class }, context);
            //此处完成自动装配的过程
			prepareContext(context, environment, listeners, applicationArguments, printedBanner);
			refreshContext(context);
			afterRefresh(context, applicationArguments);
			stopWatch.stop();
			if (this.logStartupInfo) {
				new StartupInfoLogger(this.mainApplicationClass).logStarted(getApplicationLog(), stopWatch);
			}
			listeners.started(context);
			callRunners(context, applicationArguments);
		}
		catch (Throwable ex) {
			handleRunFailure(context, ex, exceptionReporters, listeners);
			throw new IllegalStateException(ex);
		}

		try {
			listeners.running(context);
		}
		catch (Throwable ex) {
			handleRunFailure(context, ex, exceptionReporters, null);
			throw new IllegalStateException(ex);
		}
		return context;
	}
```

2、在prepareContext方法中查找load方法，一层一层向内点击，找到最终的load方法

```java
//prepareContext方法
	private void prepareContext(ConfigurableApplicationContext context, ConfigurableEnvironment environment,
			SpringApplicationRunListeners listeners, ApplicationArguments applicationArguments, Banner printedBanner) {
		context.setEnvironment(environment);
		postProcessApplicationContext(context);
		applyInitializers(context);
		listeners.contextPrepared(context);
		if (this.logStartupInfo) {
			logStartupInfo(context.getParent() == null);
			logStartupProfileInfo(context);
		}
		// Add boot specific singleton beans
		ConfigurableListableBeanFactory beanFactory = context.getBeanFactory();
		beanFactory.registerSingleton("springApplicationArguments", applicationArguments);
		if (printedBanner != null) {
			beanFactory.registerSingleton("springBootBanner", printedBanner);
		}
		if (beanFactory instanceof DefaultListableBeanFactory) {
			((DefaultListableBeanFactory) beanFactory)
					.setAllowBeanDefinitionOverriding(this.allowBeanDefinitionOverriding);
		}
		if (this.lazyInitialization) {
			context.addBeanFactoryPostProcessor(new LazyInitializationBeanFactoryPostProcessor());
		}
		// Load the sources
		Set<Object> sources = getAllSources();
		Assert.notEmpty(sources, "Sources must not be empty");
        //load方法完成该功能
		load(context, sources.toArray(new Object[0]));
		listeners.contextLoaded(context);
	}


	/**
	 * Load beans into the application context.
	 * @param context the context to load beans into
	 * @param sources the sources to load
	 * 加载bean对象到context中
	 */
	protected void load(ApplicationContext context, Object[] sources) {
		if (logger.isDebugEnabled()) {
			logger.debug("Loading source " + StringUtils.arrayToCommaDelimitedString(sources));
		}
        //获取bean对象定义的加载器
		BeanDefinitionLoader loader = createBeanDefinitionLoader(getBeanDefinitionRegistry(context), sources);
		if (this.beanNameGenerator != null) {
			loader.setBeanNameGenerator(this.beanNameGenerator);
		}
		if (this.resourceLoader != null) {
			loader.setResourceLoader(this.resourceLoader);
		}
		if (this.environment != null) {
			loader.setEnvironment(this.environment);
		}
		loader.load();
	}

	/**
	 * Load the sources into the reader.
	 * @return the number of loaded beans
	 */
	int load() {
		int count = 0;
		for (Object source : this.sources) {
			count += load(source);
		}
		return count;
	}
```

3、实际执行load的是BeanDefinitionLoader中的load方法，如下：

```java
	//实际记载bean的方法
	private int load(Object source) {
		Assert.notNull(source, "Source must not be null");
        //如果是class类型，启用注解类型
		if (source instanceof Class<?>) {
			return load((Class<?>) source);
		}
        //如果是resource类型，启动xml解析
		if (source instanceof Resource) {
			return load((Resource) source);
		}
        //如果是package类型，启用扫描包，例如@ComponentScan
		if (source instanceof Package) {
			return load((Package) source);
		}
        //如果是字符串类型，直接加载
		if (source instanceof CharSequence) {
			return load((CharSequence) source);
		}
		throw new IllegalArgumentException("Invalid source type " + source.getClass());
	}
```

4、下面方法将用来判断是否资源的类型，是使用groovy加载还是使用注解的方式

```java
	private int load(Class<?> source) {
        //判断使用groovy脚本
		if (isGroovyPresent() && GroovyBeanDefinitionSource.class.isAssignableFrom(source)) {
			// Any GroovyLoaders added in beans{} DSL can contribute beans here
			GroovyBeanDefinitionSource loader = BeanUtils.instantiateClass(source, GroovyBeanDefinitionSource.class);
			load(loader);
		}
        //使用注解加载
		if (isComponent(source)) {
			this.annotatedReader.register(source);
			return 1;
		}
		return 0;
	}
```

5、下面方法判断启动类中是否包含@Component注解，但是会神奇的发现我们的启动类中并没有该注解，继续更进发现MergedAnnotations类传入了一个参数SearchStrategy.TYPE_HIERARCHY，会查找继承关系中是否包含这个注解，@SpringBootApplication-->@SpringBootConfiguration-->@Configuration-->@Component,当找到@Component注解之后，会把该对象注册到AnnotatedBeanDefinitionReader对象中

```java
private boolean isComponent(Class<?> type) {
   // This has to be a bit of a guess. The only way to be sure that this type is
   // eligible is to make a bean definition out of it and try to instantiate it.
   if (MergedAnnotations.from(type, SearchStrategy.TYPE_HIERARCHY).isPresent(Component.class)) {
      return true;
   }
   // Nested anonymous classes are not eligible for registration, nor are groovy
   // closures
   return !type.getName().matches(".*\\$_.*closure.*") && !type.isAnonymousClass()
         && type.getConstructors() != null && type.getConstructors().length != 0;
}

	/**
	 * Register a bean from the given bean class, deriving its metadata from
	 * class-declared annotations.
	 * 从给定的bean class中注册一个bean对象，从注解中找到相关的元数据
	 */
	private <T> void doRegisterBean(Class<T> beanClass, @Nullable String name,
			@Nullable Class<? extends Annotation>[] qualifiers, @Nullable Supplier<T> supplier,
			@Nullable BeanDefinitionCustomizer[] customizers) {

		AnnotatedGenericBeanDefinition abd = new AnnotatedGenericBeanDefinition(beanClass);
		if (this.conditionEvaluator.shouldSkip(abd.getMetadata())) {
			return;
		}

		abd.setInstanceSupplier(supplier);
		ScopeMetadata scopeMetadata = this.scopeMetadataResolver.resolveScopeMetadata(abd);
		abd.setScope(scopeMetadata.getScopeName());
		String beanName = (name != null ? name : this.beanNameGenerator.generateBeanName(abd, this.registry));

		AnnotationConfigUtils.processCommonDefinitionAnnotations(abd);
		if (qualifiers != null) {
			for (Class<? extends Annotation> qualifier : qualifiers) {
				if (Primary.class == qualifier) {
					abd.setPrimary(true);
				}
				else if (Lazy.class == qualifier) {
					abd.setLazyInit(true);
				}
				else {
					abd.addQualifier(new AutowireCandidateQualifier(qualifier));
				}
			}
		}
		if (customizers != null) {
			for (BeanDefinitionCustomizer customizer : customizers) {
				customizer.customize(abd);
			}
		}

		BeanDefinitionHolder definitionHolder = new BeanDefinitionHolder(abd, beanName);
		definitionHolder = AnnotationConfigUtils.applyScopedProxyMode(scopeMetadata, definitionHolder, this.registry);
		BeanDefinitionReaderUtils.registerBeanDefinition(definitionHolder, this.registry);
	}

	/**
	 * Register the given bean definition with the given bean factory.
	 * 注册主类，如果有别名可以设置别名
	 */
	public static void registerBeanDefinition(
			BeanDefinitionHolder definitionHolder, BeanDefinitionRegistry registry)
			throws BeanDefinitionStoreException {

		// Register bean definition under primary name.
		String beanName = definitionHolder.getBeanName();
		registry.registerBeanDefinition(beanName, definitionHolder.getBeanDefinition());

		// Register aliases for bean name, if any.
		String[] aliases = definitionHolder.getAliases();
		if (aliases != null) {
			for (String alias : aliases) {
				registry.registerAlias(beanName, alias);
			}
		}
	}

//@SpringBootApplication
@Target(ElementType.TYPE)
@Retention(RetentionPolicy.RUNTIME)
@Documented
@Inherited
@SpringBootConfiguration
@EnableAutoConfiguration
@ComponentScan(excludeFilters = { @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class),
		@Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) })
public @interface SpringBootApplication {}

//@SpringBootConfiguration
@Target(ElementType.TYPE)
@Retention(RetentionPolicy.RUNTIME)
@Documented
@Configuration
public @interface SpringBootConfiguration {}

//@Configuration
@Target(ElementType.TYPE)
@Retention(RetentionPolicy.RUNTIME)
@Documented
@Component
public @interface Configuration {}
```

当看完上述代码之后，只是完成了启动对象的注入，自动装配还没有开始，下面开始进入到自动装配。

6、自动装配入口，从刷新容器开始

```java
@Override
	public void refresh() throws BeansException, IllegalStateException {
		synchronized (this.startupShutdownMonitor) {
			// Prepare this context for refreshing.
			prepareRefresh();

			// Tell the subclass to refresh the internal bean factory.
			ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory();

			// Prepare the bean factory for use in this context.
			prepareBeanFactory(beanFactory);

			try {
				// Allows post-processing of the bean factory in context subclasses.
				postProcessBeanFactory(beanFactory);

				// Invoke factory processors registered as beans in the context.
                // 此处是自动装配的入口
				invokeBeanFactoryPostProcessors(beanFactory);
            }
```

7、在invokeBeanFactoryPostProcessors方法中完成bean的实例化和执行

```java
/**
	 * Instantiate and invoke all registered BeanFactoryPostProcessor beans,
	 * respecting explicit order if given.
	 * <p>Must be called before singleton instantiation.
	 */
	protected void invokeBeanFactoryPostProcessors(ConfigurableListableBeanFactory beanFactory) {
        //开始执行beanFactoryPostProcessor对应实现类,需要知道的是beanFactoryPostProcessor是spring的扩展接口，在刷新容器之前，该接口可以用来修改bean元数据信息
		PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(beanFactory, getBeanFactoryPostProcessors());

		// Detect a LoadTimeWeaver and prepare for weaving, if found in the meantime
		// (e.g. through an @Bean method registered by ConfigurationClassPostProcessor)
		if (beanFactory.getTempClassLoader() == null && beanFactory.containsBean(LOAD_TIME_WEAVER_BEAN_NAME)) {
			beanFactory.addBeanPostProcessor(new LoadTimeWeaverAwareProcessor(beanFactory));
			beanFactory.setTempClassLoader(new ContextTypeMatchClassLoader(beanFactory.getBeanClassLoader()));
		}
	}
```

8、查看invokeBeanFactoryPostProcessors的具体执行方法

```java
	public static void invokeBeanFactoryPostProcessors(
			ConfigurableListableBeanFactory beanFactory, List<BeanFactoryPostProcessor> beanFactoryPostProcessors) {

		// Invoke BeanDefinitionRegistryPostProcessors first, if any.
		Set<String> processedBeans = new HashSet<>();

		if (beanFactory instanceof BeanDefinitionRegistry) {
			BeanDefinitionRegistry registry = (BeanDefinitionRegistry) beanFactory;
			List<BeanFactoryPostProcessor> regularPostProcessors = new ArrayList<>();
			List<BeanDefinitionRegistryPostProcessor> registryProcessors = new ArrayList<>();
			//开始遍历三个内部类，如果属于BeanDefinitionRegistryPostProcessor子类，加入到bean注册的集合，否则加入到regularPostProcessors
			for (BeanFactoryPostProcessor postProcessor : beanFactoryPostProcessors) {
				if (postProcessor instanceof BeanDefinitionRegistryPostProcessor) {
					BeanDefinitionRegistryPostProcessor registryProcessor =
							(BeanDefinitionRegistryPostProcessor) postProcessor;
					registryProcessor.postProcessBeanDefinitionRegistry(registry);
					registryProcessors.add(registryProcessor);
				}
				else {
					regularPostProcessors.add(postProcessor);
				}
			}

			// Do not initialize FactoryBeans here: We need to leave all regular beans
			// uninitialized to let the bean factory post-processors apply to them!
			// Separate between BeanDefinitionRegistryPostProcessors that implement
			// PriorityOrdered, Ordered, and the rest.
			List<BeanDefinitionRegistryPostProcessor> currentRegistryProcessors = new ArrayList<>();

			// First, invoke the BeanDefinitionRegistryPostProcessors that implement PriorityOrdered.
            //通过BeanDefinitionRegistryPostProcessor获取到对应的处理类“org.springframework.context.annotation.internalConfigurationAnnotationProcessor”，但是需要注意的是这个类在springboot中搜索不到，这个类的完全限定名在AnnotationConfigEmbeddedWebApplicationContext中，在进行初始化的时候会装配几个类，在创建AnnotatedBeanDefinitionReader对象的时候会将该类注册到bean对象中，此处可以看到internalConfigurationAnnotationProcessor为bean名称，容器中真正的类是ConfigurationClassPostProcessor
			String[] postProcessorNames =
					beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false);
            //首先执行类型为PriorityOrdered的BeanDefinitionRegistryPostProcessor
            //PriorityOrdered类型表明为优先执行
			for (String ppName : postProcessorNames) {
				if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) {
                    //获取对应的bean
					currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class));
                    //用来存储已经执行过的BeanDefinitionRegistryPostProcessor
					processedBeans.add(ppName);
				}
			}
			sortPostProcessors(currentRegistryProcessors, beanFactory);
			registryProcessors.addAll(currentRegistryProcessors);
            //开始执行装配逻辑
			invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry);
			currentRegistryProcessors.clear();

			// Next, invoke the BeanDefinitionRegistryPostProcessors that implement Ordered.
            //其次执行类型为Ordered的BeanDefinitionRegistryPostProcessor
            //Ordered表明按顺序执行
			postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false);
			for (String ppName : postProcessorNames) {
				if (!processedBeans.contains(ppName) && beanFactory.isTypeMatch(ppName, Ordered.class)) {
					currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class));
					processedBeans.add(ppName);
				}
			}
			sortPostProcessors(currentRegistryProcessors, beanFactory);
			registryProcessors.addAll(currentRegistryProcessors);
			invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry);
			currentRegistryProcessors.clear();

			// Finally, invoke all other BeanDefinitionRegistryPostProcessors until no further ones appear.
            //循环中执行类型不为PriorityOrdered，Ordered类型的BeanDefinitionRegistryPostProcessor
			boolean reiterate = true;
			while (reiterate) {
				reiterate = false;
				postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false);
				for (String ppName : postProcessorNames) {
					if (!processedBeans.contains(ppName)) {
						currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class));
						processedBeans.add(ppName);
						reiterate = true;
					}
				}
				sortPostProcessors(currentRegistryProcessors, beanFactory);
				registryProcessors.addAll(currentRegistryProcessors);
				invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry);
				currentRegistryProcessors.clear();
			}

			// Now, invoke the postProcessBeanFactory callback of all processors handled so far.	
            //执行父类方法，优先执行注册处理类
			invokeBeanFactoryPostProcessors(registryProcessors, beanFactory);
            //执行有规则处理类
			invokeBeanFactoryPostProcessors(regularPostProcessors, beanFactory);
		}

		else {
			// Invoke factory processors registered with the context instance.
			invokeBeanFactoryPostProcessors(beanFactoryPostProcessors, beanFactory);
		}

		// Do not initialize FactoryBeans here: We need to leave all regular beans
		// uninitialized to let the bean factory post-processors apply to them!
		String[] postProcessorNames =
				beanFactory.getBeanNamesForType(BeanFactoryPostProcessor.class, true, false);

		// Separate between BeanFactoryPostProcessors that implement PriorityOrdered,
		// Ordered, and the rest.
		List<BeanFactoryPostProcessor> priorityOrderedPostProcessors = new ArrayList<>();
		List<String> orderedPostProcessorNames = new ArrayList<>();
		List<String> nonOrderedPostProcessorNames = new ArrayList<>();
		for (String ppName : postProcessorNames) {
			if (processedBeans.contains(ppName)) {
				// skip - already processed in first phase above
			}
			else if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) {
				priorityOrderedPostProcessors.add(beanFactory.getBean(ppName, BeanFactoryPostProcessor.class));
			}
			else if (beanFactory.isTypeMatch(ppName, Ordered.class)) {
				orderedPostProcessorNames.add(ppName);
			}
			else {
				nonOrderedPostProcessorNames.add(ppName);
			}
		}

		// First, invoke the BeanFactoryPostProcessors that implement PriorityOrdered.
		sortPostProcessors(priorityOrderedPostProcessors, beanFactory);
		invokeBeanFactoryPostProcessors(priorityOrderedPostProcessors, beanFactory);

		// Next, invoke the BeanFactoryPostProcessors that implement Ordered.
		List<BeanFactoryPostProcessor> orderedPostProcessors = new ArrayList<>(orderedPostProcessorNames.size());
		for (String postProcessorName : orderedPostProcessorNames) {
			orderedPostProcessors.add(beanFactory.getBean(postProcessorName, BeanFactoryPostProcessor.class));
		}
		sortPostProcessors(orderedPostProcessors, beanFactory);
		invokeBeanFactoryPostProcessors(orderedPostProcessors, beanFactory);

		// Finally, invoke all other BeanFactoryPostProcessors.
		List<BeanFactoryPostProcessor> nonOrderedPostProcessors = new ArrayList<>(nonOrderedPostProcessorNames.size());
		for (String postProcessorName : nonOrderedPostProcessorNames) {
			nonOrderedPostProcessors.add(beanFactory.getBean(postProcessorName, BeanFactoryPostProcessor.class));
		}
		invokeBeanFactoryPostProcessors(nonOrderedPostProcessors, beanFactory);

		// Clear cached merged bean definitions since the post-processors might have
		// modified the original metadata, e.g. replacing placeholders in values...
		beanFactory.clearMetadataCache();
	}
```

9、开始执行自动配置逻辑（启动类指定的配置，非默认配置），可以通过debug的方式一层层向里进行查找，会发现最终会在ConfigurationClassParser类中，此类是所有配置类的解析类，所有的解析逻辑在parser.parse(candidates)中

```java
public void parse(Set<BeanDefinitionHolder> configCandidates) {
		for (BeanDefinitionHolder holder : configCandidates) {
			BeanDefinition bd = holder.getBeanDefinition();
			try {
                //是否是注解类
				if (bd instanceof AnnotatedBeanDefinition) {
					parse(((AnnotatedBeanDefinition) bd).getMetadata(), holder.getBeanName());
				}
				else if (bd instanceof AbstractBeanDefinition && ((AbstractBeanDefinition) bd).hasBeanClass()) {
					parse(((AbstractBeanDefinition) bd).getBeanClass(), holder.getBeanName());
				}
				else {
					parse(bd.getBeanClassName(), holder.getBeanName());
				}
			}
			catch (BeanDefinitionStoreException ex) {
				throw ex;
			}
			catch (Throwable ex) {
				throw new BeanDefinitionStoreException(
						"Failed to parse configuration class [" + bd.getBeanClassName() + "]", ex);
			}
		}
    	//执行配置类
		this.deferredImportSelectorHandler.process();
	}
-------------------
    	protected final void parse(AnnotationMetadata metadata, String beanName) throws IOException {
		processConfigurationClass(new ConfigurationClass(metadata, beanName));
	}
-------------------
    protected void processConfigurationClass(ConfigurationClass configClass) throws IOException {
		if (this.conditionEvaluator.shouldSkip(configClass.getMetadata(), ConfigurationPhase.PARSE_CONFIGURATION)) {
			return;
		}

		ConfigurationClass existingClass = this.configurationClasses.get(configClass);
		if (existingClass != null) {
			if (configClass.isImported()) {
				if (existingClass.isImported()) {
					existingClass.mergeImportedBy(configClass);
				}
				// Otherwise ignore new imported config class; existing non-imported class overrides it.
				return;
			}
			else {
				// Explicit bean definition found, probably replacing an import.
				// Let's remove the old one and go with the new one.
				this.configurationClasses.remove(configClass);
				this.knownSuperclasses.values().removeIf(configClass::equals);
			}
		}

		// Recursively process the configuration class and its superclass hierarchy.
		SourceClass sourceClass = asSourceClass(configClass);
		do {
            //循环处理bean,如果有父类，则处理父类，直至结束
			sourceClass = doProcessConfigurationClass(configClass, sourceClass);
		}
		while (sourceClass != null);

		this.configurationClasses.put(configClass, configClass);
	}
```

10、继续跟进doProcessConfigurationClass方法，此方式是支持注解配置的核心逻辑

```java
/**
	 * Apply processing and build a complete {@link ConfigurationClass} by reading the
	 * annotations, members and methods from the source class. This method can be called
	 * multiple times as relevant sources are discovered.
	 * @param configClass the configuration class being build
	 * @param sourceClass a source class
	 * @return the superclass, or {@code null} if none found or previously processed
	 */
	@Nullable
	protected final SourceClass doProcessConfigurationClass(ConfigurationClass configClass, SourceClass sourceClass)
			throws IOException {

        //处理内部类逻辑，由于传来的参数是启动类，并不包含内部类，所以跳过
		if (configClass.getMetadata().isAnnotated(Component.class.getName())) {
			// Recursively process any member (nested) classes first
			processMemberClasses(configClass, sourceClass);
		}

		// Process any @PropertySource annotations
        //针对属性配置的解析
		for (AnnotationAttributes propertySource : AnnotationConfigUtils.attributesForRepeatable(
				sourceClass.getMetadata(), PropertySources.class,
				org.springframework.context.annotation.PropertySource.class)) {
			if (this.environment instanceof ConfigurableEnvironment) {
				processPropertySource(propertySource);
			}
			else {
				logger.info("Ignoring @PropertySource annotation on [" + sourceClass.getMetadata().getClassName() +
						"]. Reason: Environment must implement ConfigurableEnvironment");
			}
		}

		// Process any @ComponentScan annotations
        // 这里是根据启动类@ComponentScan注解来扫描项目中的bean
		Set<AnnotationAttributes> componentScans = AnnotationConfigUtils.attributesForRepeatable(
				sourceClass.getMetadata(), ComponentScans.class, ComponentScan.class);
		if (!componentScans.isEmpty() &&
				!this.conditionEvaluator.shouldSkip(sourceClass.getMetadata(), ConfigurationPhase.REGISTER_BEAN)) {
            
			for (AnnotationAttributes componentScan : componentScans) {
				// The config class is annotated with @ComponentScan -> perform the scan immediately
                //遍历项目中的bean，如果是注解定义的bean，则进一步解析
				Set<BeanDefinitionHolder> scannedBeanDefinitions =
						this.componentScanParser.parse(componentScan, sourceClass.getMetadata().getClassName());
				// Check the set of scanned definitions for any further config classes and parse recursively if needed
				for (BeanDefinitionHolder holder : scannedBeanDefinitions) {
					BeanDefinition bdCand = holder.getBeanDefinition().getOriginatingBeanDefinition();
					if (bdCand == null) {
						bdCand = holder.getBeanDefinition();
					}
					if (ConfigurationClassUtils.checkConfigurationClassCandidate(bdCand, this.metadataReaderFactory)) {
                        //递归解析，所有的bean,如果有注解，会进一步解析注解中包含的bean
						parse(bdCand.getBeanClassName(), holder.getBeanName());
					}
				}
			}
		}

		// Process any @Import annotations
        //递归解析，获取导入的配置类，很多情况下，导入的配置类中会同样包含导入类注解
		processImports(configClass, sourceClass, getImports(sourceClass), true);

		// Process any @ImportResource annotations
        //解析@ImportResource配置类
		AnnotationAttributes importResource =
				AnnotationConfigUtils.attributesFor(sourceClass.getMetadata(), ImportResource.class);
		if (importResource != null) {
			String[] resources = importResource.getStringArray("locations");
			Class<? extends BeanDefinitionReader> readerClass = importResource.getClass("reader");
			for (String resource : resources) {
				String resolvedResource = this.environment.resolveRequiredPlaceholders(resource);
				configClass.addImportedResource(resolvedResource, readerClass);
			}
		}

		// Process individual @Bean methods
        //处理@Bean注解修饰的类
		Set<MethodMetadata> beanMethods = retrieveBeanMethodMetadata(sourceClass);
		for (MethodMetadata methodMetadata : beanMethods) {
			configClass.addBeanMethod(new BeanMethod(methodMetadata, configClass));
		}

		// Process default methods on interfaces
        // 处理接口中的默认方法
		processInterfaces(configClass, sourceClass);

		// Process superclass, if any
        //如果该类有父类，则继续返回，上层方法判断不为空，则继续递归执行
		if (sourceClass.getMetadata().hasSuperClass()) {
			String superclass = sourceClass.getMetadata().getSuperClassName();
			if (superclass != null && !superclass.startsWith("java") &&
					!this.knownSuperclasses.containsKey(superclass)) {
				this.knownSuperclasses.put(superclass, configClass);
				// Superclass found, return its annotation metadata and recurse
				return sourceClass.getSuperClass();
			}
		}

		// No superclass -> processing is complete
		return null;
	}

```

11、查看获取配置类的逻辑

```java
processImports(configClass, sourceClass, getImports(sourceClass), true);

	/**
	 * Returns {@code @Import} class, considering all meta-annotations.
	 */
	private Set<SourceClass> getImports(SourceClass sourceClass) throws IOException {
		Set<SourceClass> imports = new LinkedHashSet<>();
		Set<SourceClass> visited = new LinkedHashSet<>();
		collectImports(sourceClass, imports, visited);
		return imports;
	}
------------------
    	/**
	 * Recursively collect all declared {@code @Import} values. Unlike most
	 * meta-annotations it is valid to have several {@code @Import}s declared with
	 * different values; the usual process of returning values from the first
	 * meta-annotation on a class is not sufficient.
	 * <p>For example, it is common for a {@code @Configuration} class to declare direct
	 * {@code @Import}s in addition to meta-imports originating from an {@code @Enable}
	 * annotation.
	 * 看到所有的bean都以导入的方式被加载进去
	 */
	private void collectImports(SourceClass sourceClass, Set<SourceClass> imports, Set<SourceClass> visited)
			throws IOException {

		if (visited.add(sourceClass)) {
			for (SourceClass annotation : sourceClass.getAnnotations()) {
				String annName = annotation.getMetadata().getClassName();
				if (!annName.equals(Import.class.getName())) {
					collectImports(annotation, imports, visited);
				}
			}
			imports.addAll(sourceClass.getAnnotationAttributes(Import.class.getName(), "value"));
		}
	}
```

12、继续回到ConfigurationClassParser中的parse方法中的最后一行,继续跟进该方法：

```java
this.deferredImportSelectorHandler.process()
-------------
public void process() {
			List<DeferredImportSelectorHolder> deferredImports = this.deferredImportSelectors;
			this.deferredImportSelectors = null;
			try {
				if (deferredImports != null) {
					DeferredImportSelectorGroupingHandler handler = new DeferredImportSelectorGroupingHandler();
					deferredImports.sort(DEFERRED_IMPORT_COMPARATOR);
					deferredImports.forEach(handler::register);
					handler.processGroupImports();
				}
			}
			finally {
				this.deferredImportSelectors = new ArrayList<>();
			}
		}
---------------
  public void processGroupImports() {
			for (DeferredImportSelectorGrouping grouping : this.groupings.values()) {
				grouping.getImports().forEach(entry -> {
					ConfigurationClass configurationClass = this.configurationClasses.get(
							entry.getMetadata());
					try {
						processImports(configurationClass, asSourceClass(configurationClass),
								asSourceClasses(entry.getImportClassName()), false);
					}
					catch (BeanDefinitionStoreException ex) {
						throw ex;
					}
					catch (Throwable ex) {
						throw new BeanDefinitionStoreException(
								"Failed to process import candidates for configuration class [" +
										configurationClass.getMetadata().getClassName() + "]", ex);
					}
				});
			}
		}
------------
    /**
		 * Return the imports defined by the group.
		 * @return each import with its associated configuration class
		 */
		public Iterable<Group.Entry> getImports() {
			for (DeferredImportSelectorHolder deferredImport : this.deferredImports) {
				this.group.process(deferredImport.getConfigurationClass().getMetadata(),
						deferredImport.getImportSelector());
			}
			return this.group.selectImports();
		}
	}
------------
    public DeferredImportSelector getImportSelector() {
			return this.importSelector;
		}
------------
    @Override
		public void process(AnnotationMetadata annotationMetadata, DeferredImportSelector deferredImportSelector) {
			Assert.state(deferredImportSelector instanceof AutoConfigurationImportSelector,
					() -> String.format("Only %s implementations are supported, got %s",
							AutoConfigurationImportSelector.class.getSimpleName(),
							deferredImportSelector.getClass().getName()));
			AutoConfigurationEntry autoConfigurationEntry = ((AutoConfigurationImportSelector) deferredImportSelector)
					.getAutoConfigurationEntry(getAutoConfigurationMetadata(), annotationMetadata);
			this.autoConfigurationEntries.add(autoConfigurationEntry);
			for (String importClassName : autoConfigurationEntry.getConfigurations()) {
				this.entries.putIfAbsent(importClassName, annotationMetadata);
			}
		}

```



# 如何理解springboot中的starter？

​		使用spring+springmvc框架进行开发的时候，如果需要引入mybatis框架，那么需要在xml中定义需要的bean对象，这个过程很明显是很麻烦的，如果需要引入额外的其他组件，那么也需要进行复杂的配置，因此在springboot中引入了starter

​		starter就是一个jar包，写一个@Configuration的配置类，将这些bean定义在其中，然后再starter包的META-INF/spring.factories中写入配置类，那么springboot程序在启动的时候就会按照约定来加载该配置类

​		开发人员只需要将相应的starter包依赖进应用中，进行相关的属性配置，就可以进行代码开发，而不需要单独进行bean对象的配置

# 什么是嵌入式服务器，为什么使用嵌入式服务器？

​		在springboot框架中，大家应该发现了有一个内嵌的tomcat，在之前的开发流程中，每次写好代码之后必须要将项目部署到一个额外的web服务器中，只有这样才可以运行，这个明显要麻烦很多，而使用springboot的时候，你会发现在启动项目的时候可以直接按照java应用程序的方式来启动项目，不需要额外的环境支持，也不需要tomcat服务器，这是因为在springboot框架中内置了tomcat.jar，来通过main方法启动容器，达到一键开发部署的方式，不需要额外的任何其他操作。

# mybatis的优缺点有哪些？

1、Mybait的优点：

（1）简单易学，容易上手（相比于Hibernate）  基于SQL编程；
（2）JDBC相比，减少了50%以上的代码量，消除了JDBC大量冗余的代码，不需要手动开关连接；
（3）很好的与各种数据库兼容（因为MyBatis使用JDBC来连接数据库，所以只要JDBC支持的数据库MyBatis都支持，而JDBC提供了可扩展性，所以只要这个数据库有针对Java的jar包就可以就可以与MyBatis兼容），开发人员不需要考虑数据库的差异性。
（4）提供了很多第三方插件（分页插件 / 逆向工程）；
（5）能够与Spring很好的集成；
（6）MyBatis相当灵活，不会对应用程序或者数据库的现有设计强加任何影响，SQL写在XML里，从程序代码中彻底分离，解除sql与程序代码的耦合，便于统一管理和优化，并可重用。
（7）提供XML标签，支持编写动态SQL语句。
（8）提供映射标签，支持对象与数据库的ORM字段关系映射。
（9）提供对象关系映射标签，支持对象关系组建维护。
2、MyBatis框架的缺点：

（1）SQL语句的编写工作量较大，尤其是字段多、关联表多时，更是如此，对开发人员编写SQL语句的功底有一定要求。
（2）SQL语句依赖于数据库，导致数据库移植性差，不能随意更换数据库。

# mybatis和hibernate有什么区别？

Hibernate的优点：

1、hibernate是全自动，hibernate完全可以通过对象关系模型实现对数据库的操作，拥有完整的JavaBean对象与数据库的映射结构来自动生成sql。

2、功能强大，数据库无关性好，O/R映射能力强，需要写的代码很少，开发速度很快。

3、有更好的二级缓存机制，可以使用第三方缓存。

4、数据库移植性良好。

5、hibernate拥有完整的日志系统，hibernate日志系统非常健全，涉及广泛，包括sql记录、关系异常、优化警告、缓存提示、脏数据警告等

Hibernate的缺点：

1、学习门槛高，精通门槛更高，程序员如何设计O/R映射，在性能和对象模型之间如何取得平衡，以及怎样用好Hibernate方面需要的经验和能力都很强才行

2、hibernate的sql很多都是自动生成的，无法直接维护sql；虽然有hql查询，但功能还是不及sql强大，见到报表等变态需求时，hql查询要虚，也就是说hql查询是有局限的；hibernate虽然也支持原生sql查询，但开发模式上却与orm不同，需要转换思维，因此使用上有些不方便。总之写sql的灵活度上hibernate不及mybatis。

Mybatis的优点：

1、易于上手和掌握，提供了数据库查询的自动对象绑定功能，而且延续了很好的SQL使用经验，对于没有那么高的对象模型要求的项目来说，相当完美。

2、sql写在xml里，便于统一管理和优化， 解除sql与程序代码的耦合。

3、提供映射标签，支持对象与数据库的orm字段关系映射

4、 提供对象关系映射标签，支持对象关系组建维护

5、提供xml标签，支持编写动态sql。

6、速度相对于Hibernate的速度较快

Mybatis的缺点：

1、关联表多时，字段多的时候，sql工作量很大。

2、sql依赖于数据库，导致数据库移植性差。

3、由于xml里标签id必须唯一，导致DAO中方法不支持方法重载。

4、对象关系映射标签和字段映射标签仅仅是对映射关系的描述，具体实现仍然依赖于sql。

5、DAO层过于简单，对象组装的工作量较大。

6、不支持级联更新、级联删除。

7、Mybatis的日志除了基本记录功能外，其它功能薄弱很多。

8、编写动态sql时,不方便调试，尤其逻辑复杂时。

9、提供的写动态sql的xml标签功能简单，编写动态sql仍然受限，且可读性低。

# mybatis中#{}和${}的区别是什么？

a、#{}是预编译处理，${}是字符串替换。
b、Mybatis 在处理#{}时，会将 sql 中的#{}替换为?号，调用 PreparedStatement 的 set 方法来赋值；
c、Mybatis 在处理${}时，就是把${}替换成变量的值。
d、使用#{}可以有效的防止 SQL 注入，提高系统安全性

# 简述一下mybatis插件运行原理及开发流程？

mybatis只支持针对ParameterHandler、ResultSetHandler、StatementHandler、Executor这四种接口的插件，mybatis使用jdk的动态代理，为需要拦截的接口生成代理对象以实现接口方法拦截功能，每当执行这四种接口对象的方法时，就会进入拦截方法，具体就是InvocationHandler的invoke方法，拦截那些你指定需要拦截的方法。

编写插件：实现Mybatis的Interceptor接口并复写intercept方法啊，然后给插件编写注解，指定要拦截哪一个接口的哪些方法，在配置文件中配置编写的插件即可。

```java
@Intercepts({@Signature(type = StatementHandler.class,method = "parameterize",args = Statement.class)})
```

索引的基本原理

1、为什么要有索引?
一般的应用系统，读写比例在10:1左右，而且插入操作和一般的更新操作很少出现性能问题，在生产环境中，我们遇到最多的，也是最容易出问题的，还是一些复杂的查询操作，因此对查询语句的优化显然是重中之重。说起加速查询，就不得不提到索引了。
2、什么是索引？
索引在MySQL中也叫是一种“键”，是存储引擎用于快速找到记录的一种数据结构。索引对于良好的性能
非常关键，尤其是当表中的数据量越来越大时，索引对于性能的影响愈发重要。
索引优化应该是对查询性能优化最有效的手段了。索引能够轻易将查询性能提高好几个数量级。
索引相当于字典的音序表，如果要查某个字，如果不使用音序表，则需要从几百页中逐页去查。	

3、索引的原理

索引的目的在于提高查询效率，与我们查阅图书所用的目录是一个道理：先定位到章，然后定位到该章下的一个小节，然后找到页数。相似的例子还有：查字典，查火车车次，飞机航班等

本质都是：通过不断地缩小想要获取数据的范围来筛选出最终想要的结果，同时把随机的事件变成顺序的事件，也就是说，有了这种索引机制，我们可以总是用同一种查找方式来锁定数据。

数据库也是一样，但显然要复杂的多，因为不仅面临着等值查询，还有范围查询(>、<、between、in)、模糊查询(like)、并集查询(or)等等。数据库应该选择怎么样的方式来应对所有的问题呢？我们回想字典的例子，能不能把数据分成段，然后分段查询呢？最简单的如果1000条数据，1到100分成第一段，101到200分成第二段，201到300分成第三段…这样查第250条数据，只要找第三段就可以了，一下子去除了90%的无效数据。但如果是1千万的记录呢，分成几段比较好？按照搜索树的模型，其平均复杂度是lgN，具有不错的查询性能。但这里我们忽略了一个关键的问题，复杂度模型是基于每次相同的操作成本来考虑的。而数据库实现比较复杂，一方面数据是保存在磁盘上的，另外一方面为了提高性能，每次又可以把部分数据读入内存来计算，因为我们知道访问磁盘的成本大概是访问内存的十万倍左右，所以简单的搜索树难以满足复杂的应用场景。

4、索引的数据结构

MySQL主要用到两种结构：B+ Tree索引和Hash索引
Inodb存储引擎 默认是 B+Tree索引
Memory 存储引擎 默认 Hash索引；
MySQL中，只有Memory(Memory表只存在内存中，断电会消失，适用于临时表)存储引擎显示支持Hash索引，是Memory表的默认索引类型，尽管Memory表也可以使用B+Tree索引。Hash索引把数据以hash形式组织起来，因此当查找某一条记录的时候，速度非常快。但是因为hash结构，每个键只对应一个值，而且是散列的方式分布。所以它并不支持范围查找和排序等功能。
B+Tree是mysql使用最频繁的一个索引数据结构，是InnoDB和MyISAM存储引擎模式的索引类型。相对Hash索引，B+Tree在查找单条记录的速度比不上Hash索引，但是因为更适合排序等操作，所以它更受欢迎。毕竟不可能只对数据库进行单条记录的操作。
对比：
hash类型的索引：查询单条快，范围查询慢
btree类型的索引：b+树，层数越多，数据量指数级增长（我们就用它，因为innodb默认支持它）

# mysql聚簇和非聚簇索引的区别是什么？

​		mysql的索引类型跟存储引擎是相关的，innodb存储引擎数据文件跟索引文件全部放在ibd文件中，而myisam的数据文件放在myd文件中，索引放在myi文件中，其实区分聚簇索引和非聚簇索引非常简单，只要判断数据跟索引是否存储在一起就可以了。

​		innodb存储引擎在进行数据插入的时候，数据必须要跟索引放在一起，如果有主键就使用主键，没有主键就使用唯一键，没有唯一键就使用6字节的rowid，因此跟数据绑定在一起的就是聚簇索引，而为了避免数据冗余存储，其他的索引的叶子节点中存储的都是聚簇索引的key值，因此innodb中既有聚簇索引也有非聚簇索引，而myisam中只有非聚簇索引。

# mysql索引结构有哪些，各自的优劣是什么？

索引的数据结构和具体存储引擎的实现有关，mysql中使用较多的索引有hash索引，B+树索引，innodb的索引实现为B+树，memory存储引擎为hash索引。

B+树是一个平衡的多叉树，从根节点到每个叶子节点的高度差值不超过1，而且同层级的二节点间有指针相关连接，在B+树上的常规检索，从根节点到叶子节点的搜索效率基本相当，不会出现大幅波动，而且基于索引的顺序扫描时，也可以利用双向指针快速左右移动，效率非常高。因为，B+树索引被广泛应用于数据库、文件系统等场景。

哈希索引就是采用一定的哈希算法，把键值换算成新的哈希值，检索时不需要类似B+树那样从根节点到叶子节点逐级查找，只需一次哈希算法即可立刻定位到相应的位置，速度非常快。

如果是等值查询，那么哈希索引明显有绝对优势，因为只需要经过一次算法即可找到相应的键值，前提是键值都是唯一的。如果键值不是唯一的，就需要先找到该键所在位置，然后再根据链表往后扫描，知道找到对应的数据

如果是范围查询检索，这时候哈徐索引就毫无用武之地了，因为原先是有序的键值，经过哈希算法后，有可能变成不连续的了，就没办法再利用索引完成范围查询检索

哈希所有也没办法利用索引完成排序，以及like这样的部分模糊查询

哈希索引也不支持多列联合索引的最左匹配规则

B+树索引的关键字检索效率比较平均，不像B树那样波动大，在有大量重复键值情况下，哈希索引的效率也是极低的，因此存在哈希碰撞问题。

# 索引的设计原则有哪些？

​		在进行索引设计的时候，应该保证索引字段占用的空间越小越好，这只是一个大的方向，还有一些细节点需要注意下：

​		1、适合索引的列是出现在where字句中的列，或者连接子句中指定的列

​		2、基数较小的表，索引效果差，没必要创建索引

​		3、在选择索引列的时候，越短越好，可以指定某些列的一部分，没必要用全部字段的值

​		4、不要给表中的每一个字段都创建索引，并不是索引越多越好

​		5、定义有外键的数据列一定要创建索引

​		6、更新频繁的字段不要有索引

​		7、创建索引的列不要过多，可以创建组合索引，但是组合索引的列的个数不建议太多

​		8、大文本、大对象不要创建索引

# mysql锁的类型有哪些？

基于锁的属性分类：共享锁、排他锁。

基于锁的粒度分类：行级锁（innodb ）、表级锁（ innodb 、myisam）、页级锁（ innodb引擎）、记录锁、间隙锁、临键锁。

基于锁的状态分类：意向共享锁、意向排它锁。 

 共享锁（share lock）： 共享锁又称读锁，简称 S 锁；当一个事务为数据加上读锁之后，其他事务只能对该数据加读锁，而不能对数据加写锁，直到所有的读锁释放之后其他事务才能对其进行加持写锁。共享锁的特性主要是为了支持并发的读取数据，读取数据的时候不支持修改，避免出现重复读的问题。

 排他锁（exclusive lock）：排他锁又称写锁，简称 X 锁；当一个事务为数据加上写锁时，其他请求将不能再为数据加任何锁，直到该锁释放之后，其他事务才能对数据进行加锁。排他锁的目的是在数据修改时候，不允许其他人同时修改，也不允许其他人读取，避免了出现脏数据和脏读的问题。

表锁（table lock）：表锁是指上锁的时候锁住的是整个表，当下一个事务访问该表的时候，必须等前一个事务释放了锁才能进行对表进行访问；特点：粒度大，加锁简单，容易冲突；

行锁：行锁是指上锁的时候锁住的是表的某一行或多行记录，其他事务访问同一张表时，只有被锁住的记录不能访问，其他的记录可正常访问，特点：粒度小，加锁比表锁麻烦，不容易冲突，相比表锁支持的并发要高

记录锁（Record lock）:记录锁也属于行锁中的一种，只不过记录锁的范围只是表中的某一条记录，记录锁是说事务在加锁后锁住的只是表的某一条记录，加了记录锁之后数据可以避免数据在查询的时候被修改的重复读问题，也避免了在修改的事务未提交前被其他事务读取的脏读问题

页锁：页级锁是 MysQL 中锁定粒度介于行级锁和表级锁中间的一种锁．表级锁速度快，但冲突多，行级冲突少，但速度慢。所以取了折衷的页级，一次锁定相邻的一组记录。特点：开销和加锁时间界于表锁和行锁之间，会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。

间隙锁：是属于行锁的一种，间隙锁是在事务加锁后其锁住的是表记录的某一个区间，当表的相邻ID之间出现空隙则会形成一个区间，遵循左开右闭原则。范围查询并且查询未命中记录，查询条件必须命中索引、间隙锁只会出现在REPEATABLE_READ（重复读）的事务级别中。

临键锁（Next-Key lock)：也属于行锁的一种，并且它是INNODB的行锁默认算法，总结来说它就是记录锁和间隙锁的组合，临键锁会把查询出来的记录锁住，同时也会把该范围查询内的所有间隙空间也会锁住，再之它会把相邻的下一个区间也会锁住。

# mysql执行计划怎么看？

​       在企业的应用场景中，为了知道优化SQL语句的执行，需要查看SQL语句的具体执行过程，以加快SQL语句的执行效率。

​       可以使用explain+SQL语句来模拟优化器执行SQL查询语句，从而知道mysql是如何处理sql语句的。

​	   官网地址： https://dev.mysql.com/doc/refman/5.7/en/explain-output.html 

1、执行计划中包含的信息

| Column        | Meaning                                        |
| ------------- | ---------------------------------------------- |
| id            | The `SELECT` identifier                        |
| select_type   | The `SELECT` type                              |
| table         | The table for the output row                   |
| partitions    | The matching partitions                        |
| type          | The join type                                  |
| possible_keys | The possible indexes to choose                 |
| key           | The index actually chosen                      |
| key_len       | The length of the chosen key                   |
| ref           | The columns compared to the index              |
| rows          | Estimate of rows to be examined                |
| filtered      | Percentage of rows filtered by table condition |
| extra         | Additional information                         |

**id**

select查询的序列号，包含一组数字，表示查询中执行select子句或者操作表的顺序

id号分为三种情况：

​		1、如果id相同，那么执行顺序从上到下

```sql
explain select * from emp e join dept d on e.deptno = d.deptno join salgrade sg on e.sal between sg.losal and sg.hisal;
```

​		2、如果id不同，如果是子查询，id的序号会递增，id值越大优先级越高，越先被执行

```sql
explain select * from emp e where e.deptno in (select d.deptno from dept d where d.dname = 'SALES');
```

​		3、id相同和不同的，同时存在：相同的可以认为是一组，从上往下顺序执行，在所有组中，id值越大，优先级越高，越先执行

```sql
explain select * from emp e join dept d on e.deptno = d.deptno join salgrade sg on e.sal between sg.losal and sg.hisal where e.deptno in (select d.deptno from dept d where d.dname = 'SALES');
```

**select_type**

主要用来分辨查询的类型，是普通查询还是联合查询还是子查询

| `select_type` Value  | Meaning                                                      |
| -------------------- | ------------------------------------------------------------ |
| SIMPLE               | Simple SELECT (not using UNION or subqueries)                |
| PRIMARY              | Outermost SELECT                                             |
| UNION                | Second or later SELECT statement in a UNION                  |
| DEPENDENT UNION      | Second or later SELECT statement in a UNION, dependent on outer query |
| UNION RESULT         | Result of a UNION.                                           |
| SUBQUERY             | First SELECT in subquery                                     |
| DEPENDENT SUBQUERY   | First SELECT in subquery, dependent on outer query           |
| DERIVED              | Derived table                                                |
| UNCACHEABLE SUBQUERY | A subquery for which the result cannot be cached and must be re-evaluated for each row of the outer query |
| UNCACHEABLE UNION    | The second or later select in a UNION that belongs to an uncacheable subquery (see UNCACHEABLE SUBQUERY) |

```sql
--sample:简单的查询，不包含子查询和union
explain select * from emp;

--primary:查询中若包含任何复杂的子查询，最外层查询则被标记为Primary
explain select staname,ename supname from (select ename staname,mgr from emp) t join emp on t.mgr=emp.empno ;

--union:若第二个select出现在union之后，则被标记为union
explain select * from emp where deptno = 10 union select * from emp where sal >2000;

--dependent union:跟union类似，此处的depentent表示union或union all联合而成的结果会受外部表影响
explain select * from emp e where e.empno  in ( select empno from emp where deptno = 10 union select empno from emp where sal >2000)

--union result:从union表获取结果的select
explain select * from emp where deptno = 10 union select * from emp where sal >2000;

--subquery:在select或者where列表中包含子查询
explain select * from emp where sal > (select avg(sal) from emp) ;

--dependent subquery:subquery的子查询要受到外部表查询的影响
explain select * from emp e where e.deptno in (select distinct deptno from dept);

--DERIVED: from子句中出现的子查询，也叫做派生类，
explain select staname,ename supname from (select ename staname,mgr from emp) t join emp on t.mgr=emp.empno ;

--UNCACHEABLE SUBQUERY：表示使用子查询的结果不能被缓存
 explain select * from emp where empno = (select empno from emp where deptno=@@sort_buffer_size);
 
--uncacheable union:表示union的查询结果不能被缓存：sql语句未验证
```

**table**

对应行正在访问哪一个表，表名或者别名，可能是临时表或者union合并结果集
		1、如果是具体的表名，则表明从实际的物理表中获取数据，当然也可以是表的别名

​		2、表名是derivedN的形式，表示使用了id为N的查询产生的衍生表

​		3、当有union result的时候，表名是union n1,n2等的形式，n1,n2表示参与union的id

**type**

type显示的是访问类型，访问类型表示我是以何种方式去访问我们的数据，最容易想的是全表扫描，直接暴力的遍历一张表去寻找需要的数据，效率非常低下，访问的类型有很多，效率从最好到最坏依次是：

system > const > eq_ref > ref > fulltext > ref_or_null > index_merge > unique_subquery > index_subquery > range > index > ALL 

一般情况下，得保证查询至少达到range级别，最好能达到ref

```sql
--all:全表扫描，一般情况下出现这样的sql语句而且数据量比较大的话那么就需要进行优化。
explain select * from emp;

--index：全索引扫描这个比all的效率要好，主要有两种情况，一种是当前的查询时覆盖索引，即我们需要的数据在索引中就可以索取，或者是使用了索引进行排序，这样就避免数据的重排序
explain  select empno from emp;

--range：表示利用索引查询的时候限制了范围，在指定范围内进行查询，这样避免了index的全索引扫描，适用的操作符： =, <>, >, >=, <, <=, IS NULL, BETWEEN, LIKE, or IN() 
explain select * from emp where empno between 7000 and 7500;

--index_subquery：利用索引来关联子查询，不再扫描全表
explain select * from emp where emp.job in (select job from t_job);

--unique_subquery:该连接类型类似与index_subquery,使用的是唯一索引
 explain select * from emp e where e.deptno in (select distinct deptno from dept);
 
--index_merge：在查询过程中需要多个索引组合使用，没有模拟出来
explain select * from rental where rental_date like '2005-05-26 07:12:2%' and inventory_id=3926 and customer_id=321\G

--ref_or_null：对于某个字段即需要关联条件，也需要null值的情况下，查询优化器会选择这种访问方式
explain select * from emp e where  e.mgr is null or e.mgr=7369;

--ref：使用了非唯一性索引进行数据的查找
 create index idx_3 on emp(deptno);
 explain select * from emp e,dept d where e.deptno =d.deptno;

--eq_ref ：使用唯一性索引进行数据查找
explain select * from emp,emp2 where emp.empno = emp2.empno;

--const：这个表至多有一个匹配行，
explain select * from emp where empno = 7369;
 
--system：表只有一行记录（等于系统表），这是const类型的特例，平时不会出现
```

 **possible_keys** 

​        显示可能应用在这张表中的索引，一个或多个，查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询实际使用

```sql
explain select * from emp,dept where emp.deptno = dept.deptno and emp.deptno = 10;
```

**key**

​		实际使用的索引，如果为null，则没有使用索引，查询中若使用了覆盖索引，则该索引和查询的select字段重叠。

```sql
explain select * from emp,dept where emp.deptno = dept.deptno and emp.deptno = 10;
```

**key_len**

表示索引中使用的字节数，可以通过key_len计算查询中使用的索引长度，在不损失精度的情况下长度越短越好。

```sql
explain select * from emp,dept where emp.deptno = dept.deptno and emp.deptno = 10;
```

**ref**

显示索引的哪一列被使用了，如果可能的话，是一个常数

```sql
explain select * from emp,dept where emp.deptno = dept.deptno and emp.deptno = 10;
```

**rows**

根据表的统计信息及索引使用情况，大致估算出找出所需记录需要读取的行数，此参数很重要，直接反应的sql找了多少数据，在完成目的的情况下越少越好

```sql
explain select * from emp;
```

**extra**

包含额外的信息。

```sql
--using filesort:说明mysql无法利用索引进行排序，只能利用排序算法进行排序，会消耗额外的位置
explain select * from emp order by sal;

--using temporary:建立临时表来保存中间结果，查询完成之后把临时表删除
explain select ename,count(*) from emp where deptno = 10 group by ename;

--using index:这个表示当前的查询时覆盖索引的，直接从索引中读取数据，而不用访问数据表。如果同时出现using where 表名索引被用来执行索引键值的查找，如果没有，表面索引被用来读取数据，而不是真的查找
explain select deptno,count(*) from emp group by deptno limit 10;

--using where:使用where进行条件过滤
explain select * from t_user where id = 1;

--using join buffer:使用连接缓存，情况没有模拟出来

--impossible where：where语句的结果总是false
explain select * from emp where empno = 7469;
```

# 事务的基本特性是什么？

事务四大特征：原子性，一致性，隔离性和持久性。

1. 原子性（Atomicity）
   一个原子事务要么完整执行，要么干脆不执行。这意味着，工作单元中的每项任务都必须正确执行。如果有任一任务执行失败，则整个工作单元或事务就会被终止。即此前对数据所作的任何修改都将被撤销。如果所有任务都被成功执行，事务就会被提交，即对数据所作的修改将会是永久性的。
2. 一致性（Consistency）
   一致性代表了底层数据存储的完整性。它必须由事务系统和应用开发人员共同来保证。事务系统通过保证事务的原子性，隔离性和持久性来满足这一要求; 应用开发人员则需要保证数据库有适当的约束(主键，引用完整性等)，并且工作单元中所实现的业务逻辑不会导致数据的不一致(即，数据预期所表达的现实业务情况不相一致)。例如，在一次转账过程中，从某一账户中扣除的金额必须与另一账户中存入的金额相等。支付宝账号100 你读到余额要取，有人向你转100 但是事物没提交（这时候你读到的余额应该是100，而不是200） 这种就是一致性
3. 隔离性（Isolation）
   隔离性意味着事务必须在不干扰其他进程或事务的前提下独立执行。换言之，在事务或工作单元执行完毕之前，其所访问的数据不能受系统其他部分的影响。
4. 持久性（Durability）
   持久性表示在某个事务的执行过程中，对数据所作的所有改动都必须在事务成功结束前保存至某种物理存储设备。这样可以保证，所作的修改在任何系统瘫痪时不至于丢失。

# MySQL的隔离级别有哪些？

MySQL定义了四种隔离级别，包括一些具体规则，用于限定事务内外哪些改变是可见的，哪些改变是不可见的。低级别的隔离一般支持更高的并发处理，并且拥有更低的系统开销。
REPEATABLE READ 可重复读
MySQL数据库默认的隔离级别。该级别解决了READ UNCOMMITTED隔离级别导致的问题。它保证同一事务的多个实例在并发读取事务时，会“看到同样的”数据行。不过，这会导致另外一个棘手问题“幻读”。InnoDB和Falcon存储引擎通过多版本并发控制机制解决了幻读问题。
READ COMMITTED 读取提交内容
大多数数据库系统的默认隔离级别（但是不是MySQL的默认隔离级别），满足了隔离的早先简单定义：一个事务开始时，只能“看见”已经提交事务所做的改变，一个事务从开始到提交前，所做的任何数据改变都是不可见的，除非已经提交。这种隔离级别也支持所谓的“不可重复读”。这意味着用户运行同一个语句两次，看到的结果是不同的。
READ UNCOMMITTED 读取未提交内容
在这个隔离级别，所有事务都可以“看到”未提交事务的执行结果。在这种级别上，可能会产生很多问题，除非用户真的知道自己在做什么，并有很好的理由选择这样做。本隔离级别很少用于实际应用，因为它的性能也不必其他性能好多少，而别的级别还有其他更多的优点。读取未提交数据，也被称为“脏读”
SERIALIZABLE 可串行化
该级别是最高级别的隔离级。它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简而言之，SERIALIZABLE是在每个读的数据行上加锁。在这个级别，可能导致大量的超时Timeout和锁竞争Lock Contention现象，实际应用中很少使用到这个级别，但如果用户的应用为了数据的稳定性，需要强制减少并发的话，也可以选择这种隔离级。

1. 脏读

脏读是指一个事务读取了未提交事务执行过程中的数据。
当一个事务的操作正在多次修改数据，而在事务还未提交的时候，另外一个并发事务来读取了数据，就会导致读取到的数据并非是最终持久化之后的数据，这个数据就是脏读的数据。

2. 不可重复读

不可重复读是指对于数据库中的某个数据，一个事务执行过程中多次查询返回不同查询结果，这就是在事务执行过程中，数据被其他事务提交修改了。
不可重复读同脏读的区别在于，脏读是一个事务读取了另一未完成的事务执行过程中的数据，而不可重复读是一个事务执行过程中，另一事务提交并修改了当前事务正在读取的数据。

3. 虚读(幻读)

幻读是事务非独立执行时发生的一种现象，例如事务T1批量对一个表中某一列列值为1的数据修改为2的变更，但是在这时，事务T2对这张表插入了一条列值为1的数据，并完成提交。此时，如果事务T1查看刚刚完成操作的数据，发现还有一条列值为1的数据没有进行修改，而这条数据其实是T2刚刚提交插入的，这就是幻读。
幻读和不可重复读都是读取了另一条已经提交的事务（这点同脏读不同），所不同的是不可重复读查询的都是同一个数据项，而幻读针对的是一批数据整体（比如数据的个数）。

# 怎么处理MySQL的慢查询？

1、开启慢查询日志，准确定位到哪个sql语句出现了问题

2、分析sql语句，看看是否load了额外的数据，可能是查询了多余的行并且抛弃掉了，可能是加载了许多结果中并不需要的列，对语句进行分析以及重写

3、分析语句的执行计划，然后获得其使用索引的情况，之后修改语句或者修改索引，使得语句可以尽可能的命中索引

4、如果对语句的优化已经无法进行，可以考虑表中的数据量是否太大，如果是的话可以进行横向或者纵向的分表。

# ACID是靠什么保证的？

原子性由undolog日志来保证，它记录了需要回滚的日志信息，事务回滚时撤销已经执行成功的sql

一致性是由其他三大特性保证，程序代码要保证业务上的一致性

隔离性是由MVCC来保证

持久性由redolog来保证，mysql修改数据的时候会在redolog中记录一份日志数据，就算数据没有保存成功，只要日志保存成功了，数据仍然不会丢失





# 什么是mysql的主从复制？

​		MySQL 主从复制是指数据可以从一个MySQL数据库服务器主节点复制到一个或多个从节点。MySQL 默认采用异步复制方式，这样从节点不用一直访问主服务器来更新自己的数据，数据的更新可以在远程连接上进行，从节点可以复制主数据库中的所有数据库或者特定的数据库，或者特定的表。



# mysql为什么需要主从同步？

1、在业务复杂的系统中，有这么一个情景，有一句sql语句需要锁表，导致暂时不能使用读的服务，那么就很影响运行中的业务，使用主从复制，让主库负责写，从库负责读，这样，即使主库出现了锁表的情景，通过读从库也可以保证业务的正常运作。

2、做数据的热备

3、架构的扩展。业务量越来越大，I/O访问频率过高，单机无法满足，此时做多库的存储，降低磁盘I/O访问的频率，提高单个机器的I/O性能。



# mysql复制原理是什么？

​		（1）master服务器将数据的改变记录二进制binlog日志，当master上的数据发生改变时，则将其改变写入二进制日志中；		

​		（2）slave服务器会在一定时间间隔内对master二进制日志进行探测其是否发生改变，如果发生改变，则开始一个I/OThread请求master二进制事件

​		（3）同时主节点为每个I/O线程启动一个dump线程，用于向其发送二进制事件，并保存至从节点本地的中继日志中，从节点将启动SQL线程从中继日志中读取二进制日志，在本地重放，使得其数据和主节点的保持一致，最后I/OThread和SQLThread将进入睡眠状态，等待下一次被唤醒。

也就是说：

- 从库会生成两个线程,一个I/O线程,一个SQL线程;
- I/O线程会去请求主库的binlog,并将得到的binlog写到本地的relay-log(中继日志)文件中;
- 主库会生成一个log dump线程,用来给从库I/O线程传binlog;
- SQL线程,会读取relay log文件中的日志,并解析成sql语句逐一执行;

注意：

1--master将操作语句记录到binlog日志中，然后授予slave远程连接的权限（master一定要开启binlog二进制日志功能；通常为了数据安全考虑，slave也开启binlog功能）。
2--slave开启两个线程：IO线程和SQL线程。其中：IO线程负责读取master的binlog内容到中继日志relay log里；SQL线程负责从relay log日志里读出binlog内容，并更新到slave的数据库里，这样就能保证slave数据和master数据保持一致了。
3--Mysql复制至少需要两个Mysql的服务，当然Mysql服务可以分布在不同的服务器上，也可以在一台服务器上启动多个服务。
4--Mysql复制最好确保master和slave服务器上的Mysql版本相同（如果不能满足版本一致，那么要保证master主节点的版本低于slave从节点的版本）
5--master和slave两节点间时间需同步

![](C:/Users/y/Desktop/春招笔记/images/主从原理.png)

具体步骤：

1、从库通过手工执行change  master to 语句连接主库，提供了连接的用户一切条件（user 、password、port、ip），并且让从库知道，二进制日志的起点位置（file名 position 号）；    start  slave

2、从库的IO线程和主库的dump线程建立连接。

3、从库根据change  master  to 语句提供的file名和position号，IO线程向主库发起binlog的请求。

4、主库dump线程根据从库的请求，将本地binlog以events的方式发给从库IO线程。

5、从库IO线程接收binlog  events，并存放到本地relay-log中，传送过来的信息，会记录到master.info中

6、从库SQL线程应用relay-log，并且把应用过的记录到relay-log.info中，默认情况下，已经应用过的relay 会自动被清理purge

# 简述Myisam和Innodb的区别？

InnoDB存储引擎: 主要面向OLTP(Online Transaction Processing，在线事务处理)方面的应用，是第一个完整支持ACID事务的存储引擎(BDB第一个支持事务的存储引擎，已经停止开发)。
特点：

1 支持行锁
2 支持外键
3 支持自动增加列AUTO_INCREMENT属性
4 支持事务
5 支持MVCC模式的读写
6 读的效率低于MYISAM
7.写的效率高优于MYISAM
8.适合频繁修改以及设计到安全性较高的应用
9.清空整个表的时候，Innodb是一行一行的删除，

MyISAM存储引擎: 是MySQL官方提供的存储引擎，主要面向OLAP(Online Analytical Processing,在线分析处理)方面的应用。

特点：

1 独立于操作系统，当建立一个MyISAM存储引擎的表时，就会在本地磁盘建立三个文件，例如我建立tb_demo表，那么会生成以下三个文件tb_demo.frm,tb_demo.MYD,tb_demo.MYI
2 不支持事务，
3 支持表锁和全文索引
4 MyISAM存储引擎表由MYD和MYI组成，MYD用来存放数据文件，MYI用来存放索引文件。MySQL数据库只缓存其索引文件，数据文件的缓存交给操作系统本身来完成；
5 MySQL5.0版本开始，MyISAM默认支持256T的单表数据；
6.选择密集型的表：MYISAM存储引擎在筛选大量数据时非常迅速，这是他最突出的优点
7.读的效率优于InnoDB
8.写的效率低于InnoDB
9.适合查询以及插入为主的应用
10.清空整个表的时候，MYISAM则会新建表

# 简述mysql中索引类型有哪些，以及对数据库的性能的影响？

普通索引：允许被索引的数据列包含重复的值

唯一索引：可以保证数据记录的唯一性

主键索引：是一种特殊的唯一索引，在一张表中只能定义一个主键索引，主键用于唯一标识一条记录，使用关键字primary key来创建

联合索引：索引可以覆盖多个数据列

全文索引：通过建立倒排索引，可以极大的提升检索效率，解决判断字段是否包含的问题，是目前搜索引擎使用的一种关键技术

索引可以极大地提高数据的查询速度

通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能

但是会降低插入、删除、更新表的速度，因为在执行这些写操作的时候，还要操作索引文件

索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要简历聚簇索引，那么需要的空间就会更大，如果非聚簇索引很多，一旦聚簇索引改变，那么所有非聚簇索引都会跟着变



# 







# 类(class)初始化过程是什么？

首先类加载的机制过程分为5个部分：加载、验证、准备、解析、初始化
![](C:/Users/y/Desktop/春招笔记/images/class-int.png)


我们现在主要分析类的初始化过程：

1. 类的初始化阶段，是真正开始执行类中定义的java程序代码(字节码)并按程序员的意图去初始化类变量的过程。更直接地说，初始化阶段就是执行类构造器<clinit>()方法的过程。<clinit>()方法是由编译器自动收集类中的所有类变量的赋值动作和静态代码块static{}中的语句合并产生的，其中编译器收集的顺序是由语句在源文件中出现的顺序所决定。
1. 关于类初始化的顺序**（静态变量、静态初始化块：决于它们在类中出现的先后顺序）>（变量、初始化块：决于它们在类中出现的先后顺序）>构造器**
1. 关于类初始化的详细过程，参见 Java虚拟机规范一书中，其中类初始化过程如下：
   1. 每个类都有一个初始化锁LC，进程获取LC，这个操作会导致当前线程一直等待，直到获取到LC锁
   1. 如果C正在被其他线程初始化，当前线程会释放LC进去阻塞状态，并等待C初始化完成。此时当前线程需要重试这一过程。执行初始化过程时，线程的中断状态不受影响
   1. 如果C正在被本线程初始化，即递归初始化，释放LC并且正常返回
   1. 如果C已经被初始化完成，释放LC并且正常返回
   1. 如果C处于错误状态，表明不可能再完成初始化，释放LC并抛出异常NoClassDefFoundError异常
   1. 否则，将C标记为正在被本线程初始化，释放LC；然后，初始化那些final且为基础类型的类成员变量
   1. 如果C是类而不是接口，且C的父类Super Class（SC）和各个接口SI_n（按照implements子句中的顺序来）还没有初始化，那么就在SC上面递归地进行完整的初始化过程，如果有必要，需要先验证和准备SC ；如果SC或SIn初始化过程中抛出异常，则获取LC，将C标记为错误状态，并通知所有正在等待的线程，然后释放LC，然后再抛出同样的异常。
   1. 从C的classloader处获取assertion断言机制是否被打开
   1. 接下来，按照文本顺序执行类变量初始化和静态代码块，或接口的字段初始化，把它们当作是一个个单独的代码块。
   1. 如果执行正常，那就获取LC，标记C对象为已初始化，并通知所有正在等待的线程，然后释放LC，正常退出整个过程
   1. 否则，如果抛出了异常E那么会中断退出。若E不是Error，则以E为参数创建新的异常ExceptionInInitializerError作为E。如果因为OutOfMemoryError导致无法创建ExceptionInInitializerError，则将OutOfMemoryError作为E。
   1. 获取LC，将C标记为错误状态，通知所有等待的线程，释放LC，并抛出异常E。

可以看到 JLS确实规定了父类先初始化、static块和类变量赋值按照文本顺序来







# **什么情况下，会抛出OOM呢？**

- JVM98%的时间都花费在内存回收
- 每次回收的内存小于2%

满足这两个条件将触发OutOfMemoryException，这将会留给系统一个微小的间隙以做一些Down之前的操作，比如手动打印Heap Dump。并不是内存被耗空的时候才抛出
**​**

# **系统OOM之前都有哪些现象？**

- 每次垃圾回收的时间越来越长，由之前的10ms延长到50ms左右，FullGC的时间也有之前的0.5s延长到4、5s
- FullGC的次数越来越多，最频繁时隔不到1分钟就进行一次FullGC
- 老年代的内存越来越大并且每次FullGC后，老年代只有少量的内存被释放掉



# 如何进行堆Dump文件分析？

可以通过指定启动参数 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/usr/app/data/dump/heapdump.hpro 在发生OOM的时候自动导出Dump文件
​

# 如何进行GC日志分析？

为了方便分析GC日志信息，可以指定启动参数 【-Xloggc: app-gc.log  -XX:+PrintGCDetails -XX:+PrintGCDateStamps】,方便详细地查看GC日志信息

1. 使用 【jinfo pid】查看当前JVM堆的相关参数
1. 继续使用 【jstat -gcutil 2315 1s 10】查看10s内当前堆的占用情况
1. 也可以使用【jmap -heap pid】查看当前JVM堆的情况
1. 我们可以继续使用 【jmap -F -histo pid | head -n 20】，查看前20行打印，即查看当前top20的大对象，一般从这里可以发现一些异常的大对象，如果没有，那么可以继续排名前50的大对象，分析
1. 最后使用【jmap -F -dump:file=a.bin pid】，如果dump文件很大，可以压缩一下【tar -czvf a.tar.gz a.bin】
1. 再之后，就是对dump文件进行分析了，使用MAT分析内存泄露
1. 参考案例： [https://www.lagou.com/lgeduarticle/142372.html](https://www.lagou.com/lgeduarticle/142372.html)

# 线上死锁是如何排查的？

1. jps 查找一个可能有问题的**进程id**
1. 然后执行 【jstack -F **进程id**】
1. 如果环境允许远程连接JVM，可以使用jconsole或者jvisualvm，图形化界面检测是否存在死锁



# 线上YGC耗时过长优化方案有哪些？

1. 如果生命周期过长的对象越来越多（比如全局变量或者静态变量等），会导致标注和复制过程的耗时增加
1. 对存活对象标注时间过长：比如重载了Object类的Finalize方法，导致标注Final Reference耗时过长；或者String.intern方法使用不当，导致YGC扫描StringTable时间过长。可以通过以下参数显示GC处理Reference的耗时-XX:+PrintReferenceGC
1. 长周期对象积累过多：比如本地缓存使用不当，积累了太多存活对象；或者锁竞争严重导致线程阻塞，局部变量的生命周期变长
1. 案例参考： [https://my.oschina.net/lishangzhi/blog/4703942](https://my.oschina.net/lishangzhi/blog/4703942)

# 线上频繁FullGC优化方案有哪些？

1. 线上频繁FullGC一般会有这么几个特征：
   1. 线上多个线程的CPU都超过了100%，通过jstack命令可以看到这些线程主要是垃圾回收线程
   1. 通过jstat命令监控GC情况，可以看到Full GC次数非常多，并且次数在不断增加
2. 排查流程：
   1. top找到cpu占用最高的一个 **进程id**
   1. 然后 【top -Hp 进程id】，找到cpu占用最高的 **线程id**
   1. 【printf "%x\n" **线程id 】**，假设16进制结果为 a
   1. jstack 线程id | grep '0xa' -A 50 --color
   1. 如果是正常的用户线程， 则通过该线程的堆栈信息查看其具体是在哪处用户代码处运行比较消耗CPU
   1. 如果该线程是 VMThread，则通过 jstat-gcutil命令监控当前系统的GC状况，然后通过 jmapdump:format=b,file=导出系统当前的内存数据。导出之后将内存情况放到eclipse的mat工具中进行分析即可得出内存中主要是什么对象比较消耗内存，进而可以处理相关代码；正常情况下会发现VM Thread指的就是垃圾回收的线程
   1. 再执行【jstat -gcutil  **进程id】, **看到结果，如果FGC的数量很高，且在不断增长，那么可以定位是由于内存溢出导致FullGC频繁，系统缓慢
   1. 然后就可以Dump出内存日志，然后使用MAT的工具分析哪些对象占用内存较大，然后找到对象的创建位置，处理即可
3. 参考案例：[https://mp.weixin.qq.com/s/g8KJhOtiBHWb6wNFrCcLVg](https://mp.weixin.qq.com/s/g8KJhOtiBHWb6wNFrCcLVg)
   #### 

# 如何进行线上堆外内存泄漏的分析？（Netty尤其居多）

1. JVM的堆外内存泄露的定位一直是个比较棘手的问题
1. 对外内存的泄漏分析一般都是先从堆内内存分析的过程中衍生出来的。有可能我们分析堆内内存泄露过程中发现，我们计算出来的JVM堆内存竟然大于了整个JVM的**Xmx**的大小，那说明多出来的是堆外内存
1. 如果使用了 Netty 堆外内存，那么可以自行监控堆外内存的使用情况，不需要借助第三方工具，我们是使用的“反射”拿到的堆外内存的情况
1. 逐渐缩小范围，直到 Bug 被找到。当我们确认某个线程的执行带来 Bug 时，可单步执行，可二分执行，定位到某行代码之后，跟到这段代码，然后继续单步执行或者二分的方式来定位最终出 Bug 的代码。这个方法屡试不爽，最后总能找到想要的 Bug
1. 熟练掌握 idea 的调试，让我们的“捉虫”速度快如闪电（“闪电侠”就是这么来的）。这里，最常见的调试方式是**预执行表达式**，以及通过**线程调用栈**，死盯某个对象，就能够掌握这个对象的定义、赋值之类
1. 在使用直接内存的项目中，最好建议配置 -XX:MaxDirectMemorySize，设定一个系统实际可达的最大的直接内存的值，默认的最大直接内存大小等于 -Xmx的值
1. 排查堆外泄露，建议指定启动参数： -XX:NativeMemoryTracking=summary - Dio.netty.leakDetection.targetRecords=100-Dio.netty.leakDetection.level=PARANOID，后面两个参数是Netty的相关内存泄露检测的级别与采样级别
1. 参考案例： [https://tech.meituan.com/2018/10/18/netty-direct-memory-screening.html](https://tech.meituan.com/2018/10/18/netty-direct-memory-screening.html)



# 线上元空间内存泄露优化方案有哪些？

1. 需要注意的一点是 Java8以及Java8+的JVM已经将永久代废弃了，取而代之的是元空间，且元空间是不是在JVM堆中的，而属于堆外内存，受最大物理内存限制。最佳实践就是我们在启动参数中最好设置上 -XX:MetaspaceSize=1024m -XX:MaxMetaspaceSize=1024m。具体的值根据情况设置。为避免动态申请，可以直接都设置为最大值
1. 元空间主要存放的是类元数据，而且metaspace判断类元数据是否可以回收，是根据加载这些类元数据的Classloader是否可以回收来判断的，只要Classloader不能回收，通过其加载的类元数据就不会被回收。所以线上有时候会出现一种问题，由于框架中，往往大量采用类似ASM、javassist等工具进行字节码增强，生成代理类。如果项目中由主线程频繁生成动态代理类，那么就会导致元空间迅速占满，无法回收
1. 具体案例可以参见： [https://zhuanlan.zhihu.com/p/200802910](https://zhuanlan.zhihu.com/p/200802910)



# GC如何判断对象可以被回收？

1. 引用计数法（已被淘汰的算法）
   1. 每一个对象有一个引用属性，新增一个引用时加一，引用释放时减一，计数为0的时候可以回收。

但是这种计算方法，有一个致命的问题，无法解决循环引用的问题

2. 可达性分析算法（根引用）
   1. 从GcRoot开始向下搜索，搜索所走过的路径被称为引用链，当一个对象到GcRoot没有任何引用链相连时，则证明此对象是不可用的，那么虚拟机就可以判定回收。
   
   1. 那么GcRoot有哪些？
      1. 虚拟机栈中引用的对象
      
      1. 方法区中静态属性引用的对象。
      
      1. 方法区中常量引用的对象
      
      1. 本地方法栈中（即一般说的native方法）引用的对象
      
#### 引用类型(强引用、软引用、弱引用、虚引用)

   1. 强引用：通过关键字new的对象就是强引用对象，强引用指向的对象任何时候都不会被回收，宁愿OOM也不会回收。
   1. 软引用：如果一个对象持有软引用，那么当JVM堆空间不足时，会被回收。一个类的软引用可以通过java.lang.ref.SoftReference持有。
   1. 弱引用：如果一个对象持有弱引用，那么在GC时，只要发现弱引用对象，就会被回收。一个类的弱引用可以通过java.lang.ref.WeakReference持有。
   1. 虚引用：几乎和没有一样，随时可以被回收。通过PhantomReference持有。



# 如何回收内存对象，有哪些回收算法？ 

**1.标记-清除（Mark-Sweep）算法**  

分为标记和清除两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。![](C:/Users/y/Desktop/春招笔记/images/before.png)

它的主要不足有两个：

- 效率问题，标记和清除两个过程的效率都不高。
- 空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致以后在程序运行过程中需要分配较大对象时，无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。

2. **复制算法**

为了解决效率问题，一种称为复制（Copying）的收集算法出现了，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。这样使得每次都是对整个半区进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。
![](C:/Users/y/Desktop/春招笔记/images/copy.png)
**复制算法的代价**是将内存缩小为了原来的一半，减少了实际可用的内存。现在的商业虚拟机都采用这种收集算法来回收新生代，IBM公司的专门研究表明，新生代中的对象98%是“朝生夕死”的，所以并不需要按照1:1的比例来划分内存空间，而是将内存分为一块较大的Eden空间和两块较小的Survivor空间，每次使用Eden和其中一块Survivor。当回收时，将Eden和Survivor中还存活着的对象一次性地复制到另外一块Survivor空间上，最后清理掉Eden和刚才用过的Survivor空间。HotSpot虚拟机默认Eden和Survivor的大小比例是8:1，也就是每次新生代中可用内存空间为整个新生代容量的90%（80%+10%），只有10%的内存会被“浪费”。当然，98%的对象可回收只是一般场景下的数据，我们没有办法保证每次回收都只有不多于10%的对象存活，当Survivor空间不够用时，需要依赖其他内存（这里指老年代）进行分配担保（Handle Promotion）。

3. **标记-整理算法**

复制收集算法在对象存活率较高时就要进行较多的复制操作，效率将会变低。更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以在老年代一般不能直接选用这种算法。根据老年代的特点，有人提出了另外一种标记-整理（Mark-Compact）算法，标记过程仍然与标记-清除算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。
![](C:/Users/y/Desktop/春招笔记/images/3-1621487892206.png)

4. **分代收集算法**

当前商业虚拟机的垃圾收集都采用分代收集（Generational Collection）算法，这种算法并没有什么新的思想，只是根据对象存活周期的不同将内存划分为几块。一般是把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用标记—清理或者标记—整理算法来进行回收。

#  jvm有哪些垃圾回收器，实际中如何选择？ 

![](C:/Users/y/Desktop/春招笔记/images/gcollector.png)
图中展示了7种作用于不同分代的收集器，如果两个收集器之间存在连线，则说明它们可以搭配使用。虚拟机所处的区域则表示它是属于新生代还是老年代收集器。
新生代收集器（全部的都是复制算法）：Serial、ParNew、Parallel Scavenge
老年代收集器：CMS（标记-清理）、Serial Old（标记-整理）、Parallel Old（标记整理）
整堆收集器： G1（一个Region中是标记-清除算法，2个Region之间是复制算法）
同时，先解释几个名词：
1，**并行（Parallel）**：多个垃圾收集线程并行工作，此时用户线程处于等待状态
2，**并发（Concurrent）**：用户线程和垃圾收集线程同时执行
3，**吞吐量**：运行用户代码时间／（运行用户代码时间＋垃圾回收时间）
**1.Serial收集器是最基本的、发展历史最悠久的收集器。**
**特点：**单线程、简单高效（与其他收集器的单线程相比），对于限定单个CPU的环境来说，Serial收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程手机效率。收集器进行垃圾回收时，必须暂停其他所有的工作线程，直到它结束（Stop The World）。
**应用场景**：适用于Client模式下的虚拟机。
**Serial / Serial Old收集器运行示意图**
![](C:/Users/y/Desktop/春招笔记/images/serial.png)
**2.ParNew收集器其实就是Serial收集器的多线程版本。**
除了使用多线程外其余行为均和Serial收集器一模一样（参数控制、收集算法、Stop The World、对象分配规则、回收策略等）。
**特点**：多线程、ParNew收集器默认开启的收集线程数与CPU的数量相同，在CPU非常多的环境中，可以使用-XX:ParallelGCThreads参数来限制垃圾收集的线程数。
　　　和Serial收集器一样存在Stop The World问题
**应用场景**：ParNew收集器是许多运行在Server模式下的虚拟机中首选的新生代收集器，因为它是除了Serial收集器外，唯一一个能与CMS收集器配合工作的。
_ParNew/Serial Old组合收集器运行示意图如下：_
![](C:/Users/y/Desktop/春招笔记/images/parnew.png)
**3.Parallel Scavenge 收集器与吞吐量关系密切，故也称为吞吐量优先收集器。**
**特点**：属于新生代收集器也是采用复制算法的收集器，又是并行的多线程收集器（与ParNew收集器类似）。
该收集器的目标是达到一个可控制的吞吐量。还有一个值得关注的点是：GC自适应调节策略（与ParNew收集器最重要的一个区别）
**GC自适应调节策略**：Parallel Scavenge收集器可设置-XX:+UseAdptiveSizePolicy参数。当开关打开时不需要手动指定新生代的大小（-Xmn）、Eden与Survivor区的比例（-XX:SurvivorRation）、晋升老年代的对象年龄（-XX:PretenureSizeThreshold）等，虚拟机会根据系统的运行状况收集性能监控信息，动态设置这些参数以提供最优的停顿时间和最高的吞吐量，这种调节方式称为GC的自适应调节策略。
Parallel Scavenge收集器使用两个参数控制吞吐量：

- XX:MaxGCPauseMillis 控制最大的垃圾收集停顿时间
- XX:GCRatio 直接设置吞吐量的大小。

**4.Serial Old是Serial收集器的老年代版本。**
**特点**：同样是单线程收集器，采用标记-整理算法。
**应用场景**：主要也是使用在Client模式下的虚拟机中。也可在Server模式下使用。
Server模式下主要的两大用途（在后续中详细讲解···）：

1. 在JDK1.5以及以前的版本中与Parallel Scavenge收集器搭配使用。
1. 作为CMS收集器的后备方案，在并发收集Concurent Mode Failure时使用。

Serial / Serial Old收集器工作过程图（Serial收集器图示相同）：
![](C:/Users/y/Desktop/春招笔记/images/serial-old.png)
**5.Parallel Old是Parallel Scavenge收集器的老年代版本。**
**特点**：多线程，采用标记-整理算法。
**应用场景**：注重高吞吐量以及CPU资源敏感的场合，都可以优先考虑Parallel Scavenge+Parallel Old 收集器。
_Parallel Scavenge/Parallel Old收集器工作过程图：_
**6.CMS收集器是一种以获取最短回收停顿时间为目标的收集器。**
**特点**：基于标记-清除算法实现。并发收集、低停顿。
**应用场景**：适用于注重服务的响应速度，希望系统停顿时间最短，给用户带来更好的体验等场景下。如web程序、b/s服务。
**CMS收集器的运行过程分为下列4步：**
**初始标记**：标记GC Roots能直接到的对象。速度很快但是仍存在Stop The World问题。
**并发标记**：进行GC Roots Tracing 的过程，找出存活对象且用户线程可并发执行。
**重新标记**：为了修正并发标记期间因用户程序继续运行而导致标记产生变动的那一部分对象的标记记录。仍然存在Stop The World问题。
**并发清除**：对标记的对象进行清除回收。
CMS收集器的内存回收过程是与用户线程一起并发执行的。
 CMS收集器的工作过程图：
![](C:/Users/y/Desktop/春招笔记/images/cms.png)
CMS收集器的缺点：

- 对CPU资源非常敏感。
- 无法处理浮动垃圾，可能出现Concurrent Model Failure失败而导致另一次Full GC的产生。
- 因为采用标记-清除算法所以会存在空间碎片的问题，导致大对象无法分配空间，不得不提前触发一次Full GC。![](C:/Users/y/Desktop/春招笔记/images/cms2.png)

**​**

**7.G1收集器一款面向服务端应用的垃圾收集器。**
**特点如下：**
并行与并发：G1能充分利用多CPU、多核环境下的硬件优势，使用多个CPU来缩短Stop-The-World停顿时间。部分收集器原本需要停顿Java线程来执行GC动作，G1收集器仍然可以通过并发的方式让Java程序继续运行。
分代收集：G1能够独自管理整个Java堆，并且采用不同的方式去处理新创建的对象和已经存活了一段时间、熬过多次GC的旧对象以获取更好的收集效果。
空间整合：G1运作期间不会产生空间碎片，收集后能提供规整的可用内存。
可预测的停顿：G1除了追求低停顿外，还能建立可预测的停顿时间模型。能让使用者明确指定在一个长度为M毫秒的时间段内，消耗在垃圾收集上的时间不得超过N毫秒。
**G1收集器运行示意图：**
![](C:/Users/y/Desktop/春招笔记/images/g1.png)
**​**

关于gc的选择
除非应用程序有非常严格的暂停时间要求，否则请先运行应用程序并允许VM选择收集器（如果没有特别要求。使用VM提供给的默认GC就好）。
如有必要，调整堆大小以提高性能。 如果性能仍然不能满足目标，请使用以下准则作为选择收集器的起点：

   - 如果应用程序的数据集较小（最大约100 MB），则选择带有选项-XX：+ UseSerialGC的串行收集器。
   - 如果应用程序将在单个处理器上运行，并且没有暂停时间要求，则选择带有选项-XX：+ UseSerialGC的串行收集器。
   - 如果（a）峰值应用程序性能是第一要务，并且（b）没有暂停时间要求或可接受一秒或更长时间的暂停，则让VM选择收集器或使用-XX：+ UseParallelGC选择并行收集器 。
   - 如果响应时间比整体吞吐量更重要，并且垃圾收集暂停时间必须保持在大约一秒钟以内，则选择具有-XX：+ UseG1GC。（值得注意的是JDK9中CMS已经被Deprecated，不可使用！移除该选项）
   - 如果使用的是jdk8，并且堆内存达到了16G，那么推荐使用G1收集器，来控制每次垃圾收集的时间。
   - 如果响应时间是高优先级，或使用的堆非常大，请使用-XX：UseZGC选择完全并发的收集器。（值得注意的是JDK11开始可以启动ZGC，但是此时ZGC具有实验性质，在JDK15中[202009发布]才取消实验性质的标签，可以直接显示启用，但是JDK15默认GC仍然是G1）



这些准则仅提供选择收集器的起点，因为性能取决于堆的大小，应用程序维护的实时数据量以及可用处理器的数量和速度。
如果推荐的收集器没有达到所需的性能，则首先尝试调整堆和新生代大小以达到所需的目标。 如果性能仍然不足，尝试使用其他收集器
**总体原则**：减少STOP THE WORD时间，使用并发收集器（比如CMS+ParNew，G1）来减少暂停时间，加快响应时间，并使用并行收集器来增加多处理器硬件上的总体吞吐量。

# JVM8为什么要增加元空间？

原因：
1、字符串存在永久代中，容易出现性能问题和内存溢出。
2、类及方法的信息等比较难确定其大小，因此对于永久代的大小指定比较困难，太小容易出现永久代溢出，太大则容易导致老年代溢出。
3、永久代会为 GC 带来不必要的复杂度，并且回收效率偏低。

# JVM8中元空间有哪些特点？

1，每个加载器有专门的存储空间。
2，不会单独回收某个类。
3，元空间里的对象的位置是固定的。
4，如果发现某个加载器不再存货了，会把相关的空间整个回收

#  如何解决线上gc频繁的问题？

1. 查看监控，以了解出现问题的时间点以及当前FGC的频率（可对比正常情况看频率是否正常）
1. 了解该时间点之前有没有程序上线、基础组件升级等情况。
1. 了解JVM的参数设置，包括：堆空间各个区域的大小设置，新生代和老年代分别采用了哪些垃圾收集器，然后分析JVM参数设置是否合理。
1. 再对步骤1中列出的可能原因做排除法，其中元空间被打满、内存泄漏、代码显式调用gc方法比较容易排查。
1. 针对大对象或者长生命周期对象导致的FGC，可通过 jmap -histo 命令并结合dump堆内存文件作进一步分析，需要先定位到可疑对象。
1. 通过可疑对象定位到具体代码再次分析，这时候要结合GC原理和JVM参数设置，弄清楚可疑对象是否满足了进入到老年代的条件才能下结论。

# 内存溢出的原因有哪些，如何排查线上问题？

1. java.lang.OutOfMemoryError: ......java heap space.....   堆栈溢出，代码问题的可能性极大
1. java.lang.OutOfMemoryError: GC over head limit exceeded 系统处于高频的GC状态，而且回收的效果依然不佳的情况，就会开始报这个错误，这种情况一般是产生了很多不可以被释放的对象，有可能是引用使用不当导致，或申请大对象导致，但是java heap space的内存溢出有可能提前不会报这个错误，也就是可能内存就直接不够导致，而不是高频GC.
1. java.lang.OutOfMemoryError: PermGen space jdk1.7之前才会出现的问题 ，原因是系统的代码非常多或引用的第三方包非常多、或代码中使用了大量的常量、或通过intern注入常量、或者通过动态代码加载等方法，导致常量池的膨胀
1. java.lang.OutOfMemoryError: Direct buffer memory    直接内存不足，因为jvm垃圾回收不会回收掉直接内存这部分的内存，所以可能原因是直接或间接使用了ByteBuffer中的allocateDirect方法的时候，而没有做clear
1. java.lang.StackOverflowError -     Xss设置的太小了
1. java.lang.OutOfMemoryError: unable to create new native thread 堆外内存不足，无法为线程分配内存区域
1. java.lang.OutOfMemoryError: request {} byte for {}out of swap 地址空间不够

# Happens-Before规则是什么？

1. 程序顺序规则：一个线程中的每一个操作，happens-before于该线程中的任意后续操作。
1. 监视器规则：对一个锁的解锁，happens-before于随后对这个锁的加锁。
1. volatile规则：对一个volatile变量的写，happens-before于任意后续对一个volatile变量的读。
1. 传递性：若果A happens-before B，B happens-before C，那么A happens-before C。
1. 线程启动规则：Thread对象的start()方法，happens-before于这个线程的任意后续操作。
1. 线程终止规则：线程中的任意操作，happens-before于该线程的终止监测。我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值等手段检测到线程已经终止执行。
1. 线程中断操作：对线程interrupt()方法的调用，happens-before于被中断线程的代码检测到中断事件的发生，可以通过Thread.interrupted()方法检测到线程是否有中断发生。
1. 对象终结规则：一个对象的初始化完成，happens-before于这个对象的finalize()方法的开始。

# 介绍一下线程的生命周期及状态？

![未命名文件.jpg](C:/Users/y/Desktop/春招笔记/images/life.jpg)
**1.创建**
当程序使用new关键字创建了一个线程之后，该线程就处于一个新建状态（初始状态），此时它和其他Java对象一样，仅仅由Java虚拟机为其分配了内存，并初始化了其成员变量值。此时的线程对象没有表现出任何线程的动态特征，程序也不会执行线程的线程执行体。
**2.就绪**
当线程对象调用了Thread.start()方法之后，该线程处于就绪状态。Java虚拟机会为其创建方法调用栈和程序计数器，处于这个状态的线程并没有开始运行，它只是表示该线程可以运行了。从start()源码中看出，start后添加到了线程列表中，接着在native层添加到VM中，至于该线程何时开始运行，取决于JVM里线程调度器的调度(如果OS调度选中了，就会进入到运行状态)。
**3.运行**
当线程对象调用了Thread.start()方法之后，该线程处于就绪状态。添加到了线程列表中，如果OS调度选中了，就会进入到运行状态
**4.阻塞**
阻塞状态是线程因为某种原因放弃CPU使用权，暂时停止运行。直到线程进入就绪状态，才有机会转到运行状态。阻塞的情况大概三种：

- 1、**等待阻塞**：运行的线程执行wait()方法，JVM会把该线程放入等待池中。(wait会释放持有的锁)
- 2、**同步阻塞**：运行的线程在获取对象的同步锁时，若该同步锁被别的线程占用，则JVM会把该线程放入锁池中。
- 3、**其他阻塞**：运行的线程执行sleep()或join()方法，或者发出了I/O请求时，JVM会把该线程置为阻塞状态。当sleep()状态超时、join()等待线程终止或者超时、或者I/O处理完毕时，线程重新转入就绪状态。（注意,sleep是不会释放持有的锁）。
- 线程睡眠：Thread.sleep(long millis)方法，使线程转到阻塞状态。millis参数设定睡眠的时间，以毫秒为单位。当睡眠结束后，就转为就绪（Runnable）状态。sleep()平台移植性好。
- 线程等待：Object类中的wait()方法，导致当前的线程等待，直到其他线程调用此对象的 notify() 方法或 notifyAll() 唤醒方法。这个两个唤醒方法也是Object类中的方法，行为等价于调用 wait(0) 一样。唤醒线程后，就转为就绪（Runnable）状态。
- 线程让步：Thread.yield() 方法，暂停当前正在执行的线程对象，把执行机会让给相同或者更高优先级的线程。
- 线程加入：join()方法，等待其他线程终止。在当前线程中调用另一个线程的join()方法，则当前线程转入阻塞状态，直到另一个进程运行结束，当前线程再由阻塞转为就绪状态。
- 线程I/O：线程执行某些IO操作，因为等待相关的资源而进入了阻塞状态。比如说监听system.in，但是尚且没有收到键盘的输入，则进入阻塞状态。
- 线程唤醒：Object类中的notify()方法，唤醒在此对象监视器上等待的单个线程。如果所有线程都在此对象上等待，则会选择唤醒其中一个线程，选择是任意性的，并在对实现做出决定时发生。类似的方法还有一个notifyAll()，唤醒在此对象监视器上等待的所有线程。

**5.死亡**
线程会以以下三种方式之一结束，结束后就处于死亡状态:

- run()方法执行完成，线程正常结束。
- 线程抛出一个未捕获的Exception或Error。
- 直接调用该线程的stop()方法来结束该线程——该方法容易导致死锁，通常不推荐使用

#  线程的sleep、wait、join、yield如何使用？

**sleep**:让线程睡眠，期间会出让cpu，在同步代码块中，不会释放锁
**wait**(必须先获得对应的锁才能调用):让线程进入等待状态,释放当前线程持有的锁资源线程只有在notify 或者notifyAll方法调用后才会被唤醒,然后去争夺锁.
**join**:线程之间协同方式,使用场景: 线程A必须等待线程B运行完毕后才可以执行,那么就可以在线程A的代码中加入ThreadB.join();
**yield**:让当前正在运行的线程回到可运行状态，以允许具有相同优先级的其他线程获得运行的机会。因此，使用yield()的目的是让具有相同优先级的线程之间能够适当的轮换执行。但是，实际中无法保证yield()达到让步的目的，因为，让步的线程可能被线程调度程序再次选中。

# 创建线程有哪些方式？

1）继承Thread类创建线程
2）实现Runnable接口创建线程
3）使用Callable和Future创建线程
4）使用线程池例如用Executor框架

# 什么是守护线程？

在Java中有两类线程：User Thread(用户线程)、Daemon Thread(守护线程) 
任何一个守护线程都是整个JVM中所有非守护线程的保姆：
只要当前JVM实例中尚存在任何一个非守护线程没有结束，守护线程就全部工作；只有当最后一个非守护线程结束时，守护线程随着JVM一同结束工作。Daemon的作用是为其他线程的运行提供便利服务，守护线程最典型的应用就是 GC (垃圾回收器)，它就是一个很称职的守护者。
User和Daemon两者几乎没有区别，唯一的不同之处就在于虚拟机的离开：如果 User Thread已经全部退出运行了，只剩下Daemon Thread存在了，虚拟机也就退出了。 因为没有了被守护者，Daemon也就没有工作可做了，也就没有继续运行程序的必要了。
注意事项:
(1) thread.setDaemon(true)必须在thread.start()之前设置，否则会出现一个IllegalThreadStateException异常。只能在线程未开始运行之前设置为守护线程。
(2) 在Daemon线程中产生的新线程也是Daemon的。
(3) 不要认为所有的应用都可以分配给Daemon来进行读写操作或者计算逻辑，因为这会可能回到数据不一致的状态。

# ThreadLocal的原理是什么，使用场景有哪些？

Thread类中有两个变量threadLocals和inheritableThreadLocals，二者都是ThreadLocal内部类ThreadLocalMap类型的变量，我们通过查看内部内ThreadLocalMap可以发现实际上它类似于一个HashMap。在默认情况下，每个线程中的这两个变量都为null:

```java
ThreadLocal.ThreadLocalMap threadLocals = null;
ThreadLocal.ThreadLocalMap inheritableThreadLocals = null;
```

只有当线程第一次调用ThreadLocal的set或者get方法的时候才会创建他们。

```java
public T get() {
        Thread t = Thread.currentThread();
        ThreadLocalMap map = getMap(t);
        if (map != null) {
            ThreadLocalMap.Entry e = map.getEntry(this);
            if (e != null) {
                @SuppressWarnings("unchecked")
                T result = (T)e.value;
                return result;
            }
        }
        return setInitialValue();
}
    
ThreadLocalMap getMap(Thread t) {
        return t.threadLocals;
}
```

除此之外，每个线程的本地变量不是存放在ThreadLocal实例中，而是放在调用线程的**ThreadLocals**变量里面。也就是说，**ThreadLocal类型的本地变量是存放在具体的线程空间上**，其本身相当于一个装载本地变量的载体，通过set方法将value添加到调用线程的threadLocals中，当调用线程调用get方法时候能够从它的threadLocals中取出变量。如果调用线程一直不终止，那么这个本地变量将会一直存放在他的threadLocals中，所以不使用本地变量的时候需要调用remove方法将threadLocals中删除不用的本地变量,防止出现内存泄漏。

```java
public void set(T value) {
        Thread t = Thread.currentThread();
        ThreadLocalMap map = getMap(t);
        if (map != null)
            map.set(this, value);
        else
            createMap(t, value);
}
public void remove() {
         ThreadLocalMap m = getMap(Thread.currentThread());
         if (m != null)
             m.remove(this);
}
```

# ThreadLocal有哪些内存泄露问题，如何避免？

每个Thread都有一个ThreadLocal.ThreadLocalMap的map，该map的key为ThreadLocal实例，它为一个弱引用，我们知道弱引用有利于GC回收。当ThreadLocal的key == null时，GC就会回收这部分空间，但是value却不一定能够被回收，因为他还与Current Thread存在一个强引用关系，如下

![](C:/Users/y/Desktop/春招笔记/images/threadlocal.png)由于存在这个强引用关系，会导致value无法回收。如果这个线程对象不会销毁那么这个强引用关系则会一直存在，就会出现内存泄漏情况。所以说只要这个线程对象能够及时被GC回收，就不会出现内存泄漏。如果碰到线程池，那就更坑了。 那么要怎么避免这个问题呢？ 在前面提过，在ThreadLocalMap中的setEntry()、getEntry()，如果遇到key == null的情况，会对value设置为null。当然我们也可以显示调用ThreadLocal的remove()方法进行处理。 下面再对ThreadLocal进行简单的总结：

- ThreadLocal 不是用于解决共享变量的问题的，也不是为了协调线程同步而存在，而是为了方便每个线程处理自己的状态而引入的一个机制。这点至关重要。
- 每个Thread内部都有一个ThreadLocal.ThreadLocalMap类型的成员变量，该成员变量用来存储实际的ThreadLocal变量副本。
- ThreadLocal并不是为线程保存对象的副本，它仅仅只起到一个索引的作用。它的主要木得视为每一个线程隔离一个类的实例，这个实例的作用范围仅限于线程内部。

# 为什么要使用线程池？

为了减少创建和销毁线程的次数，让每个线程可以多次使用,可根据系统情况**调整执行**的线程数量，防止消耗过多内存,所以我们可以使用线程池.

# 线程池线程复用的原理是什么？

思考这么一个问题：任务结束后会不会回收线程？
答案是：allowCoreThreadTimeOut控制

```java
/java/util/concurrent/ThreadPoolExecutor.java:1127
final void runWorker(Worker w) {
        Thread wt = Thread.currentThread();
        Runnable task = w.firstTask;
        w.firstTask = null;
        w.unlock(); // allow interrupts
        boolean completedAbruptly = true;
        try {
            while (task != null || (task = getTask()) != null) {...执行任务...}
            completedAbruptly = false;
        } finally {
            processWorkerExit(w, completedAbruptly);
        }
    }
首先线程池内的线程都被包装成了一个个的java.util.concurrent.ThreadPoolExecutor.Worker,然后这个worker会马不停蹄的执行任务,执行完任务之后就会在while循环中去取任务,取到任务就继续执行,取不到任务就跳出while循环(这个时候worker就不能再执行任务了)执行 processWorkerExit方法,这个方法呢就是做清场处理,将当前woker线程从线程池中移除,并且判断是否是异常的进入processWorkerExit方法,如果是非异常情况,就对当前线程池状态(RUNNING,shutdown)和当前工作线程数和当前任务数做判断,是否要加入一个新的线程去完成最后的任务(防止没有线程去做剩下的任务).
那么什么时候会退出while循环呢?取不到任务的时候(getTask() == null).下面看一下getTask方法

private Runnable getTask() {
        boolean timedOut = false; // Did the last poll() time out?

        for (;;) {
            int c = ctl.get();
            int rs = runStateOf(c);

            //(rs == SHUTDOWN && workQueue.isEmpty()) || rs >=STOP
            //若线程池状态是SHUTDOWN 并且 任务队列为空,意味着已经不需要工作线程执行任务了,线程池即将关闭
            //若线程池的状态是 STOP TIDYING TERMINATED,则意味着线程池已经停止处理任何任务了,不在需要线程
            if (rs >= SHUTDOWN && (rs >= STOP || workQueue.isEmpty())) {
            	//把此工作线程从线程池中删除
                decrementWorkerCount();
                return null;
            }

            int wc = workerCountOf(c);

            //allowCoreThreadTimeOut:当没有任务的时候,核心线程数也会被剔除,默认参数是false,官方推荐在创建线程池并且还未使用的时候,设置此值
            //如果当前工作线程数 大于 核心线程数,timed为true
            boolean timed = allowCoreThreadTimeOut || wc > corePoolSize;
			
            //(wc > maximumPoolSize || (timed && timedOut)):当工作线程超过最大线程数,或者 允许超时并且超时过一次了
            //(wc > 1 || workQueue.isEmpty()):工作线程数至少为1个 或者 没有任务了
            //总的来说判断当前工作线程还有没有必要等着拿任务去执行
            //wc > maximumPoolSize && wc>1 : 就是判断当前工作线程是否超过最大值
            //或者 wc > maximumPoolSize && workQueue.isEmpty():工作线程超过最大,基本上不会走到这,
            //		如果走到这,则意味着wc=1 ,只有1个工作线程了,如果此时任务队列是空的,则把最后的线程删除
            //或者(timed && timedOut) && wc>1:如果允许超时并且超时过一次,并且至少有1个线程,则删除线程
            //或者 (timed && timedOut) && workQueue.isEmpty():如果允许超时并且超时过一次,并且此时工作					队列为空，那么妥妥可以把最后一个线程（因为上面的wc>1不满足，则可以得出来wc=1）删除
            if ((wc > maximumPoolSize  || (timed && timedOut))
                && (wc > 1 || workQueue.isEmpty())) {
                if (compareAndDecrementWorkerCount(c))
                	//如果减去工作线程数成功,则返回null出去,也就是说 让工作线程停止while轮训,进行收尾
                    return null;
                continue;
            }

            try {
            	//判断是否要阻塞获取任务
                Runnable r = timed ?
                    workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) :
                    workQueue.take();
                if (r != null)
                    return r;
                timedOut = true;
            } catch (InterruptedException retry) {
                timedOut = false;
            }
        }
    }
    
//综上所述,如果allowCoreThreadTimeOut为true,并且在第1次阻塞获取任务失败了,那么当前getTask会返回null,不管是不是核心线程;那么runWorker中将推出while循环,也就意味着当前工作线程被销毁

```

通过上面这个问题可以得出一个结论：当你的线程池参数配置合理的时候，执行完任务的线程是不会被销毁的，而是会从任务队列中取出任务继续执行！

# 如何预防死锁？

1. 首先需要将死锁发生的是个必要条件讲出来:
   1. 互斥条件 同一时间只能有一个线程获取资源。
   1. 不可剥夺条件 一个线程已经占有的资源，在释放之前不会被其它线程抢占
   1. 请求和保持条件 线程等待过程中不会释放已占有的资源
   1. 循环等待条件 多个线程互相等待对方释放资源
2. 死锁预防，那么就是需要破坏这四个必要条件
   1. 由于资源互斥是资源使用的固有特性，无法改变，我们不讨论
   1. 破坏不可剥夺条件
      1. 一个进程不能获得所需要的全部资源时便处于等待状态，等待期间他占有的资源将被隐式的释放重新加入到系统的资源列表中，可以被其他的进程使用，而等待的进程只有重新获得自己原有的资源以及新申请的资源才可以重新启动，执行
3. 破坏请求与保持条件
   1. 第一种方法静态分配即每个进程在开始执行时就申请他所需要的全部资源
   1. 第二种是动态分配即每个进程在申请所需要的资源时他本身不占用系统资源
4. 破坏循环等待条件
   1. 采用资源有序分配其基本思想是将系统中的所有资源顺序编号，将紧缺的，稀少的采用较大的编号，在申请资源时必须按照编号的顺序进行，一个进程只有获得较小编号的进程才能申请较大编号的进程。

# 描述一下线程安全活跃态问题？

线程安全的活跃性问题可以分为 死锁、活锁、饥饿   

1. 活锁 就是有时线程虽然没有发生阻塞，但是仍然会存在执行不下去的情况，活锁不会阻塞线程，线程会一直重复执行某个相同的操作，并且一直失败重试
   1. 我们开发中使用的异步消息队列就有可能造成活锁的问题，在消息队列的消费端如果没有正确的ack消息，并且执行过程中报错了，就会再次放回消息头，然后再拿出来执行，一直循环往复的失败。这个问题除了正确的ack之外，往往是通过将失败的消息放入到延时队列中，等到一定的延时再进行重试来解决。
   1. 解决活锁的方案很简单，尝试等待一个随机的时间就可以，会按时间轮去重试
2. 饥饿  就是 线程因无法访问所需资源而无法执行下去的情况
   1. 饥饿 分为两种情况：
      1. 一种是其他的线程在临界区做了无限循环或无限制等待资源的操作，让其他的线程一直不能拿到锁进入临界区，对其他线程来说，就进入了饥饿状态
      1. 另一种是因为线程优先级不合理的分配，导致部分线程始终无法获取到CPU资源而一直无法执行
   2. 解决饥饿的问题有几种方案:
      1.  保证资源充足，很多场景下，资源的稀缺性无法解决
      1.  公平分配资源，在并发编程里使用公平锁，例如FIFO策略，线程等待是有顺序的，排在等待队列前面的线程会优先获得资源
      1.  避免持有锁的线程长时间执行，很多场景下，持有锁的线程的执行时间也很难缩短
3. 死锁  线程在对同一把锁进行竞争的时候，未抢占到锁的线程会等待持有锁的线程释放锁后继续抢占，如果两个或两个以上的线程互相持有对方将要抢占的锁，互相等待对方先行释放锁就会进入到一个循环等待的过程，这个过程就叫做死锁

# 线程安全的竞态条件有哪些？

1. 同一个程序多线程访问同一个资源，如果对资源的访问顺序敏感，就称存在竞态条件，代码区成为临界区。 大多数并发错误一样，竞态条件不总是会产生问题，还需要不恰当的执行时序
1. 最常见的竞态条件为
   1. 先检测后执行执行依赖于检测的结果，而检测结果依赖于多个线程的执行时序，而多个线程的执行时序通常情况下是不固定不可判断的，从而导致执行结果出现各种问题，见一种可能 的解决办法就是：在一个线程修改访问一个状态时，要防止其他线程访问修改，也就是加锁机制，保证原子性
   1. 延迟初始化（典型为单例）

# 程序开多少线程合适？

1. CPU 密集型程序，一个完整请求，I/O操作可以在很短时间内完成，CPU还有很多运算要处理，也就是说 CPU 计算的比例占很大一部分，线程等待时间接近0   
   1. 单核CPU： 一个完整请求，I/O操作可以在很短时间内完成， CPU还有很多运算要处理，也就是说 CPU 计算的比例占很大一部分，线程等待时间接近0。单核CPU处理CPU密集型程序，这种情况并不太适合使用多线程。
   1. 多核 ： 如果是多核CPU 处理 CPU 密集型程序，我们完全可以最大化的利用 CPU 核心数，应用并发编程来提高效率。CPU 密集型程序的最佳线程数就是：理论上线程数量 = CPU 核数（逻辑），但是实际上，数量一般会设置为 CPU 核数（逻辑）+ 1（经验值）,计算(CPU)密集型的线程恰好在某时因为发生一个页错误或者因其他原因而暂停，刚好有一个“额外”的线程，可以确保在这种情况下CPU周期不会中断工作
2. I/O 密集型程序，与 CPU 密集型程序相对，一个完整请求，CPU运算操作完成之后还有很多 I/O 操作要做，也就是说 I/O 操作占比很大部分，等待时间较长，线程等待时间所占比例越高，需要越多线程；线程CPU时间所占比例越高，需要越少线程
   1. I/O 密集型程序的最佳线程数就是： 最佳线程数 = CPU核心数 *(1/CPU利用率) = CPU核心数* (1 + (I/O耗时/CPU耗时))
   1. 如果几乎全是 I/O耗时，那么CPU耗时就无限趋近于0，所以纯理论你就可以说是 2N（N=CPU核数），当然也有说 2N + 1的，1应该是backup
   1. 一般我们说 2N + 1 就即可

#  synchronized和lock有哪些区别？

| 区别类型     | synchronized                                                 | Lock                                                         |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 存在层次     | Java的关键字，在jvm层面上                                    | 是JVM的一个接口                                              |
| 锁的获取     | 假设A线程获得锁，B线程等待。如果A线程阻塞，B线程会一直等待   | 情况而定，Lock有多个锁获取的方式，大致就是可以尝试获得锁，线程可以不用一直等待(可以通过tryLock判断有没有锁) |
| 锁的释放     | 1、以获取锁的线程执行完同步代码，释放锁2、线程执行发生异常，jvm会让线程释放 | 在finally中必须释放锁，不然容易造成线程死锁                  |
| 锁类型       | 锁可重入、不可中断、非公平                                   | 可重入、可判断 可公平（两者皆可）                            |
| 性能         | 少量同步                                                     | 适用于大量同步                                               |
| 支持锁的场景 | 1.  独占锁                                                   | 1.  公平锁与非公平锁                                         |

# ABA问题遇到过吗，详细说一下？

1. 有两个线程同时去修改一个变量的值，比如线程1、线程2，都更新变量值，将变量值从A更新成B。
1. 首先线程1获取到CPU的时间片，线程2由于某些原因发生阻塞进行等待，此时线程1进行比较更新（CompareAndSwap），成功将变量的值从A更新成B。
1. 更新完毕之后，恰好又有线程3进来想要把变量的值从B更新成A，线程3进行比较更新，成功将变量的值从B更新成A。
1. 线程2获取到CPU的时间片，然后进行比较更新，发现值是预期的A，然后有更新成了B。但是线程1并不知道，该值已经有了A->B->A这个过程，这也就是我们常说的ABA问题。

# volatile的可见性和禁止指令重排序怎么实现的？

- 可见性：
  volatile的功能就是被修饰的变量在被修改后可以立即同步到主内存，被修饰的变量在每次是用之前都从主内存刷新。本质也是通过内存屏障来实现可见性
  写内存屏障（Store Memory Barrier）可以促使处理器将当前store buffer（存储缓存）的值写回主存。读内存屏障（Load Memory Barrier）可以促使处理器处理invalidate queue（失效队列）。进而避免由于Store Buffer和Invalidate Queue的非实时性带来的问题。
- 禁止指令重排序：
  volatile是通过**内存屏障**来禁止指令重排序
  JMM内存屏障的策略
   - 在每个 volatile 写操作的前面插入一个 StoreStore 屏障。
   - 在每个 volatile 写操作的后面插入一个 StoreLoad 屏障。
   - 在每个 volatile 读操作的后面插入一个 LoadLoad 屏障。
   - 在每个 volatile 读操作的后面插入一个 LoadStore 屏障。

# ConcurrentHashMap底层原理是什么？

1.7
数据结构：
内部主要是一个Segment数组，而数组的每一项又是一个HashEntry数组，元素都存在HashEntry数组里。因为每次锁定的是Segment对象，也就是整个HashEntry数组，所以又叫分段锁。
![1.7ConcurrentHashMap.png](C:/Users/y/Desktop/春招笔记/images/1.7ConcurrentHashMap.png)
1.8
数据结构：
与HashMap一样采用：数组+链表+红黑树
![ConCurrentHashMap.png](C:/Users/y/Desktop/春招笔记/images/ConCurrentHashMap.png)
底层原理则是采用锁链表或者红黑树头结点，相比于HashTable的方法锁，力度更细，是对数组（table）中的桶（链表或者红黑树）的头结点进行锁定，这样锁定，只会影响数组（table）当前下标的数据，不会影响其他下标节点的操作，可以提高读写效率。
putVal执行流程：

1. 判断存储的key、value是否为空，若为空，则抛出异常
1. 计算key的hash值，随后死循环（该循环可以确保成功插入，当满足适当条件时，会主动终止），判断table表为空或者长度为0，则初始化table表
1. 根据hash值获取table中该下标对应的节点，如果该节点为空，则根据参数生成新的节点，并以CAS的方式进行更新，并终止死循环。
1. 如果该节点的hash值是MOVED(-1)，表示正在扩容，则辅助对该节点进行转移。
1. 对数组（table）中的节点，即桶的头结点进行锁定，如果该节点的hash大于等于0，表示此桶是链表，然后对该桶进行遍历（死循环），寻找链表中与put的key的hash值相等，并且key相等的元素，然后进行值的替换，如果到链表尾部都没有符合条件的，就新建一个node，然后插入到该桶的尾部，并终止该循环遍历。
1. 如果该节点的hash小于0，并且节点类型是TreeBin，则走红黑树的插入方式。
1. 判断是否达到转化红黑树的阈值，如果达到阈值，则链表转化为红黑树。



# 分布式id生成方案有哪些？

UUID,数据库主键自增，Redis自增ID，雪花算法。

|                       | 描述                                                         | 优点                                                         | 缺点                                                         |
| --------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| UUID                  | UUID是通用唯一标识码的缩写，其目的是让分布式系统中的所有元素都有唯一的辨识信息，而不需要通过中央控制器来指定唯一标识。 | 1. 降低全局节点的压力，使得主键生成速度更快；<br/>2. 生成的主键全局唯一；<br/>3. 跨服务器合并数据方便。 | 1. UUID占用16个字符，空间占用较多；<br/>2. 不是递增有序的数字，数据写入IO随机性很大，且索引效率下降 |
| 数据库主键自增        | MySQL数据库设置主键且主键自动增长                            | 1. INT和BIGINT类型占用空间较小；<br/>2. 主键自动增长，IO写入连续性好；<br/>3. 数字类型查询速度优于字符串 | 1. 并发性能不高，受限于数据库性能；<br/>2. 分库分表，需要改造，复杂；<br/>3. 自增：数据和数据量泄露 |
| Redis自增             | Redis计数器，原子性自增                                      | 使用内存，并发性能好                                         | 1. 数据丢失；<br/>2. 自增：数据量泄露                        |
| 雪花算法（snowflake） | 大名鼎鼎的雪花算法，分布式ID的经典解决方案                   | 1. 不依赖外部组件；<br/>2. 性能好                            | 时钟回拨                                                     |



# 雪花算法生成的ID由哪些部分组成?

1. 符号位，占用1位。
2. 时间戳，占用41位，可以支持69年的时间跨度。
3. 机器ID，占用10位。
4. 序列号，占用12位。一毫秒可以生成4095个ID。

![image-20210521124236027](C:/Users/y/Desktop/春招笔记/images/image-20210521124236027.png)



# 分布式锁在项目中有哪些应用场景？

使用分布式锁的场景一般需要满足以下场景：

1. 系统是一个分布式系统,集群集群，java的锁已经锁不住了。
2. 操作共享资源，比如库里唯一的用户数据。
3. 同步访问，即多个进程同时操作共享资源。



# 分布锁有哪些解决方案？

1. Reids的分布式锁，很多大公司会基于Reidis做扩展开发。setnx key value ex 10s，Redisson。

   watch dog.

2. 基于Zookeeper。临时节点，顺序节点。

3. 基于数据库，比如Mysql。主键或唯一索引的唯一性。



# Redis做分布式锁用什么命令？

SETNX
格式：setnx key value 将 key 的值设为 value ，当且仅当 key 不存在。
若给定的 key 已经存在，则 SETNX 不做任何动作,操作失败。

SETNX 是『SET if Not eXists』(如果不存在，则 SET)的简写。

加锁：set key value nx ex 10s

释放锁：delete key



# Redis做分布式锁死锁有哪些情况，如何解决？

情况1：加锁，没有释放锁。需要加释放锁的操作。比如delete key。

情况2：加锁后，程序还没有执行释放锁，程序挂了。需要用的key的过期机制。

# Redis如何做分布式锁？

假设有两个服务A、B都希望获得锁，执行过程大致如下:

Step1： 服务A为了获得锁，向Redis发起如下命令: SET productId:lock 0xx9p03001 NX EX 30000 其中，"productId"由自己定义，可以是与本次业务有关的id，"0xx9p03001"是一串随机值，必须保证全局唯一，“NX"指的是当且仅当key(也就是案例中的"productId:lock”)在Redis中不存在时，返回执行成功，否则执行失败。"EX 30000"指的是在30秒后，key将被自动删除。执行命令后返回成功，表明服务成功的获得了锁。

Step2: 服务B为了获得锁，向Redis发起同样的命令: SET productId:lock 0000111 NX  EX 30000
由于Redis内已经存在同名key，且并未过期，因此命令执行失败，服务B未能获得锁。服务B进入循环请求状态，比如每隔1秒钟(自行设置)向Redis发送请求，直到执行成功并获得锁。

Step3: 服务A的业务代码执行时长超过了30秒，导致key超时，因此Redis自动删除了key。此时服务B再次发送命令执行成功，假设本次请求中设置的value值为0000222。此时需要在服务A中对key进行续期，watch dog。

Step4: 服务A执行完毕，为了释放锁，服务A会主动向Redis发起删除key的请求。注意: 在删除key之前，一定要判断服务A持有的value与Redis内存储的value是否一致。比如当前场景下，Redis中的锁早就不是服务A持有的那一把了，而是由服务2创建，如果贸然使用服务A持有的key来删除锁，则会误将服务2的锁释放掉。此外，由于删除锁时涉及到一系列判断逻辑，因此一般使用lua脚本，具体如下:

```sql
if redis.call("get", KEYS[1])==ARGV[1] then
	return redis.call("del", KEYS[1])
else
	return 0
end
```



# 基于 ZooKeeper 的分布式锁实现原理是什么?

顺序节点特性：

使用 ZooKeeper 的顺序节点特性，假如我们在/lock/目录下创建3个节点，ZK集群会按照发起创建的顺序来创建节点，节点分别为/lock/0000000001、/lock/0000000002、/lock/0000000003，最后一位数是依次递增的，节点名由zk来完成。

临时节点特性：

ZK中还有一种名为临时节点的节点，临时节点由某个客户端创建，当客户端与ZK集群断开连接，则该节点自动被删除。EPHEMERAL_SEQUENTIAL为临时顺序节点。

根据ZK中节点是否存在，可以作为分布式锁的锁状态，以此来实现一个分布式锁，下面是分布式锁的基本逻辑：

1. 客户端1调用create()方法创建名为“/业务ID/lock-”的临时顺序节点。
2. 客户端1调用getChildren(“业务ID”)方法来获取所有已经创建的子节点。
3. 客户端获取到所有子节点path之后，如果发现自己在步骤1中创建的节点是所有节点中序号最小的，就是看自己创建的序列号是否排第一，如果是第一，那么就认为这个客户端1获得了锁，在它前面没有别的客户端拿到锁。
4. 如果创建的节点不是所有节点中需要最小的，那么则监视比自己创建节点的序列号小的最大的节点，进入等待。直到下次监视的子节点变更的时候，再进行子节点的获取，判断是否获取锁。



# ZooKeeper和Reids做分布式锁的区别？

Reids：

1. Redis只保证最终一致性，副本间的数据复制是异步进行（Set是写，Get是读，Reids集群一般是读写分离架构，存在主从同步延迟情况），主从切换之后可能有部分数据没有复制过去可能会 **「丢失锁」** 情况，故强一致性要求的业务不推荐使用Reids，推荐使用zk。
2. Redis集群各方法的响应时间均为最低。随着并发量和业务数量的提升其响应时间会有明显上升（公网集群影响因素偏大），但是极限qps可以达到最大且基本无异常

ZooKeeper：

1. 使用ZooKeeper集群，锁原理是使用ZooKeeper的临时顺序节点，临时顺序节点的生命周期在Client与集群的Session结束时结束。因此如果某个Client节点存在网络问题，与ZooKeeper集群断开连接，Session超时同样会导致锁被错误的释放（导致被其他线程错误地持有），因此ZooKeeper也无法保证完全一致。
2. ZK具有较好的稳定性；响应时间抖动很小，没有出现异常。但是随着并发量和业务数量的提升其响应时间和qps会明显下降。

总结：

1. Zookeeper每次进行锁操作前都要创建若干节点，完成后要释放节点，会浪费很多时间；
2. 而Redis只是简单的数据操作，没有这个问题。





# MySQL如何做分布式锁？

在Mysql中创建一张表，设置一个 主键或者UNIQUE KEY 这个 KEY 就是要锁的 KEY（商品ID），所以同一个 KEY 在mysql表里只能插入一次了，这样对锁的竞争就交给了数据库，处理同一个 KEY 数据库保证了只有一个节点能插入成功，其他节点都会插入失败。

DB分布式锁的实现：通过主键id 或者 唯一索性 的唯一性进行加锁，说白了就是加锁的形式是向一张表中插入一条数据，该条数据的id就是一把分布式锁，例如当一次请求插入了一条id为1的数据，其他想要进行插入数据的并发请求必须等第一次请求执行完成后删除这条id为1的数据才能继续插入，实现了分布式锁的功能。

这样 lock 和 unlock 的思路就很简单了，伪代码：

```
def lock ：
    exec sql: insert into locked—table (xxx) values (xxx)
    if result == true :
        return true
    else :
        return false

def unlock ：
    exec sql: delete from lockedOrder where order_id='order_id'
```





# 计数器算法是什么？

​		计数器算法，是指在指定的时间周期内累加访问次数，达到设定的阈值时，触发限流策略。下一个时间周期进行访问时，访问次数清零。此算法无论在单机还是分布式环境下实现都非常简单，使用redis的incr原子自增性，再结合key的过期时间，即可轻松实现。

![4-6 计数器算法](C:/Users/y/Desktop/春招笔记/images/4-6 计数器算法-1621753094321.jpg)



​		从上图我们来看，我们设置一分钟的阈值是100，在0:00到1:00内请求数是60，当到1:00时，请求数清零，从0开始计算，这时在1:00到2:00之间我们能处理的最大的请求为100，超过100个的请求，系统都拒绝。

​		这个算法有一个临界问题，比如在上图中，在0:00到1:00内，只在0:50有60个请求，而在1:00到2:00之间，只在1:10有60个请求，虽然在两个一分钟的时间内，都没有超过100个请求，但是在0:50到1:10这20秒内，确有120个请求，虽然在每个周期内，都没超过阈值，但是在这20秒内，已经远远超过了我们原来设置的1分钟内100个请求的阈值。



# 滑动时间窗口算法是什么？

​		为了解决计数器算法的临界值的问题，发明了滑动窗口算法。在TCP网络通信协议中，就采用滑动时间窗口算法来解决网络拥堵问题。

​		滑动时间窗口是将计数器算法中的实际周期切分成多个小的时间窗口，分别在每个小的时间窗口中记录访问次数，然后根据时间将窗口往前滑动并删除过期的小时间窗口。最终只需要统计滑动窗口范围内的小时间窗口的总的请求数即可。

![4-7 滑动窗口算法](C:/Users/y/Desktop/春招笔记/images/4-7 滑动窗口算法-1621753118270.jpg)



​		在上图中，假设我们设置一分钟的请求阈值是100，我们将一分钟拆分成4个小时间窗口，这样，每个小的时间窗口只能处理25个请求，我们用虚线方框表示滑动时间窗口，当前窗口的大小是2，也就是在窗口内最多能处理50个请求。随着时间的推移，滑动窗口也随着时间往前移动，比如上图开始时，窗口是0:00到0:30的这个范围，过了15秒后，窗口是0:15到0:45的这个范围，窗口中的请求重新清零，这样就很好的解决了计数器算法的临界值问题。

​		在滑动时间窗口算法中，我们的小窗口划分的越多，滑动窗口的滚动就越平滑，限流的统计就会越精确。



# 漏桶限流算法是什么？

​		漏桶算法的原理就像它的名字一样，我们维持一个漏斗，它有恒定的流出速度，不管水流流入的速度有多快，漏斗出水的速度始终保持不变，类似于消息中间件，不管消息的生产者请求量有多大，消息的处理能力取决于消费者。

​		漏桶的容量=漏桶的流出速度*可接受的等待时长。在这个容量范围内的请求可以排队等待系统的处理，超过这个容量的请求，才会被抛弃。

​		在漏桶限流算法中，存在下面几种情况：

1. 当请求速度大于漏桶的流出速度时，也就是请求量大于当前服务所能处理的最大极限值时，触发限流策略。

2. 请求速度小于或等于漏桶的流出速度时，也就是服务的处理能力大于或等于请求量时，正常执行。

   漏桶算法有一个缺点：当系统在短时间内有突发的大流量时，漏桶算法处理不了。



# 令牌桶限流算法是什么？

​		令牌桶算法，是增加一个大小固定的容器，也就是令牌桶，系统以恒定的速率向令牌桶中放入令牌，如果有客户端来请求，先需要从令牌桶中拿一个令牌，拿到令牌，才有资格访问系统，这时令牌桶中少一个令牌。当令牌桶满的时候，再向令牌桶生成令牌时，令牌会被抛弃。

​		在令牌桶算法中，存在以下几种情况：

1. 请求速度大于令牌的生成速度：那么令牌桶中的令牌会被取完，后续再进来的请求，由于拿不到令牌，会被限流。

2. 请求速度等于令牌的生成速度：那么此时系统处于平稳状态。

3. 请求速度小于令牌的生成速度：那么此时系统的访问量远远低于系统的并发能力，请求可以被正常处理。

   令牌桶算法，由于有一个桶的存在，可以处理短时间大流量的场景。这是令牌桶和漏桶的一个区别。

# 你设计微服务时遵循什么原则？

1. 单一职责原则：让每个服务能独立，有界限的工作，每个服务只关注自己的业务。做到高内聚。
2. 服务自治原则：每个服务要能做到独立开发、独立测试、独立构建、独立部署，独立运行。与其他服务进行解耦。
3. 轻量级通信原则：让每个服务之间的调用是轻量级，并且能够跨平台、跨语言。比如采用RESTful风格，利用消息队列进行通信等。
4. 粒度进化原则：对每个服务的粒度把控，其实没有统一的标准，这个得结合我们解决的具体业务问题。不要过度设计。服务的粒度随着业务和用户的发展而发展。

​	总结一句话，软件是为业务服务的，好的系统不是设计出来的，而是进化出来的。

# CAP定理是什么？

​		CAP定理，又叫布鲁尔定理。指的是：在一个分布式系统中，最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项。

+ C：一致性（Consistency），数据在多个副本中保持一致，可以理解成两个用户访问两个系统A和B，当A系统数据有变化时，及时同步给B系统，让两个用户看到的数据是一致的。

+ A：可用性（Availability），系统对外提供服务必须一直处于可用状态，在任何故障下，客户端都能在合理时间内获得服务端非错误的响应。

+ P：分区容错性（Partition tolerance），在分布式系统中遇到任何网络分区故障，系统仍然能对外提供服务。网络分区，可以这样理解，在分布式系统中，不同的节点分布在不同的子网络中，有可能子网络中只有一个节点，在所有网络正常的情况下，由于某些原因导致这些子节点之间的网络出现故障，导致整个节点环境被切分成了不同的独立区域，这就是网络分区。

  ​	

  我们来详细分析一下CAP，为什么只能满足两个。看下图所示：

  ![10-4 CAP演示](C:/Users/y/Desktop/春招笔记/images/10-4 CAP演示-1617721637028.jpg)

  

  ​		用户1和用户2分别访问系统A和系统B，系统A和系统B通过网络进行同步数据。理想情况是：用户1访问系统A对数据进行修改，将data1改成了data2，同时用户2访问系统B，拿到的是data2数据。

  ​		但是实际中，由于分布式系统具有八大谬论：

  + 网络相当可靠

  + 延迟为零

  + 传输带宽是无限的

  + 网络相当安全

  + 拓扑结构不会改变

  + 必须要有一名管理员

  + 传输成本为零

  + 网络同质化

  我们知道，只要有网络调用，网络总是不可靠的。我们来一一分析。

  1. 当网络发生故障时，系统A和系统B没法进行数据同步，也就是我们不满足P，同时两个系统依然可以访问，那么此时其实相当于是单机系统，就不是分布式系统了，所以既然我们是分布式系统，P必须满足。
  2. 当P满足时，如果用户1通过系统A对数据进行了修改将data1改成了data2，也要让用户2通过系统B正确的拿到data2，那么此时是满足C，就必须等待网络将系统A和系统B的数据同步好，并且在同步期间，任何人不能访问系统B（让系统不可用），否则数据就不是一致的。此时满足的是CP。
  3. 当P满足时，如果用户1通过系统A对数据进行了修改将data1改成了data2，也要让系统B能继续提供服务，那么此时，只能接受系统A没有将data2同步给系统B（牺牲了一致性）。此时满足的就是AP。

​        我们在前面学过的注册中心Eureka就是满足 的AP，它并不保证C。而Zookeeper是保证CP，它不保证A。在生产中，A和C的选择，没有正确的答案，是取决于自己的业务的。比如12306，是满足CP，因为买票必须满足数据的一致性，不然一个座位多卖了，对铁路运输都是不可以接受的。

# BASE理论是什么？

由于CAP中一致性C和可用性A无法兼得，eBay的架构师，提出了BASE理论，它是通过牺牲数据的强一致性，来获得可用性。它由于如下3种特征：

+ **B**asically **A**vailable（基本可用）：分布式系统在出现不可预知故障的时候，允许损失部分可用性，保证核心功能的可用。

+ **S**oft state（软状态）：软状态也称为弱状态，和硬状态相对，是指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。、

+ **E**ventually consistent（最终一致性）：最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。

​        BASE理论并没有要求数据的强一致性，而是允许数据在一定的时间段内是不一致的，但在最终某个状态会达到一致。在生产环境中，很多公司，会采用BASE理论来实现数据的一致，因为产品的可用性相比强一致性来说，更加重要。比如在电商平台中，当用户对一个订单发起支付时，往往会调用第三方支付平台，比如支付宝支付或者微信支付，调用第三方成功后，第三方并不能及时通知我方系统，在第三方没有通知我方系统的这段时间内，我们给用户的订单状态显示支付中，等到第三方回调之后，我们再将状态改成已支付。虽然订单状态在短期内存在不一致，但是用户却获得了更好的产品体验。

# 2PC提交协议是什么？

二阶段提交(Two-phaseCommit)是指，在计算机网络以及数据库领域内，为了使基于分布式系统架构下的所有节点在进行事务提交时保持一致性而设计的一种算法(Algorithm)。通常，二阶段提交也被称为是一种协议(Protocol))。在分布式系统中，每个节点虽然可以知晓自己的操作时成功或者失败，却无法知道其他节点的操作的成功或失败。当一个事务跨越多个节点时，为了保持事务的ACID特性，需要引入一个作为协调者的组件来统一掌控所有节点(称作参与者)的操作结果并最终指示这些节点是否要把操作结果进行真正的提交(比如将更新后的数据写入磁盘等等)。因此，**二阶段提交的算法思路可以概括为：参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。**



所谓的两个阶段是指：第一阶段：**准备阶段(投票阶段)**和第二阶段：**提交阶段（执行阶段）**。

准备阶段

事务协调者(事务管理器)给每个参与者(资源管理器)发送Prepare消息，每个参与者要么直接返回失败(如权限验证失败)，要么在本地执行事务，写本地的redo和undo日志，但不提交，到达一种“万事俱备，只欠东风”的状态。

可以进一步将准备阶段分为以下三个步骤：

> 1）协调者节点向所有参与者节点询问是否可以执行提交操作(vote)，并开始等待各参与者节点的响应。
>
> 2）参与者节点执行询问发起为止的所有事务操作，并将Undo信息和Redo信息写入日志。（注意：若成功这里其实每个参与者已经执行了事务操作）
>
> 3）各参与者节点响应协调者节点发起的询问。如果参与者节点的事务操作实际执行成功，则它返回一个”同意”消息；如果参与者节点的事务操作实际执行失败，则它返回一个”中止”消息。

提交阶段

> 如果协调者收到了参与者的失败消息或者超时，直接给每个参与者发送回滚(Rollback)消息；否则，发送提交(Commit)消息；参与者根据协调者的指令执行提交或者回滚操作，释放所有事务处理过程中使用的锁资源。(注意:必须在最后阶段释放锁资源)
>
> 接下来分两种情况分别讨论提交阶段的过程。
>
> 当协调者节点从所有参与者节点获得的相应消息都为”同意”时:
>
> [![success](C:/Users/y/Desktop/春招笔记/images/success.png)](http://www.hollischuang.com/wp-content/uploads/2015/12/success.png)
>
> > 1）协调者节点向所有参与者节点发出”正式提交(commit)”的请求。
> >
> > 2）参与者节点正式完成操作，并释放在整个事务期间内占用的资源。
> >
> > 3）参与者节点向协调者节点发送”完成”消息。
> >
> > 4）协调者节点受到所有参与者节点反馈的”完成”消息后，完成事务。

如果任一参与者节点在第一阶段返回的响应消息为”中止”，或者 协调者节点在第一阶段的询问超时之前无法获取所有参与者节点的响应消息时：

[![fail](C:/Users/y/Desktop/春招笔记/images/fail.png)](http://www.hollischuang.com/wp-content/uploads/2015/12/fail.png)

> 1）协调者节点向所有参与者节点发出”回滚操作(rollback)”的请求。
>
> 2）参与者节点利用之前写入的Undo信息执行回滚，并释放在整个事务期间内占用的资源。
>
> 3）参与者节点向协调者节点发送”回滚完成”消息。
>
> 4）协调者节点受到所有参与者节点反馈的”回滚完成”消息后，取消事务。

　　不管最后结果如何，第二阶段都会结束当前事务。

# 2PC提交协议有什么缺点？

1. **同步阻塞问题**。执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。

2. **单点故障**。由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题）

3. **数据不一致**。在二阶段提交的阶段二中，当协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送commit请求过程中协调者发生了故障，这回导致只有一部分参与者接受到了commit请求。而在这部分参与者接到commit请求之后就会执行commit操作。但是其他部分未接到commit请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据部一致性的现象。

4. 二阶段无法解决的问题：协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。



# 3PC提交协议是什么？

CanCommit阶段

3PC的CanCommit阶段其实和2PC的准备阶段很像。协调者向参与者发送commit请求，参与者如果可以提交就返回Yes响应，否则返回No响应。

> **1.事务询问** 协调者向参与者发送CanCommit请求。询问是否可以执行事务提交操作。然后开始等待参与者的响应。
>
> **2.响应反馈** 参与者接到CanCommit请求之后，正常情况下，如果其自身认为可以顺利执行事务，则返回Yes响应，并进入预备状态。否则反馈No

PreCommit阶段

协调者根据参与者的反应情况来决定是否可以进行事务的PreCommit操作。根据响应情况，有以下两种可能。

**假如协调者从所有的参与者获得的反馈都是Yes响应，那么就会执行事务的预执行。**

> **1.发送预提交请求** 协调者向参与者发送PreCommit请求，并进入Prepared阶段。
>
> **2.事务预提交** 参与者接收到PreCommit请求后，会执行事务操作，并将undo和redo信息记录到事务日志中。
>
> **3.响应反馈** 如果参与者成功的执行了事务操作，则返回ACK响应，同时开始等待最终指令。

**假如有任何一个参与者向协调者发送了No响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断。**

> **1.发送中断请求** 协调者向所有参与者发送abort请求。
>
> **2.中断事务** 参与者收到来自协调者的abort请求之后（或超时之后，仍未收到协调者的请求），执行事务的中断。



pre阶段参与者没收到请求，rollback。

doCommit阶段

该阶段进行真正的事务提交，也可以分为以下两种情况。

**执行提交**

> **1.发送提交请求** 协调接收到参与者发送的ACK响应，那么他将从预提交状态进入到提交状态。并向所有参与者发送doCommit请求。
>
> **2.事务提交** 参与者接收到doCommit请求之后，执行正式的事务提交。并在完成事务提交之后释放所有事务资源。
>
> **3.响应反馈** 事务提交完之后，向协调者发送Ack响应。
>
> **4.完成事务** 协调者接收到所有参与者的ack响应之后，完成事务。

**中断事务** 协调者没有接收到参与者发送的ACK响应（可能是接受者发送的不是ACK响应，也可能响应超时），那么就会执行中断事务。

> **1.发送中断请求** 协调者向所有参与者发送abort请求
>
> **2.事务回滚** 参与者接收到abort请求之后，利用其在阶段二记录的undo信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。
>
> **3.反馈结果** 参与者完成事务回滚之后，向协调者发送ACK消息
>
> **4.中断事务** 协调者接收到参与者反馈的ACK消息之后，执行事务的中断。

# 2PC和3PC的区别是什么？

1、引入超时机制。同时在协调者和参与者中都引入超时机制。

2、三阶段在2PC的第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。



+ # TCC解决方案是什么?

  ​		TCC（Try-Confirm-Cancel）是一种常用的分布式事务解决方案，它将一个事务拆分成三个步骤：

  + T（Try）：业务检查阶段，这阶段主要进行业务校验和检查或者资源预留；也可能是直接进行业务操作。

  + C（Confirm）：业务确认阶段，这阶段对Try阶段校验过的业务或者预留的资源进行确认。

  + C（Cancel）：业务回滚阶段，这阶段和上面的C（Confirm）是互斥的，用于释放Try阶段预留的资源或者业务。

  ![image-20210521230854476](C:/Users/y/Desktop/春招笔记/images/image-20210521230854476-1621753201509.png)

  

  ![image-20210521230904203](C:/Users/y/Desktop/春招笔记/images/image-20210521230904203-1621753201509.png)

  

  ![image-20210521230912365](C:/Users/y/Desktop/春招笔记/images/image-20210521230912365-1621753201509.png)

  

  ![image-20210521230919795](C:/Users/y/Desktop/春招笔记/images/image-20210521230919795-1621753201509.png)

  

# TCC空回滚是解决什么问题的？

​        在没有调用TCC资源Try方法的情况下，调用了二阶段的Cancel方法。比如当Try请求由于网络延迟或故障等原因，没有执行，结果返回了异常，那么此时Cancel就不能正常执行，因为Try没有对数据进行修改，如果Cancel进行了对数据的修改，那就会导致数据不一致。
​		解决思路是关键就是要识别出这个空回滚。思路很简单就是需要知道Try阶段是否执行，如果执行了，那就是正常回滚；如果没执行，那就是空回滚。建议TM在发起全局事务时生成全局事务记录，全局事务ID贯穿整个分布式事务调用链条。再额外增加一张**分支事务记录表**，其中有全局事务ID和分支事务ID，第一阶段Try方法里会插入一条记录，表示Try阶段执行了。Cancel接口里读取该记录，如果该记录存在，则正常回滚；如果该记录不存在，则是空回滚。

# 如何解决TCC幂等问题？

为了保证TCC二阶段提交重试机制不会引发数据不一致，要求TCC的二阶段Confirm和Cancel接口保证幂等，这样不会重复使用或者释放资源。如果幂等控制没有做好，很有可能导致数据不一致等严重问题。
解决思路在上述 **分支事务记录**中增加执行状态，每次执行前都查询该状态。



分布式锁。

# 如何解决TCC中悬挂问题？

悬挂就是对于一个分布式事务，其二阶段Cancel接口比Try接口先执行。
		出现原因是在调用分支事务Try时，由于网络发生拥堵，造成了超时，TM就会通知RM回滚该分布式事务，可能回滚完成后，Try请求才到达参与者真正执行，而一个Try方法预留的业务资源，只有该分布式事务才能使用，该分布式事务第一阶段预留的业务资源就再也没有人能够处理了，对于这种情况，我们就称为悬挂，即业务资源预留后无法继续处理。
		解决思路是如果二阶段执行完成，那一阶段就不能再继续执行。在执行一阶段事务时判断在该全局事务下，判断**分支事务记录表**中是否已经有二阶段事务记录，如果有则不执行Try。



# 可靠消息服务方案是什么？

​		可靠消息最终一致性方案指的是：当事务的发起方（事务参与者，消息发送者）执行完本地事务后，同时发出一条消息，事务参与方（事务参与者，消息的消费者）一定能够接受消息并可以成功处理自己的事务。

​		这里面强调两点：

1. 可靠消息：发起方一定得把消息传递到消费者。
2. 最终一致性：最终发起方的业务处理和消费方的业务处理得完成，达成最终一致。

![image-20210522125830646](C:/Users/y/Desktop/春招笔记/images/image-20210522125830646.png)



# 最大努力通知方案的关键是什么？

1.  有一定的消息重复通知机制。因为接收通知方（上图中的我方支付系统）可能没有接收到通知，此时要有一定的机制对消息重复通知。
2.  消息校对机制。如果尽最大努力也没有通知到接收方，或者接收方消费消息后要再次消费，此时可由接收方主动向通知方查询消息信息来满足需求。

# 什么是分布式系统中的幂等？

幂等（idempotent、idempotence）是一个数学与计算机学概念，常见于抽象代数中。

在编程中，一个幂等操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同。幂等函数，或幂等方法，是指可以使用相同参数重复执行，并能获得相同结果的函数。这些函数不会影响系统状态，也不用担心重复执行会对系统造成改变。

例如，“getUsername()和 setTrue()”函数就是一个幂等函数. 更复杂的操作幂等保证是利用唯一交易号(流水号)实现. 我的理解：幂等就是一个操作，不论执行多少次，产生的效果和返回的结果都是一样的。



操作：查询，set固定值。逻辑删除。set 固定值。

流程：分布式系统中，网络调用，重试机制。



# 幂等有哪些技术解决方案？

1.查询操作

查询一次和查询多次，在数据不变的情况下，查询结果是一样的。select 是天然的幂等操作；

2.删除操作

删除操作也是幂等的，删除一次和多次删除都是把数据删除。(注意可能返回结果不一样，删除的数据不存在，返回 0，删除的数据多条，返回结果多个。

3.唯一索引

防止新增脏数据。比如：支付宝的资金账户，支付宝也有用户账户，每个用户只能有一个资金账户，怎么防止给用户创建多个资金账户，那么给资金账户表中的用户 ID 加唯一索引，所以一个用户新增成功一个资金账户记录。要点：唯一索引或唯一组合索引来防止新增数据存在脏数据（当表存在唯一索引，并发时新增报错时，再查询一次就可以了，数据应该已经存在了，返回结果即可。

4.token 机制

防止页面重复提交。

**业务要求：**页面的数据只能被点击提交一次；

**发生原因：**由于重复点击或者网络重发，或者 nginx 重发等情况会导致数据被重复提交；

**解决办法：**集群环境采用 token 加 redis(redis 单线程的，处理需要排队)；单 JVM 环境：采用 token 加 redis 或 token 加 jvm 锁。

**处理流程：**

1. 数据提交前要向服务的申请 token，token 放到 redis 或 jvm 内存，token 有效时间；
2. 提交后后台校验 token，同时删除 token，生成新的 token 返回。

**token 特点：**要申请，一次有效性，可以限流。

注意：redis 要用删除操作来判断 token，删除成功代表 token 校验通过。

5. traceId

   操作时唯一的。

# 对外提供的API如何保证幂等？

举例说明： 银联提供的付款接口：需要接入商户提交付款请求时附带：source 来源，seq 序列号。

source+seq 在数据库里面做唯一索引，防止多次付款(并发时，只能处理一个请求) 。重点：对外提供接口为了支持幂等调用，接口有两个字段必须传，一个是来源 source，一个是来源方序列号 seq，这个两个字段在提供方系统里面做联合唯一索引，这样当第三方调用时，先在本方系统里面查询一下，是否已经处理过，返回相应处理结果；没有处理过，进行相应处理，返回结果。

注意，为了幂等友好，一定要先查询一下，是否处理过该笔业务，不查询直接插入业务系统，会报错，但实际已经处理。



# 双写一致性问题如何解决？

先做一个说明，从理论上来说，给缓存设置过期时间，是保证最终一致性的解决方案。这种方案下，我们可以对存入缓存的数据设置过期时间，所有的写操作以数据库为准，对缓存操作只是尽最大努力更新即可。也就是说如果数据库写成功，缓存更新失败，那么只要到达过期时间，则后面的读请求自然会从数据库中读取新值然后回填缓存。因此，接下来讨论的思路不依赖于给缓存设置过期时间这个方案。
在这里，我们讨论**三种**更新策略：

1. 先更新缓存，再更新数据库。（不可取）
2. 先更新数据库，再更新缓存。（不可取）
3. 先删除缓存，再更新数据库。（不可取）
4. 先更新数据库，再删除缓存。（可取，有问题待解决）

### 大前提：

先读缓存，如果缓存没有，才从数据库读取。

### (1)先更新数据库，再更新缓存

这套方案，大家是普遍反对的。为什么呢？有如下两点原因。
**原因一（线程安全角度）**
同时有请求A和请求B进行更新操作，那么会出现
（1）线程A更新了数据库
（2）线程B更新了数据库
（3）线程B更新了缓存
（4）线程A更新了缓存
这就出现请求A更新缓存应该比请求B更新缓存早才对，但是因为网络等原因，B却比A更早更新了缓存。这就导致了脏数据，因此不考虑。
**原因二（业务场景角度）**
有如下两点：
（1）如果你是一个写数据库场景比较多，而读数据场景比较少的业务需求，采用这种方案就会导致，数据压根还没读到，缓存就被频繁的更新，浪费性能。
（2）如果你写入数据库的值，并不是直接写入缓存的，而是要经过一系列复杂的计算再写入缓存。那么，每次写入数据库后，都再次计算写入缓存的值，无疑是浪费性能的。显然，删除缓存更为适合。

接下来讨论的就是争议最大的，先删缓存，再更新数据库。还是先更新数据库，再删缓存的问题。

### (2)先删缓存，再更新数据库

该方案会导致不一致的原因是。同时有一个请求A进行更新操作，另一个请求B进行查询操作。那么会出现如下情形:
（1）请求A进行写操作，删除缓存
（2）请求B查询发现缓存不存在
（3）请求B去数据库查询得到旧值
（4）请求B将旧值写入缓存
（5）请求A将新值写入数据库
上述情况就会导致不一致的情形出现。而且，如果不采用给缓存设置过期时间策略，该数据永远都是脏数据。
那么，**如何解决呢？采用延时双删策略**

（1）先淘汰缓存
（2）再写数据库（这两步和原来一样）
（3）休眠1秒，再次淘汰缓存
这么做，可以将1秒内所造成的缓存脏数据，再次删除。
**那么，这个1秒怎么确定的，具体该休眠多久呢？**
针对上面的情形，读者应该自行评估自己的项目的读数据业务逻辑的耗时。然后写数据的休眠时间则在读数据业务逻辑的耗时基础上，加几百ms即可。这么做的目的，就是确保读请求结束，写请求可以删除读请求造成的缓存脏数据。
**如果你用了mysql的读写分离架构怎么办？**
ok，在这种情况下，造成数据不一致的原因如下，还是两个请求，一个请求A进行更新操作，另一个请求B进行查询操作。
（1）请求A进行写操作，删除缓存
（2）请求A将数据写入数据库了，
（3）请求B查询缓存发现，缓存没有值
（4）请求B去从库查询，这时，还没有完成主从同步，因此查询到的是旧值
（5）请求B将旧值写入缓存
（6）数据库完成主从同步，从库变为新值
上述情形，就是数据不一致的原因。还是使用双删延时策略。只是，睡眠时间修改为在主从同步的延时时间基础上，加几百ms。
**采用这种同步淘汰策略，吞吐量降低怎么办？**
ok，那就将第二次删除作为异步的。自己起一个线程，异步删除。这样，写的请求就不用沉睡一段时间后了，再返回。这么做，加大吞吐量。
**第二次删除,如果删除失败怎么办？**
这是个非常好的问题，因为第二次删除失败，就会出现如下情形。还是有两个请求，一个请求A进行更新操作，另一个请求B进行查询操作，为了方便，假设是单库：
（1）请求A进行写操作，删除缓存
（2）请求B查询发现缓存不存在
（3）请求B去数据库查询得到旧值
（4）请求B将旧值写入缓存
（5）请求A将新值写入数据库
（6）请求A试图去删除，请求B写入对的缓存值，结果失败了。
ok,这也就是说。如果第二次删除缓存失败，会再次出现缓存和数据库不一致的问题。
**如何解决呢？**

### (3)先更新数据库，再删缓存

首先，先说一下。老外提出了一个缓存更新套路，名为[《Cache-Aside pattern》](https://docs.microsoft.com/en-us/azure/architecture/patterns/cache-aside)。其中就指出

- **失效**：应用程序先从cache取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。
- **命中**：应用程序从cache中取数据，取到后返回。
- **更新**：先把数据存到数据库中，成功后，再让缓存失效。

另外，知名社交网站facebook也在论文[《Scaling Memcache at Facebook》](https://www.usenix.org/system/files/conference/nsdi13/nsdi13-final170_update.pdf)中提出，他们用的也是先更新数据库，再删缓存的策略。
**这种情况不存在并发问题么？**
不是的。假设这会有两个请求，一个请求A做查询操作，一个请求B做更新操作，那么会有如下情形产生
（1）缓存刚好失效
（2）请求A查询数据库，得一个旧值
（3）请求B将新值写入数据库
（4）请求B删除缓存
（5）请求A将查到的旧值写入缓存
ok，如果发生上述情况，确实是会发生脏数据。
**然而，发生这种情况的概率又有多少呢？**
发生上述情况有一个先天性条件，就是步骤（3）的写数据库操作比步骤（2）的读数据库操作耗时更短，才有可能使得步骤（4）先于步骤（5）。可是，大家想想，数据库的读操作的速度远快于写操作的（不然做读写分离干嘛，做读写分离的意义就是因为读操作比较快，耗资源少），因此步骤（3）耗时比步骤（2）更短，这一情形很难出现。
假设，有人非要抬杠，有强迫症，一定要解决怎么办？
**如何解决上述并发问题？**
首先，给缓存设有效时间是一种方案。其次，采用策略（2）里给出的异步延时删除策略，保证读请求完成以后，再进行删除操作。
**还有其他造成不一致的原因么？**
有的，这也是缓存更新策略（2）和缓存更新策略（3）都存在的一个问题，如果删缓存失败了怎么办，那不是会有不一致的情况出现么。比如一个写数据请求，然后写入数据库了，删缓存失败了，这会就出现不一致的情况了。这也是缓存更新策略（2）里留下的最后一个疑问。
**如何解决？**
提供一个保障的重试机制即可，这里给出两套方案。
**方案一**：
如下图所示
![image](C:/Users/y/Desktop/春招笔记/images/o_update1.png)
流程如下所示
（1）更新数据库数据；
（2）缓存因为种种问题删除失败
（3）将需要删除的key发送至消息队列
（4）自己消费消息，获得需要删除的key
（5）继续重试删除操作，直到成功
然而，该方案有一个缺点，对业务线代码造成大量的侵入。于是有了方案二，在方案二中，启动一个订阅程序去订阅数据库的binlog，获得需要操作的数据。在应用程序中，另起一段程序，获得这个订阅程序传来的信息，进行删除缓存操作。
**方案二**：
![image](C:/Users/y/Desktop/春招笔记/images/o_update2.png)
流程如下图所示：
（1）更新数据库数据
（2）数据库会将操作信息写入binlog日志当中
（3）订阅程序提取出所需要的数据以及key
（4）另起一段非业务代码，获得该信息
（5）尝试删除缓存操作，发现删除失败
（6）将这些信息发送至消息队列
（7）重新从消息队列中获得该数据，重试操作。

**备注说明：**上述的订阅binlog程序在mysql中有现成的中间件叫canal，可以完成订阅binlog日志的功能。至于oracle中，博主目前不知道有没有现成中间件可以使用。另外，重试机制，博主是采用的是消息队列的方式。如果对一致性要求不是很高，直接在程序中另起一个线程，每隔一段时间去重试即可，这些大家可以灵活自由发挥，只是提供一个思路。







# 分布式微服务项目你是如何设计的？

我一般设计成两层：业务层和能力层（中台），业务层接受用户请求，然后通过调用能力层来完成业务逻辑。

![image-20210522172654370](C:/Users/y/Desktop/春招笔记/images/image-20210522172654370.png)



# 认证 (Authentication) 和授权 (Authorization)的区别是什么？

Authentication（认证） 是验证您的身份的凭据（例如用户名/用户ID和密码），通过这个凭据，系统得以知道你就是你，也就是说系统存在你这个用户。所以，Authentication 被称为身份/用户验证。
Authorization（授权） 发生在 Authentication（认证） 之后。授权，它主要掌管我们访问系统的权限。比如有些特定资源只能具有特定权限的人才能访问比如admin，有些对系统资源操作比如删除、添加、更新只能特定人才具有。
这两个一般在我们的系统中被结合在一起使用，目的就是为了保护我们系统的安全性。



# Cookie 和 Session 有什么区别？如何使用Session进行身份验证？

Session 的主要作用就是通过服务端记录用户的状态。 典型的场景是购物车，当你要添加商品到购物车的时候，系统不知道是哪个用户操作的，因为 HTTP 协议是无状态的。服务端给特定的用户创建特定的 Session 之后就可以标识这个用户并且跟踪这个用户了。

Cookie 数据保存在客户端(浏览器端)，Session 数据保存在服务器端。相对来说 Session 安全性更高。如果使用 Cookie 的一些敏感信息不要写入 Cookie 中，最好能将 Cookie 信息加密然后使用到的时候再去服务器端解密。

那么，如何使用Session进行身份验证？

很多时候我们都是通过 SessionID 来实现特定的用户，SessionID 一般会选择存放在 Redis 中。举个例子：用户成功登陆系统，然后返回给客户端具有 SessionID 的 Cookie，当用户向后端发起请求的时候会把 SessionID 带上，这样后端就知道你的身份状态了。关于这种认证方式更详细的过程如下：

![image-20210520130119426](C:/Users/y/Desktop/春招笔记/images/image-20210520130119426.png)

用户向服务器发送用户名和密码用于登陆系统。
服务器验证通过后，服务器为用户创建一个 Session，并将 Session信息存储 起来。
服务器向用户返回一个 SessionID，写入用户的 Cookie。
当用户保持登录状态时，Cookie 将与每个后续请求一起被发送出去。
服务器可以将存储在 Cookie 上的 Session ID 与存储在内存中或者数据库中的 Session 信息进行比较，以验证用户的身份，返回给用户客户端响应信息的时候会附带用户当前的状态。
使用 Session 的时候需要注意下面几个点：

依赖Session的关键业务一定要确保客户端开启了Cookie。
注意Session的过期时间





# 为什么Cookie 无法防止CSRF攻击，而token可以？

**CSRF（Cross Site Request Forgery）**一般被翻译为 跨站请求伪造 。那么什么是 跨站请求伪造 呢？说简单用你的身份去发送一些对你不友好的请求。举个简单的例子：

小壮登录了某网上银行，他来到了网上银行的帖子区，看到一个帖子下面有一个链接写着“科学理财，年盈利率过万”，小壮好奇的点开了这个链接，结果发现自己的账户少了10000元。这是这么回事呢？原来黑客在链接中藏了一个请求，这个请求直接利用小壮的身份给银行发送了一个转账请求,也就是通过你的 Cookie 向银行发出请求。

<a src=http://www.mybank.com/Transfer?bankId=11&money=10000>科学理财，年盈利率过万</>
进行Session 认证的时候，我们一般使用 Cookie 来存储 SessionId,当我们登陆后后端生成一个SessionId放在Cookie中返回给客户端，服务端通过Redis或者其他存储工具记录保存着这个Sessionid，客户端登录以后每次请求都会带上这个SessionId，服务端通过这个SessionId来标示你这个人。如果别人通过 cookie拿到了 SessionId 后就可以代替你的身份访问系统了。

Session 认证中 Cookie 中的 SessionId是由浏览器发送到服务端的，借助这个特性，攻击者就可以通过让用户误点攻击链接，达到攻击效果。

但是，我们使用 token 的话就不会存在这个问题，在我们登录成功获得 token 之后，一般会选择存放在 local storage 中。然后我们在前端通过某些方式会给每个发到后端的请求加上这个 token,这样就不会出现 CSRF 漏洞的问题。因为，即使有个你点击了非法链接发送了请求到服务端，这个非法请求是不会携带 token 的，所以这个请求将是非法的。





# 什么是 Token?什么是 JWT?如何基于Token进行身份验证？

我们知道 Session 信息需要保存一份在服务器端。这种方式会带来一些麻烦，比如需要我们保证保存 Session 信息服务器的可用性、不适合移动端（依赖Cookie）等等。

有没有一种不需要自己存放 Session 信息就能实现身份验证的方式呢？使用 Token 即可！JWT （JSON Web Token） 就是这种方式的实现，通过这种方式服务器端就不需要保存 Session 数据了，只用在客户端保存服务端返回给客户的 Token 就可以了，扩展性得到提升。

JWT 本质上就一段签名的 JSON 格式的数据。由于它是带有签名的，因此接收者便可以验证它的真实性。

下面是 RFC 7519 对 JWT 做的较为正式的定义。

JSON Web Token (JWT) is a compact, URL-safe means of representing claims to be transferred between two parties. The claims in a JWT are encoded as a JSON object that is used as the payload of a JSON Web Signature (JWS) structure or as the plaintext of a JSON Web Encryption (JWE) structure, enabling the claims to be digitally signed or integrity protected with a Message Authentication Code (MAC) and/or encrypted. ——JSON Web Token (JWT)

JWT 由 3 部分构成:

Header :描述 JWT 的元数据。定义了生成签名的算法以及 Token 的类型。
Payload（负载）:用来存放实际需要传递的数据
Signature（签名）：服务器通过Payload、Header和一个密钥(secret)使用 Header 里面指定的签名算法（默认是 HMAC SHA256）生成。
在基于 Token 进行身份验证的的应用程序中，服务器通过Payload、Header和一个密钥(secret)创建令牌（Token）并将 Token 发送给客户端，客户端将 Token 保存在 Cookie 或者 localStorage 里面，以后客户端发出的所有请求都会携带这个令牌。你可以把它放在 Cookie 里面自动发送，但是这样不能跨域，所以更好的做法是放在 HTTP Header 的 Authorization字段中：Authorization: Bearer Token。

![image-20210520130410868](C:/Users/y/Desktop/春招笔记/images/image-20210520130410868.png)

用户向服务器发送用户名和密码用于登陆系统。
身份验证服务响应并返回了签名的 JWT，上面包含了用户是谁的内容。
用户以后每次向后端发请求都在Header中带上 JWT。
服务端检查 JWT 并从中获取用户相关信息。





# 分布式架构下，Session 共享有什么方案?

1. 不要有session：但是确实在某些场景下，是可以没有session的，其实在很多接口类系统当中，都提倡【API无状态服务】；也就是每一次的接口访问，都不依赖于session、不依赖于前一次的接口访问；
2. 存入cookie中：将session存储到cookie中，但是缺点也很明显，例如每次请求都得带着session，数据存储在客户端本地，是有风险的；
3. session同步：对个服务器之间同步session，这样可以保证每个服务器上都有全部的session信息，不过当服务器数量比较多的时候，同步是会有延迟甚至同步失败；
4. 使用Nginx（或其他复杂均衡软硬件）中的ip绑定策略，同一个ip只能在指定的同一个机器访问，但是这样做风险也比较大，而且也是去了负载均衡的意义；
5. 我们现在的系统会把session放到Redis中存储，虽然架构上变得复杂，并且需要多访问一次Redis，但是这种方案带来的好处也是很大的：实现session共享，可以水平扩展（增加Redis服务器），服务器重启session不丢失（不过也要注意session在Redis中的刷新/失效机制），不仅可以跨服务器session共享，甚至可以跨平台（例如网页端和APP端）。

# springcloud核心组件有哪些？

服务注册与发现——Netflix Eureka、Nacos、Zookeeper

客户端负载均衡——Netflix Ribbon、SpringCloud LoadBalancer

服务熔断器——Netflix Hystrix、Alibaba Sentinel、Resilience4J

服务网关——Netflix Zuul、SpringCloud Gateway

服务接口调用——Netflix Feign、 Resttemplate、Openfeign

链路追踪——Netflix Sleuth、Skywalking、Pinpoint

聚合Hystrix监控数据——Netflix Turbine

监控中心---- SpringBoot Admin

配置中心——Spring Cloud Config 、Apollo、nacos

# 微服务架构原理是什么？

主要是面向SOA理念，更细小粒度服务的拆分，将功能分解到各个服务当中，从而降低系统的耦合性，并提供更加灵活的服务支持。

# 注册中心的原理是什么？

服务启动后向Eureka注册，Eureka Server会将注册信息向其他Eureka Server进行同步，当服务消费者要调用服务提供者，则向服务注册中心获取服务提供者地址，然后会将服务提供者地址缓存在本地，下次再调用时，则直接从本地缓存中取，完成一次调用



# 配置中心的原理是什么？

在服务运行之前，将所需的配置信息从配置仓库拉取到本地服务，达到统一化配置管理的目的



# 配置中心是如何实现自动刷新的？

1. 配置中心Server端承担起配置刷新的职责

2. 提交配置触发post请求给server端的bus/refresh接口

3. server端接收到请求并发送给Spring Cloud Bus总线

4. Spring Cloud bus接到消息并通知给其它连接到总线的客户端

5. 其它客户端接收到通知，请求Server端获取最新配置

6. 全部客户端均获取到最新的配置



# 配置中心是如何保证数据安全的?

1.保证容器文件访问的安全性，即保证所有的网络资源请求都需要登录

2.将配置中心里所有配置文件中的密码进行加密，保证其密文性

3.开发环境禁止拉取生产环境的配置文件



# 用zookeeper和eureka做注册中心有什么区别?

Zookeeper保证的是CP（一致性，容错性）, 而Eureka则是AP（可用性，容错性）。

# Spring Cloud和Dubbo有哪些区别?

1. dubbo 是二进制传输，对象直接转成二进制，使用RPC通信。

SpringCloud是http 传输，同时使用http协议一般会使用JSON报文，json再转二进制，消耗会更大。

2. Dubbo只是实现了服务治理，而Spring Cloud下面有几十个子项目分别覆盖了微服务架构下的方方面面，服务治理只是其中的一个方面，一定程度来说，Dubbo只是Spring Cloud Netflix中的一个子集。

# Ribbon负载均衡原理是什么?

1. Ribbon通过ILoadBalancer接口对外提供统一的选择服务器(Server)的功能，此接口会根据不同的负载均衡策略(IRule)选择合适的Server返回给使用者。

2. IRule是负载均衡策略的抽象，ILoadBalancer通过调用IRule的choose()方法返回Server

3. IPing用来检测Server是否可用，ILoadBalancer的实现类维护一个Timer每隔10s检测一次Server的可用状态

4. IClientConfig主要定义了用于初始化各种客户端和负载均衡器的配置信息，器实现类为DefaultClientConfigImpl

# 微服务熔断降级机制是什么?

微服务框架是许多服务互相调用的，要是不做任何保护的话，某一个服务挂了，就会引起连锁反应，导致别的服务也挂。Hystrix 是隔离、熔断以及降级的一个框架。如果调用某服务报错（或者挂了），就对该服务熔断，在 5 分钟内请求此服务直接就返回一个默认值，不需要每次都卡几秒，这个过程，就是所谓的熔断。但是熔断了之后就会少调用一个服务，此时需要做下标记，标记本来需要做什么业务，但是因为服务挂了，暂时没有做，等该服务恢复了，就可以手工处理这些业务。这个过程，就是所谓的降级。

# 什么是Hystrix？实现原理是什么?

Hystrix是一个延迟和容错库，旨在隔离对远程系统、服务和第三方库的访问点，停止级联故障，并在 不可避免发生故障的复杂分布式系统中实现快速恢复。主要靠Spring的AOP实现

 实现原理

正常情况下，断路器关闭，服务消费者正常请求微服务

一段事件内，失败率达到一定阈值，断路器将断开，此时不再请求服务提供者，而是只是快速失败的方法（断路方法）

 断路器打开一段时间，自动进入“半开”状态，此时，断路器可允许一个请求方法服务提供者，如果请求调用成功，则关闭断路器，否则继续保持断路器打开状态。

 断路器hystrix是保证了局部发生的错误，不会扩展到整个系统，从而保证系统的即使出现局部问题也不会造成系统雪崩



# 注册中心挂了，或者服务挂了，应该如何处理?

注册中心挂了，可以读取本地持久化里的配置

服务挂了 应该配有服务监控中心 感知到服务下线后可以通过配置的邮件通知相关人员排查问题。

# 说说你对RPC、RMI如何理解?

RPC 远程过程调用协议，通过网络从远程计算机上请求调用某种服务。

RMI:远程方法调用 能够让在客户端Java虚拟机上的对象像调用本地对象一样调用服务端java 虚拟机中的对象上的方法。







# redis持久化机制：RDB和AOF

## Redis 持久化

Redis 提供了不同级别的持久化方式:

- RDB持久化方式能够在指定的时间间隔能对你的数据进行快照存储.

- AOF持久化方式记录每次对服务器写的操作,当服务器重启的时候会重新执行这些命令来恢复原始的数据,AOF命令以redis协议追加保存每次写的操作到文件末尾.Redis还能对AOF文件进行后台重写,使得AOF文件的体积不至于过大.

- 如果你只希望你的数据在服务器运行的时候存在,你也可以不使用任何持久化方式.

- 你也可以同时开启两种持久化方式, 在这种情况下, 当redis重启的时候会优先载入AOF文件来恢复原始的数据,因为在通常情况下AOF文件保存的数据集要比RDB文件保存的数据集要完整.

- 最重要的事情是了解RDB和AOF持久化方式的不同,让我们以RDB持久化方式开始:

  ## **RDB的优点**

- RDB是一个非常紧凑的文件,它保存了某个时间点得数据集,非常适用于数据集的备份,比如你可以在每个小时报保存一下过去24小时内的数据,同时每天保存过去30天的数据,这样即使出了问题你也可以根据需求恢复到不同版本的数据集.

- RDB是一个紧凑的单一文件,很方便传送到另一个远端数据中心或者亚马逊的S3（可能加密），非常适用于灾难恢复.

- RDB在保存RDB文件时父进程唯一需要做的就是fork出一个子进程,接下来的工作全部由子进程来做，父进程不需要再做其他IO操作，所以RDB持久化方式可以最大化redis的性能.

- 与AOF相比,在恢复大的数据集的时候，RDB方式会更快一些.

  ## **RDB的缺点**

- 如果你希望在redis意外停止工作（例如电源中断）的情况下丢失的数据最少的话，那么RDB不适合你.虽然你可以配置不同的save时间点(例如每隔5分钟并且对数据集有100个写的操作),是Redis要完整的保存整个数据集是一个比较繁重的工作,你通常会每隔5分钟或者更久做一次完整的保存,万一在Redis意外宕机,你可能会丢失几分钟的数据.

- RDB 需要经常fork子进程来保存数据集到硬盘上,当数据集比较大的时候,fork的过程是非常耗时的,可能会导致Redis在一些毫秒级内不能响应客户端的请求.如果数据集巨大并且CPU性能不是很好的情况下,这种情况会持续1秒,AOF也需要fork,但是你可以调节重写日志文件的频率来提高数据集的耐久度.

  ## **AOF 优点**

- 使用AOF 会让你的Redis更加耐久: 你可以使用不同的fsync策略：无fsync,每秒fsync,每次写的时候fsync.使用默认的每秒fsync策略,Redis的性能依然很好(fsync是由后台线程进行处理的,主线程会尽力处理客户端请求),一旦出现故障，你最多丢失1秒的数据.

- AOF文件是一个只进行追加的日志文件,所以不需要写入seek,即使由于某些原因(磁盘空间已满，写的过程中宕机等等)未执行完整的写入命令,你也也可使用redis-check-aof工具修复这些问题.

- Redis 可以在 AOF 文件体积变得过大时，自动地在后台对 AOF 进行重写： 重写后的新 AOF 文件包含了恢复当前数据集所需的最小命令集合。 整个重写操作是绝对安全的，因为 Redis 在创建新 AOF 文件的过程中，会继续将命令追加到现有的 AOF 文件里面，即使重写过程中发生停机，现有的 AOF 文件也不会丢失。 而一旦新 AOF 文件创建完毕，Redis 就会从旧 AOF 文件切换到新 AOF 文件，并开始对新 AOF 文件进行追加操作。

- AOF 文件有序地保存了对数据库执行的所有写入操作， 这些写入操作以 Redis 协议的格式保存， 因此 AOF 文件的内容非常容易被人读懂， 对文件进行分析（parse）也很轻松。 导出（export） AOF 文件也非常简单： 举个例子， 如果你不小心执行了 FLUSHALL 命令， 但只要 AOF 文件未被重写， 那么只要停止服务器， 移除 AOF 文件末尾的 FLUSHALL 命令， 并重启 Redis ， 就可以将数据集恢复到 FLUSHALL 执行之前的状态。

  ## **AOF 缺点**

- 对于相同的数据集来说，AOF 文件的体积通常要大于 RDB 文件的体积。

- 根据所使用的 fsync 策略，AOF 的速度可能会慢于 RDB 。 在一般情况下， 每秒 fsync 的性能依然非常高， 而关闭 fsync 可以让 AOF 的速度和 RDB 一样快， 即使在高负荷之下也是如此。 不过在处理巨大的写入载入时，RDB 可以提供更有保证的最大延迟时间（latency）。

  ## 4.X版本的整合策略

  在AOF重写策略上做了优化

  在重写AOF文件时，4.x版本以前是把内存数据集的操作指令落地，而新版本是把内存的数据集以rdb的形式落地

  这样重写后的AOF依然追加的是日志，但是，在恢复的时候是先rdb再增量的日志，性能更优秀

## 扩展知识

异步线程知识点

计算机组成原理

fork

copy on write

系统IO

pagecache

fsync























---

# redis的过期键有哪些删除策略

## 过期精度

在 Redis 2.4 及以前版本，过期期时间可能不是十分准确，有0-1秒的误差。

从 Redis 2.6 起，过期时间误差缩小到0-1毫秒。

## 过期和持久

Keys的过期时间使用Unix时间戳存储(从Redis 2.6开始以毫秒为单位)。这意味着即使Redis实例不可用，时间也是一直在流逝的。

要想过期的工作处理好，计算机必须采用稳定的时间。 如果你将RDB文件在两台时钟不同步的电脑间同步，有趣的事会发生（所有的 keys装载时就会过期）。

即使正在运行的实例也会检查计算机的时钟，例如如果你设置了一个key的有效期是1000秒，然后设置你的计算机时间为未来2000秒，这时key会立即失效，而不是等1000秒之后。

## Redis如何淘汰过期的keys

Redis keys过期有两种方式：被动和主动方式。

当一些客户端尝试访问它时，key会被发现并主动的过期。

当然，这样是不够的，因为有些过期的keys，永远不会访问他们。 无论如何，这些keys应该过期，所以定时随机测试设置keys的过期时间。所有这些过期的keys将会从密钥空间删除。

具体就是Redis每秒10次做的事情：

1. 测试随机的20个keys进行相关过期检测。
2. 删除所有已经过期的keys。
3. 如果有多于25%的keys过期，重复步奏1.

这是一个平凡的概率算法，基本上的假设是，我们的样本是这个密钥控件，并且我们不断重复过期检测，直到过期的keys的百分百低于25%,这意味着，在任何给定的时刻，最多会清除1/4的过期keys。

## 在复制AOF文件时如何处理过期

为了获得正确的行为而不牺牲一致性，当一个key过期，`DEL`将会随着AOF文字一起合成到所有附加的slaves。在master实例中，这种方法是集中的，并且不存在一致性错误的机会。

然而，当slaves连接到master时，不会独立过期keys（会等到master执行DEL命令），他们任然会在数据集里面存在，所以当slave当选为master时淘汰keys会独立执行，然后成为master。

## ﻿扩展

绝对时间点过期

相对时间点过期

时钟轮算法































---

# redis线程模型有哪些，单线程为什么快

## IO模型维度的特征

IO模型使用了多路复用器，在linux系统中使用的是EPOLL

类似netty的BOSS,WORKER使用一个EventLoopGroup(threads=1)

单线程的Reactor模型，每次循环取socket中的命令然后逐一操作，可以保证socket中的指令是按顺序的，不保证不同的socket也就是客户端的命令的顺序性

命令操作在单线程中顺序操作，没有多线程的困扰不需要锁的复杂度，在操作数据上相对来说是原子性质的

## 架构设计模型

自身的内存存储数据，读写操作不设计磁盘IO

redis除了提供了Value具备类型还为每种类型实现了一些操作命令

实现了计算向数据移动，而非数据想计算移动，这样在IO的成本上有一定的优势

且在数据结构类型上，丰富了一些统计类属性，读写操作中，写操作会O(1)负载度更新length类属性，使得读操作也是O(1)的









---



# 缓存雪崩、缓存穿透、缓存击穿在实际中如何处理

## 缓存穿透

缓存穿透是指查询一个一定不存在的数据，由于缓存是不命中时被动写的，并且出于容错考虑，如果从存储层查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到存储层去查询，失去了缓存的意义。在流量大时，可能DB就挂掉了，要是有人利用不存在的key频繁攻击我们的应用，这就是漏洞。

### 解决方案

有很多种方法可以有效地解决缓存穿透问题，最常见的则是采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被 这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。另外也有一个更为简单粗暴的方法（我们采用的就是这种），如果一个查询返回的数据为空（不管是数 据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。

## 缓存击穿

对于一些设置了过期时间的key，如果这些key可能会在某些时间点被超高并发地访问，是一种非常“热点”的数据。这个时候，需要考虑一个问题：缓存被“击穿”的问题，这个和缓存雪崩的区别在于这里针对某一key缓存，前者则是很多key。

缓存在某个时间点过期的时候，恰好在这个时间点对这个Key有大量的并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端DB压垮。

### 解决方案

缓存失效时的雪崩效应对底层系统的冲击非常可怕。大多数系统设计者考虑用加锁或者队列的方式保证缓存的单线 程（进程）写，从而避免失效时大量的并发请求落到底层存储系统上。这里分享一个简单方案就时讲缓存失效时间分散开，比如我们可以在原有的失效时间基础上增加一个随机值，比如1-5分钟随机，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。

## 缓存雪崩

缓存雪崩是指在我们设置缓存时采用了相同的过期时间，导致缓存在某一时刻同时失效，请求全部转发到DB，DB瞬时压力过重雪崩。

### 解决方案

1.使用互斥锁(mutex key)
业界比较常用的做法，是使用mutex。简单地来说，就是在缓存失效的时候（判断拿出来的值为空），不是立即去load db，而是先使用缓存工具的某些带成功操作返回值的操作（比如Redis的SETNX或者Memcache的ADD）去set一个mutex key，当操作返回成功时，再进行load db的操作并回设缓存；否则，就重试整个get缓存的方法。
SETNX，是「SET if Not eXists」的缩写，也就是只有不存在的时候才设置，可以利用它来实现锁的效果。在redis2.6.1之前版本未实现setnx的过期时间

2."提前"使用互斥锁(mutex key)：
在value内部设置1个超时值(timeout1), timeout1比实际的memcache timeout(timeout2)小。当从cache读取到timeout1发现它已经过期时候，马上延长timeout1并重新设置到cache。然后再从数据库加载数据并设置到cache中。

3."永远不过期"：  
这里的“永远不过期”包含两层意思：

(1) 从redis上看，确实没有设置过期时间，这就保证了，不会出现热点key过期问题，也就是“物理”不过期。

(2) 从功能上看，如果不过期，那不就成静态的了吗？所以我们把过期时间存在key对应的value里，如果发现要过期了，通过一个后台的异步线程进行缓存的构建，也就是“逻辑”过期

从实战看，这种方法对于性能非常友好，唯一不足的就是构建缓存时候，其余线程(非构建缓存的线程)可能访问的是老数据，但是对于一般的互联网功能来说这个还是可以忍受。

## 总结

穿透：缓存不存在，数据库不存在，高并发，少量key

击穿：缓存不存在，数据库存在，高并发，少量key

雪崩：缓存不存在，数据库存在，高并发，大量key

## 语义有些许差异，但是，都可以使用限流的互斥锁，保障数据库的稳定





























----

# redis事务是怎么实现的

MULTI 、 EXEC 、 DISCARD 和 WATCH 是 Redis 事务相关的命令。事务可以一次执行多个命令， 并且带有以下两个重要的保证：

```
事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。

事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。
```

EXEC 命令负责触发并执行事务中的所有命令：

```
如果客户端在使用 MULTI 开启了一个事务之后，却因为断线而没有成功执行 EXEC ，那么事务中的所有命令都不会被执行。
另一方面，如果客户端成功在开启事务之后执行 EXEC ，那么事务中的所有命令都会被执行。
```


当使用 AOF 方式做持久化的时候， Redis 会使用单个 write(2) 命令将事务写入到磁盘中。

然而，如果 Redis 服务器因为某些原因被管理员杀死，或者遇上某种硬件故障，那么可能只有部分事务命令会被成功写入到磁盘中。

如果 Redis 在重新启动时发现 AOF 文件出了这样的问题，那么它会退出，并汇报一个错误。

使用redis-check-aof程序可以修复这一问题：它会移除 AOF 文件中不完整事务的信息，确保服务器可以顺利启动。

从 2.2 版本开始，Redis 还可以通过乐观锁（optimistic lock）实现 CAS （check-and-set）操作，具体信息请参考文档的后半部分。

## 事务中的错误

使用事务时可能会遇上以下两种错误：

```
事务在执行 EXEC 之前，入队的命令可能会出错。比如说，命令可能会产生语法错误（参数数量错误，参数名错误，等等），或者其他更严重的错误，比如内存不足（如果服务器使用 maxmemory 设置了最大内存限制的话）。
命令可能在 EXEC 调用之后失败。举个例子，事务中的命令可能处理了错误类型的键，比如将列表命令用在了字符串键上面，诸如此类。
```

对于发生在 EXEC 执行之前的错误，客户端以前的做法是检查命令入队所得的返回值：如果命令入队时返回 QUEUED ，那么入队成功；否则，就是入队失败。如果有命令在入队时失败，那么大部分客户端都会停止并取消这个事务。

不过，从 Redis 2.6.5 开始，服务器会对命令入队失败的情况进行记录，并在客户端调用 EXEC 命令时，拒绝执行并自动放弃这个事务。

在 Redis 2.6.5 以前， Redis 只执行事务中那些入队成功的命令，而忽略那些入队失败的命令。 而新的处理方式则使得在流水线（pipeline）中包含事务变得简单，因为发送事务和读取事务的回复都只需要和服务器进行一次通讯。

至于那些在 EXEC 命令执行之后所产生的错误， 并没有对它们进行特别处理： 即使事务中有某个/某些命令在执行时产生了错误， 事务中的其他命令仍然会继续执行。

## 为什么 Redis 不支持回滚（roll back）

如果你有使用关系式数据库的经验， 那么 “Redis 在事务失败时不进行回滚，而是继续执行余下的命令”这种做法可能会让你觉得有点奇怪。

以下是这种做法的优点：

- Redis 命令只会因为错误的语法而失败（并且这些问题不能在入队时发现），或是命令用在了错误类型的键上面：这也就是说，从实用性的角度来说，失败的命令是由编程错误造成的，而这些错误应该在开发的过程中被发现，而不应该出现在生产环境中。
- 因为不需要对回滚进行支持，所以 Redis 的内部可以保持简单且快速。

有种观点认为 Redis 处理事务的做法会产生 bug ， 然而需要注意的是， 在通常情况下， 回滚并不能解决编程错误带来的问题。 举个例子， 如果你本来想通过 [INCR](http://redis.cn/commands/incr.html) 命令将键的值加上 1 ， 却不小心加上了 2 ， 又或者对错误类型的键执行了 [INCR](http://redis.cn/commands/incr.html) ， 回滚是没有办法处理这些情况的。



















---

# redis集群方案有哪些

## 常见集群分类

主从复制集群

分片集群

## redis有那些：

主从复制集群，手动切换

带有哨兵的HA的主从复制集群

客户端实现路由索引的分片集群

使用中间件代理层的分片集群

redis自身实现的cluster分片集群





---

# redis主从复制的原理是什么

## 主从复制机制

```
当一个 master 实例和一个 slave 实例连接正常时， master 会发送一连串的命令流来保持对 slave 的更新，以便于将自身数据集的改变复制给 slave ， ：包括客户端的写入、key 的过期或被逐出等等。

当 master 和 slave 之间的连接断开之后，因为网络问题、或者是主从意识到连接超时， slave 重新连接上 master 并会尝试进行部分重同步：这意味着它会尝试只获取在断开连接期间内丢失的命令流。

当无法进行部分重同步时， slave 会请求进行全量重同步。这会涉及到一个更复杂的过程，例如 master 需要创建所有数据的快照，将之发送给 slave ，之后在数据集更改时持续发送命令流到 slave 。
```

## 主从复制的关注点

```
Redis 使用异步复制，slave 和 master 之间异步地确认处理的数据量

一个 master 可以拥有多个 slave

slave 可以接受其他 slave 的连接。除了多个 slave 可以连接到同一个 master 之外， slave 之间也可以像层叠状的结构（cascading-like structure）连接到其他 slave 。自 Redis 4.0 起，所有的 sub-slave 将会从 master 收到完全一样的复制流。

Redis 复制在 master 侧是非阻塞的。这意味着 master 在一个或多个 slave 进行初次同步或者是部分重同步时，可以继续处理查询请求。

复制在 slave 侧大部分也是非阻塞的。当 slave 进行初次同步时，它可以使用旧数据集处理查询请求，假设你在 redis.conf 中配置了让 Redis 这样做的话。否则，你可以配置如果复制流断开， Redis slave 会返回一个 error 给客户端。但是，在初次同步之后，旧数据集必须被删除，同时加载新的数据集。 slave 在这个短暂的时间窗口内（如果数据集很大，会持续较长时间），会阻塞到来的连接请求。自 Redis 4.0 开始，可以配置 Redis 使删除旧数据集的操作在另一个不同的线程中进行，但是，加载新数据集的操作依然需要在主线程中进行并且会阻塞 slave 。

复制既可以被用在可伸缩性，以便只读查询可以有多个 slave 进行（例如 O(N) 复杂度的慢操作可以被下放到 slave ），或者仅用于数据安全。

可以使用复制来避免 master 将全部数据集写入磁盘造成的开销：一种典型的技术是配置你的 master Redis.conf 以避免对磁盘进行持久化，然后连接一个 slave ，其配置为不定期保存或是启用 AOF。但是，这个设置必须小心处理，因为重新启动的 master 程序将从一个空数据集开始：如果一个 slave 试图与它同步，那么这个 slave 也会被清空。
任何时候数据安全性都是很重要的，所以如果 master 使用复制功能的同时未配置持久化，那么自动重启进程这项应该被禁用。
```

## Redis 复制功能是如何工作的

每一个 Redis master 都有一个 replication ID ：这是一个较大的伪随机字符串，标记了一个给定的数据集。每个 master 也持有一个偏移量，master 将自己产生的复制流发送给 slave 时，发送多少个字节的数据，自身的偏移量就会增加多少，目的是当有新的操作修改自己的数据集时，它可以以此更新 slave 的状态。复制偏移量即使在没有一个 slave 连接到 master 时，也会自增，所以基本上每一对给定的

> Replication ID, offset

都会标识一个 master 数据集的确切版本。

当 slave 连接到 master 时，它们使用 PSYNC 命令来发送它们记录的旧的 master replication ID 和它们至今为止处理的偏移量。通过这种方式， master 能够仅发送 slave 所需的增量部分。但是如果 master 的缓冲区中没有足够的命令积压缓冲记录，或者如果 slave 引用了不再知道的历史记录（replication ID），则会转而进行一个全量重同步：在这种情况下， slave 会得到一个完整的数据集副本，从头开始。

下面是一个全量同步的工作细节：

master 开启一个后台保存进程，以便于生产一个 RDB 文件。同时它开始缓冲所有从客户端接收到的新的写入命令。当后台保存完成时， master 将数据集文件传输给 slave， slave将之保存在磁盘上，然后加载文件到内存。再然后 master 会发送所有缓冲的命令发给 slave。这个过程以指令流的形式完成并且和 Redis 协议本身的格式相同。

你可以用 telnet 自己进行尝试。在服务器正在做一些工作的同时连接到 Redis 端口并发出 [SYNC](https://redis.io/commands/sync) 命令。你将会看到一个批量传输，并且之后每一个 master 接收到的命令都将在 telnet 回话中被重新发出。事实上 SYNC 是一个旧协议，在新的 Redis 实例中已经不再被使用，但是其仍然向后兼容：但它不允许部分重同步，所以现在 **PSYNC** 被用来替代 SYNC。

之前说过，当主从之间的连接因为一些原因崩溃之后， slave 能够自动重连。如果 master 收到了多个 slave 要求同步的请求，它会执行一个单独的后台保存，以便于为多个 slave 服务。

## 无需磁盘参与的复制

正常情况下，一个全量重同步要求在磁盘上创建一个 RDB 文件，然后将它从磁盘加载进内存，然后 slave以此进行数据同步。

如果磁盘性能很低的话，这对 master 是一个压力很大的操作。Redis 2.8.18 是第一个支持无磁盘复制的版本。在此设置中，子进程直接发送 RDB 文件给 slave，无需使用磁盘作为中间储存介质。







---



# redis缓存如何回收

## 回收策略

```
noeviction:返回错误当内存限制达到并且客户端尝试执行会让更多内存被使用的命令（大部分的写入指令，但DEL和几个例外）
allkeys-lru: 尝试回收最少使用的键（LRU），使得新添加的数据有空间存放。
volatile-lru: 尝试回收最少使用的键（LRU），但仅限于在过期集合的键,使得新添加的数据有空间存放。
allkeys-random: 回收随机的键使得新添加的数据有空间存放。
volatile-random: 回收随机的键使得新添加的数据有空间存放，但仅限于在过期集合的键。
volatile-ttl: 回收在过期集合的键，并且优先回收存活时间（TTL）较短的键,使得新添加的数据有空间存放。
volatile-lfu：从所有配置了过期时间的键中驱逐使用频率最少的键
allkeys-lfu：从所有键中驱逐使用频率最少的键
```

如果没有键满足回收的前提条件的话，策略**volatile-lru**, **volatile-random**以及**volatile-ttl**就和noeviction 差不多了。

选择正确的回收策略是非常重要的，这取决于你的应用的访问模式，不过你可以在运行时进行相关的策略调整，并且监控缓存命中率和没命中的次数，通过RedisINFO命令输出以便调优。

一般的经验规则:

- 使用**allkeys-lru**策略：当你希望你的请求符合一个幂定律分布，也就是说，你希望部分的子集元素将比其它其它元素被访问的更多。如果你不确定选择什么，这是个很好的选择。.
- 使用**allkeys-random**：如果你是循环访问，所有的键被连续的扫描，或者你希望请求分布正常（所有元素被访问的概率都差不多）。
- 使用**volatile-ttl**：如果你想要通过创建缓存对象时设置TTL值，来决定哪些对象应该被过期。

**allkeys-lru** 和 **volatile-random**策略对于当你想要单一的实例实现缓存及持久化一些键时很有用。不过一般运行两个实例是解决这个问题的更好方法。

为了键设置过期时间也是需要消耗内存的，所以使用**allkeys-lru**这种策略更加高效，因为没有必要为键取设置过期时间当内存有压力时。

## **回收进程如何工作**

理解回收进程如何工作是非常重要的:

- 一个客户端运行了新的命令，添加了新的数据。
- Redi检查内存使用情况，如果大于maxmemory的限制, 则根据设定好的策略进行回收。
- 一个新的命令被执行，等等。
- 所以我们不断地穿越内存限制的边界，通过不断达到边界然后不断地回收回到边界以下。

如果一个命令的结果导致大量内存被使用（例如很大的集合的交集保存到一个新的键），不用多久内存限制就会被这个内存使用量超越。



















# RabbitMQ的架构设计是什么样的

## 是AMQP的实现，相关概念语义

Broker:它提供一种传输服务,它的角色就是维护一条从生产者到消费者的路线，保证数据能按照指定的方式进行传输

Exchange：消息交换机,它指定消息按什么规则,路由到哪个队列。 

Queue:消息的载体,每个消息都会被投到一个或多个队列。 

Binding:绑定，它的作用就是把exchange和queue按照路由规则绑定起来. 

Routing Key:路由关键字,exchange根据这个关键字进行消息投递。 

vhost:虚拟主机,一个broker里可以有多个vhost，用作不同用户的权限分离。

Producer:消息生产者,就是投递消息的程序. 

Consumer:消息消费者,就是接受消息的程序. 

Channel:消息通道,在客户端的每个连接里,可建立多个channel.

### 核心概念

在mq领域中，producer将msg发送到queue，然后consumer通过消费queue完成P.C解耦

kafka是由producer决定msg发送到那个queue

rabbitmq是由Exchange决定msg应该怎么样发送到目标queue，这就是binding及对应的策略

### Exchange

Direct Exchange:直接匹配,通过Exchange名称+RountingKey来发送与接收消息. 
Fanout Exchange:广播订阅,向所有的消费者发布消息,但是只有消费者将队列绑定到该路由器才能收到消息,忽略Routing Key. 
Topic Exchange：主题匹配订阅,这里的主题指的是RoutingKey,RoutingKey可以采用通配符,如:*或#，RoutingKey命名采用.来分隔多个词,只有消息这将队列绑定到该路由器且指定RoutingKey符合匹配规则时才能收到消息; 
Headers Exchange:消息头订阅,消息发布前,为消息定义一个或多个键值对的消息头,然后消费者接收消息同时需要定义类似的键值对请求头:(如:x-mactch=all或者x_match=any)，只有请求头与消息头匹配,才能接收消息,忽略RoutingKey. 
默认的exchange:如果用空字符串去声明一个exchange，那么系统就会使用”amq.direct”这个exchange，我们创建一个queue时,默认的都会有一个和新建queue同名的routingKey绑定到这个默认的exchange上去

## 复杂与精简

在众多的MQ中间件中，首先学习Rabbitmq的时候，就理解他是一个单机的mq组件，为了系统的解耦，可以自己在业务层面做AKF

其在内卷能力做的非常出色，这得益于AMQP，也就是消息的传递形式、复杂度有exchange和queue的binding实现，这，对于P.C有很大的帮助

















---

# RabbitMQ如何确保消息发送和消息接收

## 消息发送确认

### 1 ConfirmCallback方法

ConfirmCallback 是一个回调接口，消息发送到 Broker 后触发回调，确认消息是否到达 Broker 服务器，**也就是只确认是否正确到达 Exchange 中。**



### 2 ReturnCallback方法

通过实现 ReturnCallback 接口，启动消息失败返回，此接口是在交换器路由不到队列时触发回调，该方法可以不使用，因为交换器和队列是在代码里绑定的，如果消息成功投递到 Broker 后几乎不存在绑定队列失败，除非你代码写错了。

## 消息接收确认

RabbitMQ 消息确认机制（ACK）默认是自动确认的，自动确认会在消息发送给消费者后立即确认，但存在丢失消息的可能，如果消费端消费逻辑抛出异常，假如你用回滚了也只是保证了数据的一致性，但是消息还是丢了，也就是消费端没有处理成功这条消息，那么就相当于丢失了消息。

消息确认模式有：

AcknowledgeMode.NONE：自动确认。
AcknowledgeMode.AUTO：根据情况确认。
AcknowledgeMode.MANUAL：手动确认。
消费者收到消息后，手动调用 Basic.Ack 或 Basic.Nack 或 Basic.Reject 后，RabbitMQ 收到这些消息后，才认为本次投递完成。

Basic.Ack 命令：用于确认当前消息。
Basic.Nack 命令：用于否定当前消息（注意：这是AMQP 0-9-1的RabbitMQ扩展） 。
Basic.Reject 命令：用于拒绝当前消息。
Nack,Reject后都有能力要求是否requeue消息或者进入死信队列































---

# RabbitMQ事务消息原理是什么

## 事务V.S确认

确认是对一件事的确认

事务是对批量的确认

增删改查中，事务是对于增删改的保证

## 发送方事务

开启事务，发送多条数据，事务提交或回滚是原子的，要么都提交，要么都回滚

## 消费方事务

消费方是读取行为，那么事务体现在哪里呢

rabbitmq的消费行为会触发queue中msg的是否删除、是否重新放回队列等行为，类增删改

所以，消费方的ack是要手动提交的，且最终确定以事务的提交和回滚决定

















---

# RabbitMQ死信队列、延时队列分别是什么

## 死信队列

DLX（Dead Letter Exchange），**死信交换器**。

当队列中的消息被拒绝、或者过期会变成死信，死信可以被重新发布到另一个交换器，这个交换器就是DLX，与DLX绑定的队列称为死信队列。
造成死信的原因：

- 信息被拒绝
- 信息超时
- 超过了队列的最大长度

### 过期消息：

    在 rabbitmq 中存在2种方可设置消息的过期时间，第一种通过对队列进行设置，这种设置后，该队列中所有的消息都存在相同的过期时间，第二种通过对消息本身进行设置，那么每条消息的过期时间都不一样。如果同时使用这2种方法，那么以过期时间小的那个数值为准。当消息达到过期时间还没有被消费，那么那个消息就成为了一个 死信 消息。
    
    队列设置：在队列申明的时候使用 x-message-ttl 参数，单位为 毫秒
    
    单个消息设置：是设置消息属性的 expiration 参数的值，单位为 毫秒

## 延迟队列

延迟队列存储的是延迟消息

延迟消息指的是，当消息被发发布出去之后，并不立即投递给消费者，而是在指定时间之后投递。如：

在订单系统中，订单有30秒的付款时间，在订单超时之后在投递给消费者处理超时订单。

rabbitMq没有直接支持延迟队列，可以通过死信队列实现。

在死信队列中，可以为普通交换器绑定多个消息队列，假设绑定过期时间为5分钟，10分钟和30分钟，3个消息队列，然后为每个消息队列设置DLX，为每个DLX关联一个死信队列。

当消息过期之后，被转存到对应的死信队列中，然后投递给指定的消费者消费。













---

# 简述kafka架构设计是什么样

语义概念

```
1 broker
Kafka 集群包含一个或多个服务器，服务器节点称为broker。

broker存储topic的数据。如果某topic有N个partition，集群有N个broker，那么每个broker存储该topic的一个partition。

如果某topic有N个partition，集群有(N+M)个broker，那么其中有N个broker存储该topic的一个partition，剩下的M个broker不存储该topic的partition数据。

如果某topic有N个partition，集群中broker数目少于N个，那么一个broker存储该topic的一个或多个partition。在实际生产环境中，尽量避免这种情况的发生，这种情况容易导致Kafka集群数据不均衡。

2 Topic
每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处）

类似于数据库的表名

3 Partition
topic中的数据分割为一个或多个partition。每个topic至少有一个partition。每个partition中的数据使用多个segment文件存储。partition中的数据是有序的，不同partition间的数据丢失了数据的顺序。如果topic有多个partition，消费数据时就不能保证数据的顺序。在需要严格保证消息的消费顺序的场景下，需要将partition数目设为1。

4 Producer
生产者即数据的发布者，该角色将消息发布到Kafka的topic中。broker接收到生产者发送的消息后，broker将该消息追加到当前用于追加数据的segment文件中。生产者发送的消息，存储到一个partition中，生产者也可以指定数据存储的partition。

5 Consumer
消费者可以从broker中读取数据。消费者可以消费多个topic中的数据。

6 Consumer Group
每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。这是kafka用来实现一个topic消息的广播（发给所有的consumer）和单播（发给任意一个consumer）的手段。一个topic可以有多个CG。topic的消息会复制-给consumer。如果需要实现广播，只要每个consumer有一个独立的CG就可以了。要实现单播只要所有的consumer在同一个CG。用CG还可以将consumer进行自由的分组而不需要多次发送消息到不同的topic。

7 Leader
每个partition有多个副本，其中有且仅有一个作为Leader，Leader是当前负责数据的读写的partition。

8 Follower
Follower跟随Leader，所有写请求都通过Leader路由，数据变更会广播给所有Follower，Follower与Leader保持数据同步。如果Leader失效，则从Follower中选举出一个新的Leader。当Follower与Leader挂掉、卡住或者同步太慢，leader会把这个follower从“in sync replicas”（ISR）列表中删除，重新创建一个Follower。

9 Offset
kafka的存储文件都是按照offset.kafka来命名，用offset做名字的好处是方便查找。例如你想找位于2049的位置，只要找到2048.kafka的文件即可。当然the first offset就是00000000000.kafka
```

KAFKA天生是分布式的，满足AKF的XYZ轴特点，扩展性，可靠性，高性能是没得说

而且，kafka具备自己的特色，比如动态ISR集合，是在强一致性，过半一致性之外的另一个实现手段





---



---

# Kafka消息丢失的场景有哪些

生产者在生产过程中的消息丢失

broker在故障后的消息丢失

消费者在消费过程中的消息丢失

## ACK机制

ack有3个可选值，分别是1，0，-1。

## ack=0：生产者在生产过程中的消息丢失

简单来说就是，producer发送一次就不再发送了，不管是否发送成功。

## ack=1：broker在故障后的消息丢失

简单来说就是，producer只要收到一个分区副本成功写入的通知就认为推送消息成功了。这里有一个地方需要注意，这个副本必须是leader副本。只有leader副本成功写入了，producer才会认为消息发送成功。

注意，ack的默认值就是1。这个默认值其实就是吞吐量与可靠性的一个折中方案。生产上我们可以根据实际情况进行调整，比如如果你要追求高吞吐量，那么就要放弃可靠性。

## ack=-1：生产侧和存储侧不会丢失数据

简单来说就是，producer只有收到分区内所有副本的成功写入的通知才认为推送消息成功了。

## Offset机制

kafka消费者的三种消费语义

at-most-once：最多一次，可能丢数据

at-least-once：最少一次，可能重复消费数据

exact-once message：精确一次





















---

# Kafka是pull？push？以及优劣势分析



Kafka最初考虑的问题是，customer应该从brokes拉取消息还是brokers将消息推送到consumer，也就是pull还push。

Kafka遵循了一种大部分消息系统共同的传统的设计：producer将消息推送到broker，consumer从broker拉取消息。

一些消息系统比如Scribe和Apache Flume采用了push模式，将消息推送到下游的consumer。

这样做有好处也有坏处：由broker决定消息推送的速率，对于不同消费速率的consumer就不太好处理了。

消息系统都致力于让consumer以最大的速率最快速的消费消息，但不幸的是，push模式下，当broker推送的速率远大于consumer消费的速率时，consumer恐怕就要崩溃了。

最终Kafka还是选取了传统的pull模式。

Pull模式的另外一个好处是consumer可以自主决定是否批量的从broker拉取数据。

Push模式必须在不知道下游consumer消费能力和消费策略的情况下决定是立即推送每条消息还是缓存之后批量推送。

如果为了避免consumer崩溃而采用较低的推送速率，将可能导致一次只推送较少的消息而造成浪费。

Pull模式下，consumer就可以根据自己的消费能力去决定这些策略。

Pull有个缺点是，如果broker没有可供消费的消息，将导致consumer不断在循环中轮询，直到新消息到达。

为了避免这点，Kafka有个参数可以让consumer阻塞知道新消息到达(当然也可以阻塞知道消息的数量达到某个特定的量这样就可以批量发



















---

# Kafka中zk的作用是什么

Zookeeper是分布式协调，注意它不是数据库

kafka中使用了zookeeper的分布式锁和分布式配置及统一命名的分布式协调解决方案

在kafka的broker集群中的controller的选择，是通过zk的临时节点争抢获得的

brokerID等如果自增的话也是通过zk的节点version实现的全局唯一

kafka中broker中的状态数据也是存储在zk中，不过这里要注意，zk不是数据库，所以存储的属于元数据

而，新旧版本变化中，就把曾经的offset从zk中迁移出了zk















---

# Kafka中高性能如何保障

首先，性能的最大瓶颈依然是IO，这个是不能逾越的鸿沟

虽然，broker在持久化数据的时候已经最大努力的使用了磁盘的顺序读写

更进一步的性能优化是零拷贝的使用，也就是从磁盘日志到消费者客户端的数据传递，因为kafka是mq，对于msg不具备加工处理，所以得以实现

然后就是大多数分布式系统一样，总要做tradeoff，在速度与可用性/可靠性中挣扎

ACK的0，1，-1级别就是在性能和可靠中权衡















---

# kafka的rebalance机制是什么

## 消费者分区分配策略

Range 范围分区(默认的)

RoundRobin 轮询分区

Sticky策略

## 触发 Rebalance 的时机

Rebalance 的触发条件有3个。

- 组成员个数发生变化。例如有新的 consumer 实例加入该消费组或者离开组。
- 订阅的 Topic 个数发生变化。
- 订阅 Topic 的分区数发生变化。

## Coordinator协调过程

消费者如何发现协调者

消费者如何确定分配策略

如果需要再均衡分配策略的影响















---

# zk的数据模型和节点类型有哪些

## ZooKeeper数据模型

ZooKeeper的数据模型，在结构上和标准文件系统的非常相似，拥有一个层次的命名空间，都是采用树形层次结构，ZooKeeper树中的每个节点被称为—Znode。

和文件系统的目录树一样，ZooKeeper树中的每个节点可以拥有子节点。但也有不同之处：

```
	Znode兼具文件和目录两种特点。既像文件一样维护着数据、元信息、ACL、时间戳等数据结构，又像目录一样可以作为路径标识的一部分，并可以具有子Znode。用户对Znode具有增、删、改、查等操作（权限允许的情况下）
	
	Znode具有原子性操作，读操作将获取与节点相关的所有数据，写操作也将替换掉节点的所有数据。另外，每一个节点都拥有自己的ACL(访问控制列表)，这个列表规定了用户的权限，即限定了特定用户对目标节点可以执行的操作
	
	Znode存储数据大小有限制。ZooKeeper虽然可以关联一些数据，但并没有被设计为常规的数据库或者大数据存储，相反的是，它用来管理调度数据，比如分布式应用中的配置文件信息、状态信息、汇集位置等等。这些数据的共同特性就是它们都是很小的数据，通常以KB为大小单位。ZooKeeper的服务器和客户端都被设计为严格检查并限制每个Znode的数据大小至多1M，当时常规使用中应该远小于此值
	
	Znode通过路径引用，如同Unix中的文件路径。路径必须是绝对的，因此他们必须由斜杠字符来开头。除此以外，他们必须是唯一的，也就是说每一个路径只有一个表示，因此这些路径不能改变。在ZooKeeper中，路径由Unicode字符串组成，并且有一些限制。字符串"/zookeeper"用以保存管理信息，比如关键配额信息。
```

## 节点类型

Znode有两种，分别为临时节点和永久节点。
节点的类型在创建时即被确定，并且不能改变。
临时节点：该节点的生命周期依赖于创建它们的会话。一旦会话结束，临时节点将被自动删除，当然可以也可以手动删除。临时节点不允许拥有子节点。

永久节点：该节点的生命周期不依赖于会话，并且只有在客户端显示执行删除操作的时候，他们才能被删除。
　　
Znode还有一个序列化的特性，如果创建的时候指定的话，该Znode的名字后面会自动追加一个不断增加的序列号。序列号对于此节点的父节点来说是唯一的，这样便会记录每个子节点创建的先后顺序。它的格式为“%10d”(10位数字，没有数值的数位用0补充，例如“0000000001”)

在ZooKeeper中，每个数据节点都是有生命周期的，其生命周期的长短取决于数据节点的节点类型。

1、持久节点（PERSISTENT）

该数据节点别创建后，就会一直存在于ZooKeeper服务器上，直到有删除操作来主动删除该节点。

2、持久顺序节点（PERSISTENT_SEQUENTIAL）

持久顺序节点的基本特性和持久节点是一致的，额外的特性表现在顺序性上。在ZooKeeper中，每个父节点都会为它的第一级子节点维护一份顺序，用于记录每个子节点创建的先后顺序。

3、临时节点（EPHEMERAL）

临时节点的生命周期和客户端的回话绑定在一起，如果客户端会话失效，那么这个节点就会被自动地清理掉。

ZooKeeper规定了不能基于临时节点来创建子节点，即临时节点只能作为叶子节点。

4、临时顺序节点（EPHEMERAL_SEQUENTIAL）







---

# Zookeeper watch机制是什么

ZooKeeper是用来协调（同步）分布式进程的服务，提供了一个简单高性能的协调内核，用户可以在此之上构建更多复杂的分布式协调功能。

多个分布式进程通过ZooKeeper提供的API来操作共享的ZooKeeper内存数据对象ZNode来达成某种一致的行为或结果，这种模式本质上是基于状态共享的并发模型，与Java的多线程并发模型一致，他们的线程或进程都是”共享式内存通信“。

Java没有直接提供某种响应式通知接口来监控某个对象状态的变化，只能要么浪费CPU时间毫无响应式的轮询重试，或基于Java提供的某种主动通知（Notif）机制（内置队列）来响应状态变化，但这种机制是需要循环阻塞调用。

而ZooKeeper实现这些分布式进程的状态（ZNode的Data、Children）共享时，基于性能的考虑采用了类似的异步非阻塞的主动通知模式即Watch机制，使得分布式进程之间的“共享状态通信”更加实时高效，其实这也是ZooKeeper的主要任务决定的—协调。Consul虽然也实现了Watch机制，但它是阻塞的长轮询。

## ZooKeeper的Watch特性

1. Watch是一次性的，每次都需要重新注册，并且客户端在会话异常结束时不会收到任何通知，而快速重连接时仍不影响接收通知。
2. Watch的回调执行都是顺序执行的，并且客户端在没有收到关注数据的变化事件通知之前是不会看到最新的数据，另外需要注意不要在Watch回调逻辑中阻塞整个客户端的Watch回调
3. Watch是轻量级的，WatchEvent是最小的通信单元，结构上只包含通知状态、事件类型和节点路径。ZooKeeper服务端只会通知客户端发生了什么，并不会告诉具体内容。

## Zookeeper状态

Disconnected：客户端是断开连接的状态，不能连接服务集合中的任意一个
SyncConnected：客户端是连接状态，连接其中的一个服务
AuthFailed：鉴权失败
ConnectedReadOnly：客户端连接只读的服务器
SaslAuthenticated：SASL认证
Expired：服务器已经过期了该客户端的Session

## Zookeeper事件类型

None：无
NodeCreated：节点创建
NodeDeleted：节点删除
NodeDataChanged：节点数据改变
NodeChildrenChanged：子节点改变（添加/删除）

## Watcher使用的注意事项

Watcher是一次触发器，假如需要持续监听数据变更，需要在每次获取时设置Watcher
会话过期：当客户端会话过期时，该客户端注册的Watcher会失效
事件丢失：在接收通知和注册监视点之间，可能会丢失事件,但Zookeeper的状态变更和数据变化，都会记录在状态元数据信息和ZK数据节点上，所以能够获取最终一致的ZK信息状态
避免Watcher过多：服务器会对每一个注册Watcher事件的客户端发送通知，通知通过Socket连接的方式发送，当Watcher过多时，会产生一个尖峰的通知





















# zk的命名服务、配置管理、集群管理分别是什么

## 分布式协调

大于等于一的情况下，才会有协调，在协调的事务进行分类得到一些名词，语义能够接受就可以

## 命名服务

通过使用有序节点的特性做到协调命名规则

通过zk的事务ID递增，做到有序行命名规则

通过使用自己点做map映射，做到1:N的命名映射，比如DNS

顺序关系、映射关系

## 配置管理

配置、元数据、状态等语义可以通过ZK的节点1MB存储，或者通过zk的节点目录结构特性存储

并且通过watch机制，满足配置变化的全局通知能力

## 集群管理

通过zk的排他性，有序性

满足分布式锁、分布式选主、队列锁

串行化回调调度

分布式调度等











